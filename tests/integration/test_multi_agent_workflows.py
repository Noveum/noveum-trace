"""
Integration tests for multi-agent workflows.
"""

import pytest
import asyncio
import time
from unittest.mock import Mock, patch

from noveum_trace import (
    Agent, AgentConfig, AgentRegistry, get_agent_registry,
    AgentContext, get_current_agent,
    trace, observe, llm_trace, update_current_span
)
from noveum_trace.sinks.file import FileSink, FileSinkConfig
from noveum_trace.core.tracer import TracerConfig
from noveum_trace.types import CustomHeaders


class TestMultiAgentWorkflows:
    """Test complete multi-agent workflows."""
    
    def setup_method(self):
        """Set up test method."""
        self.registry = get_agent_registry()
        
        # Create file sink for testing
        self.sink_config = FileSinkConfig(
            file_path="/tmp/test_traces.jsonl",
            max_file_size_mb=10,
            max_files=5
        )
        self.file_sink = FileSink(self.sink_config)
        
        # Create project tracer config
        self.project_config = TracerConfig(
            project_name="multi-agent-test",
            custom_headers=CustomHeaders(
                project_id="test-project",
                org_id="test-org"
            ),
            sinks=[self.file_sink]
        )
    
    def teardown_method(self):
        """Tear down test method."""
        self.registry.shutdown()
        self.file_sink.shutdown()
    
    def test_simple_multi_agent_workflow(self):
        """Test a simple workflow with multiple agents."""
        # Create agents
        coordinator_config = AgentConfig(
            name="coordinator",
            agent_type="coordinator",
            description="Coordinates the workflow"
        )
        
        worker1_config = AgentConfig(
            name="worker-1",
            agent_type="worker",
            description="First worker agent",
            parent_agent="coordinator"
        )
        
        worker2_config = AgentConfig(
            name="worker-2", 
            agent_type="worker",
            description="Second worker agent",
            parent_agent="coordinator"
        )
        
        coordinator = self.registry.register_agent(coordinator_config)
        worker1 = self.registry.register_agent(worker1_config)
        worker2 = self.registry.register_agent(worker2_config)
        
        # Define workflow functions
        @trace(name="coordinate_workflow")
        def coordinate_workflow(task):
            """Coordinator function."""
            update_current_span(
                input=task,
                metadata={"workflow_type": "parallel_processing"}
            )
            
            # Simulate coordination work
            time.sleep(0.01)
            
            # Process with workers
            result1 = process_with_worker1(f"{task}-part1")
            result2 = process_with_worker2(f"{task}-part2")
            
            # Combine results
            final_result = f"Combined: {result1} + {result2}"
            update_current_span(output=final_result)
            
            return final_result
        
        @trace(name="worker1_processing")
        def process_with_worker1(subtask):
            """Worker 1 function."""
            update_current_span(
                input=subtask,
                metadata={"worker_id": "worker-1"}
            )
            
            # Simulate work
            time.sleep(0.005)
            result = f"W1:{subtask}"
            
            update_current_span(output=result)
            return result
        
        @trace(name="worker2_processing")
        def process_with_worker2(subtask):
            """Worker 2 function."""
            update_current_span(
                input=subtask,
                metadata={"worker_id": "worker-2"}
            )
            
            # Simulate work
            time.sleep(0.005)
            result = f"W2:{subtask}"
            
            update_current_span(output=result)
            return result
        
        # Execute workflow
        with AgentContext(coordinator):
            coordinator_result = coordinate_workflow("main-task")
            
            # Switch to worker contexts for processing
            with AgentContext(worker1):
                # This would be called from coordinate_workflow in real scenario
                pass
            
            with AgentContext(worker2):
                # This would be called from coordinate_workflow in real scenario
                pass
        
        assert "Combined:" in coordinator_result
        assert "W1:main-task-part1" in coordinator_result
        assert "W2:main-task-part2" in coordinator_result
    
    @pytest.mark.asyncio
    async def test_async_multi_agent_workflow(self):
        """Test async multi-agent workflow."""
        # Create agents
        orchestrator_config = AgentConfig(
            name="orchestrator",
            agent_type="orchestrator",
            capabilities={"async_processing"}
        )
        
        async_worker_config = AgentConfig(
            name="async-worker",
            agent_type="async_worker",
            capabilities={"async_processing"},
            parent_agent="orchestrator"
        )
        
        orchestrator = self.registry.register_agent(orchestrator_config)
        async_worker = self.registry.register_agent(async_worker_config)
        
        # Define async workflow functions
        @trace(name="orchestrate_async_workflow")
        async def orchestrate_async_workflow(tasks):
            """Orchestrator function."""
            update_current_span(
                input=tasks,
                metadata={"workflow_type": "async_parallel"}
            )
            
            # Process tasks concurrently
            results = await asyncio.gather(*[
                process_async_task(task) for task in tasks
            ])
            
            final_result = f"Processed: {', '.join(results)}"
            update_current_span(output=final_result)
            
            return final_result
        
        @trace(name="async_task_processing")
        async def process_async_task(task):
            """Async worker function."""
            update_current_span(
                input=task,
                metadata={"processing_type": "async"}
            )
            
            # Simulate async work
            await asyncio.sleep(0.01)
            result = f"ASYNC:{task}"
            
            update_current_span(output=result)
            return result
        
        # Execute async workflow
        from noveum_trace.agents.context import AsyncAgentContext
        
        async with AsyncAgentContext(orchestrator):
            tasks = ["task1", "task2", "task3"]
            result = await orchestrate_async_workflow(tasks)
            
            # Switch to worker context for individual tasks
            async with AsyncAgentContext(async_worker):
                # Individual task processing would happen here
                pass
        
        assert "Processed:" in result
        assert "ASYNC:task1" in result
        assert "ASYNC:task2" in result
        assert "ASYNC:task3" in result
    
    def test_llm_multi_agent_workflow(self):
        """Test multi-agent workflow with LLM operations."""
        # Create LLM-focused agents
        planner_config = AgentConfig(
            name="planner",
            agent_type="planner",
            capabilities={"planning", "llm"},
            custom_headers=CustomHeaders(
                additional_headers={"agent_role": "planner"}
            )
        )
        
        executor_config = AgentConfig(
            name="executor",
            agent_type="executor", 
            capabilities={"execution", "llm"},
            custom_headers=CustomHeaders(
                additional_headers={"agent_role": "executor"}
            ),
            parent_agent="planner"
        )
        
        planner = self.registry.register_agent(planner_config)
        executor = self.registry.register_agent(executor_config)
        
        # Mock LLM responses
        class MockLLMResponse:
            def __init__(self, content):
                self.choices = [MockChoice(content)]
                self.usage = MockUsage()
                self.model = "gpt-4"
                self.id = f"mock-{int(time.time())}"
        
        class MockChoice:
            def __init__(self, content):
                self.message = MockMessage(content)
                self.finish_reason = "stop"
        
        class MockMessage:
            def __init__(self, content):
                self.content = content
        
        class MockUsage:
            def __init__(self):
                self.prompt_tokens = 50
                self.completion_tokens = 100
                self.total_tokens = 150
        
        # Define LLM workflow functions
        @observe(name="planning_component", metrics=["planning_accuracy"])
        def create_plan(objective):
            """Planning component."""
            update_current_span(
                input=objective,
                metadata={"component_type": "planner"}
            )
            
            # Simulate LLM planning call
            plan = call_planning_llm(f"Create a plan for: {objective}")
            
            update_current_span(
                output=plan,
                metadata={"plan_complexity": "medium"}
            )
            
            return plan
        
        @llm_trace(model="gpt-4", operation="chat", ai_system="openai")
        def call_planning_llm(prompt):
            """Mock LLM call for planning."""
            # Simulate LLM call
            time.sleep(0.01)
            return MockLLMResponse(f"Plan: Step 1, Step 2, Step 3 for '{prompt}'")
        
        @observe(name="execution_component", metrics=["execution_success"])
        def execute_plan(plan):
            """Execution component."""
            update_current_span(
                input=plan,
                metadata={"component_type": "executor"}
            )
            
            # Simulate LLM execution call
            result = call_execution_llm(f"Execute: {plan}")
            
            update_current_span(
                output=result,
                metadata={"execution_status": "completed"}
            )
            
            return result
        
        @llm_trace(model="gpt-4", operation="chat", ai_system="openai")
        def call_execution_llm(prompt):
            """Mock LLM call for execution."""
            # Simulate LLM call
            time.sleep(0.01)
            return MockLLMResponse(f"Executed: {prompt}")
        
        # Execute LLM workflow
        objective = "Build a web application"
        
        # Planning phase
        with AgentContext(planner):
            plan = create_plan(objective)
            plan_content = plan.choices[0].message.content
        
        # Execution phase
        with AgentContext(executor):
            execution_result = execute_plan(plan_content)
            result_content = execution_result.choices[0].message.content
        
        assert "Plan:" in plan_content
        assert "Step 1, Step 2, Step 3" in plan_content
        assert "Executed:" in result_content
    
    def test_agent_communication_workflow(self):
        """Test workflow with explicit agent communication."""
        # Create communicating agents
        sender_config = AgentConfig(
            name="sender",
            agent_type="communicator",
            capabilities={"send_messages"}
        )
        
        receiver_config = AgentConfig(
            name="receiver",
            agent_type="communicator",
            capabilities={"receive_messages"}
        )
        
        sender = self.registry.register_agent(sender_config)
        receiver = self.registry.register_agent(receiver_config)
        
        # Shared message queue (simulating inter-agent communication)
        message_queue = []
        
        @trace(name="send_message")
        def send_message(message, target_agent):
            """Send message to another agent."""
            update_current_span(
                input={"message": message, "target": target_agent},
                metadata={"communication_type": "async_message"}
            )
            
            # Add message to queue with sender info
            current_agent = get_current_agent()
            message_data = {
                "content": message,
                "sender": current_agent.name if current_agent else "unknown",
                "target": target_agent,
                "timestamp": time.time()
            }
            message_queue.append(message_data)
            
            update_current_span(
                output=message_data,
                metadata={"message_queued": True}
            )
            
            return message_data
        
        @trace(name="receive_messages")
        def receive_messages(agent_name):
            """Receive messages for an agent."""
            update_current_span(
                input={"agent": agent_name},
                metadata={"communication_type": "message_retrieval"}
            )
            
            # Get messages for this agent
            agent_messages = [
                msg for msg in message_queue 
                if msg["target"] == agent_name
            ]
            
            # Remove processed messages
            for msg in agent_messages:
                message_queue.remove(msg)
            
            update_current_span(
                output=agent_messages,
                metadata={"messages_received": len(agent_messages)}
            )
            
            return agent_messages
        
        @trace(name="process_received_message")
        def process_message(message_data):
            """Process a received message."""
            update_current_span(
                input=message_data,
                metadata={"processing_type": "message_handling"}
            )
            
            # Simulate message processing
            response = f"Processed: {message_data['content']} from {message_data['sender']}"
            
            update_current_span(
                output=response,
                metadata={"processing_completed": True}
            )
            
            return response
        
        # Execute communication workflow
        
        # Sender sends messages
        with AgentContext(sender):
            msg1 = send_message("Hello receiver!", "receiver")
            msg2 = send_message("How are you?", "receiver")
        
        # Receiver processes messages
        with AgentContext(receiver):
            messages = receive_messages("receiver")
            responses = []
            
            for message in messages:
                response = process_message(message)
                responses.append(response)
        
        assert len(messages) == 2
        assert len(responses) == 2
        assert "Hello receiver!" in responses[0]
        assert "How are you?" in responses[1]
        assert "sender" in responses[0]
    
    def test_hierarchical_agent_workflow(self):
        """Test hierarchical agent workflow with parent-child relationships."""
        # Create hierarchical agents
        manager_config = AgentConfig(
            name="manager",
            agent_type="manager",
            capabilities={"delegation", "coordination"}
        )
        
        team_lead_config = AgentConfig(
            name="team-lead",
            agent_type="team_lead",
            capabilities={"team_management"},
            parent_agent="manager"
        )
        
        developer1_config = AgentConfig(
            name="developer-1",
            agent_type="developer",
            capabilities={"coding"},
            parent_agent="team-lead"
        )
        
        developer2_config = AgentConfig(
            name="developer-2",
            agent_type="developer",
            capabilities={"coding"},
            parent_agent="team-lead"
        )
        
        manager = self.registry.register_agent(manager_config)
        team_lead = self.registry.register_agent(team_lead_config)
        developer1 = self.registry.register_agent(developer1_config)
        developer2 = self.registry.register_agent(developer2_config)
        
        # Define hierarchical workflow
        @trace(name="manage_project")
        def manage_project(project_spec):
            """Manager delegates project."""
            update_current_span(
                input=project_spec,
                metadata={"management_level": "executive"}
            )
            
            # Delegate to team lead
            result = delegate_to_team_lead(project_spec)
            
            update_current_span(
                output=result,
                metadata={"delegation_completed": True}
            )
            
            return result
        
        @trace(name="lead_team")
        def delegate_to_team_lead(project_spec):
            """Team lead manages development team."""
            update_current_span(
                input=project_spec,
                metadata={"management_level": "team"}
            )
            
            # Split work between developers
            task1 = f"{project_spec} - Frontend"
            task2 = f"{project_spec} - Backend"
            
            # Simulate delegation to developers
            result1 = develop_component(task1, "developer-1")
            result2 = develop_component(task2, "developer-2")
            
            # Combine results
            final_result = f"Team Result: {result1} + {result2}"
            
            update_current_span(
                output=final_result,
                metadata={"team_coordination": "completed"}
            )
            
            return final_result
        
        @trace(name="develop_component")
        def develop_component(task, developer_name):
            """Developer implements component."""
            update_current_span(
                input={"task": task, "developer": developer_name},
                metadata={"development_level": "individual"}
            )
            
            # Simulate development work
            time.sleep(0.005)
            result = f"Developed: {task} by {developer_name}"
            
            update_current_span(
                output=result,
                metadata={"development_completed": True}
            )
            
            return result
        
        # Execute hierarchical workflow
        project_spec = "E-commerce Platform"
        
        # Manager level
        with AgentContext(manager):
            # Team lead level
            with AgentContext(team_lead):
                # Developer level
                with AgentContext(developer1):
                    dev1_result = develop_component(f"{project_spec} - Frontend", "developer-1")
                
                with AgentContext(developer2):
                    dev2_result = develop_component(f"{project_spec} - Backend", "developer-2")
                
                team_result = delegate_to_team_lead(project_spec)
            
            final_result = manage_project(project_spec)
        
        assert "Team Result:" in final_result
        assert "Frontend" in final_result
        assert "Backend" in final_result
        assert "developer-1" in final_result
        assert "developer-2" in final_result
    
    def test_agent_metrics_and_monitoring(self):
        """Test agent metrics and monitoring in workflows."""
        # Create monitored agents
        monitored_config = AgentConfig(
            name="monitored-agent",
            agent_type="monitored",
            enable_metrics=True,
            metadata={"monitoring_level": "detailed"}
        )
        
        monitored_agent = self.registry.register_agent(monitored_config)
        
        @observe(
            name="monitored_operation",
            metrics=["latency", "success_rate", "throughput"],
            capture_input=True,
            capture_output=True
        )
        def monitored_operation(data):
            """Operation with detailed monitoring."""
            update_current_span(
                metadata={
                    "operation_type": "data_processing",
                    "data_size": len(str(data)),
                    "processing_mode": "batch"
                }
            )
            
            # Simulate processing with metrics
            start_time = time.time()
            
            # Simulate work
            time.sleep(0.02)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            result = f"Processed: {data}"
            
            update_current_span(
                output=result,
                metadata={
                    "processing_time_ms": processing_time * 1000,
                    "success": True,
                    "items_processed": 1
                }
            )
            
            return result
        
        # Execute monitored workflow
        with AgentContext(monitored_agent):
            results = []
            
            # Process multiple items to generate metrics
            for i in range(3):
                data = f"data_item_{i}"
                result = monitored_operation(data)
                results.append(result)
        
        assert len(results) == 3
        for i, result in enumerate(results):
            assert f"data_item_{i}" in result
        
        # Check agent activity
        assert monitored_agent.trace_count >= 3
        assert monitored_agent.last_activity is not None


if __name__ == "__main__":
    pytest.main([__file__])

