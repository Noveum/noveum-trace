{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Noveum Trace SDK Testing Notebook\n",
    "\n",
    "This notebook demonstrates the complete functionality of the Noveum Trace SDK installed from PyPI.\n",
    "\n",
    "## Features Tested:\n",
    "- Basic installation and setup\n",
    "- Environment variable configuration\n",
    "- Function tracing with decorators\n",
    "- LLM call tracing\n",
    "- Agent workflow tracing\n",
    "- Multi-agent systems\n",
    "- Tool tracing\n",
    "- Context managers\n",
    "- Streaming support\n",
    "- Integration examples\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Install Required Dependencies\n",
    "\n",
    "First, we'll install noveum-trace from PyPI along with some additional dependencies for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: noveum-trace in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (0.3.2)\n",
      "Requirement already satisfied: python-dotenv in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: openai in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (1.97.0)\n",
      "Requirement already satisfied: anthropic in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (0.57.1)\n",
      "Requirement already satisfied: requests>=2.25.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from noveum-trace) (2.32.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: noveum-trace in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (0.3.2)\n",
      "Requirement already satisfied: pip in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (25.1.1)\n",
      "Requirement already satisfied: python-dotenv>=0.19.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from noveum-trace) (1.1.1)\n",
      "Requirement already satisfied: requests>=2.25.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from noveum-trace) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install noveum-trace from PyPI and testing dependencies\n",
    "%pip install noveum-trace python-dotenv openai anthropic\n",
    "%pip install --upgrade noveum-trace pip\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Set Up Environment Variables\n",
    "\n",
    "Configure the necessary environment variables for the SDK to work properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded .env from: ../../../.env\n",
      "‚úÖ NOVEUM_API_KEY found in environment\n",
      "‚úÖ OPENAI_API_KEY found in environment\n",
      "\n",
      "üìã Environment Variables Status:\n",
      "NOVEUM_API_KEY: ‚úì\n",
      "OPENAI_API_KEY: ‚úì\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "# Note: Using robust path detection since __file__ is not available in Jupyter notebooks\n",
    "try:\n",
    "    # Try to get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    print(f\"üìÅ Current directory: {current_dir}\")\n",
    "    \n",
    "    # Look for .env file in current directory and parent directories\n",
    "    env_file_found = False\n",
    "    search_dir = current_dir\n",
    "    \n",
    "    for _ in range(5):  # Search up to 5 levels up\n",
    "        env_file = os.path.join(search_dir, '.env')\n",
    "        if os.path.exists(env_file):\n",
    "            print(f\"üìÑ Found .env file: {env_file}\")\n",
    "            load_dotenv(env_file)\n",
    "            env_file_found = True\n",
    "            break\n",
    "        search_dir = os.path.dirname(search_dir)\n",
    "        if search_dir == os.path.dirname(search_dir):  # Reached root\n",
    "            break\n",
    "    \n",
    "    if not env_file_found:\n",
    "        print(\"‚ÑπÔ∏è  No .env file found - continuing without it\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error loading .env file: {e}\")\n",
    "    print(\"‚ÑπÔ∏è  Continuing without .env file\")\n",
    "\n",
    "# Set up environment variables for testing\n",
    "# Replace with your actual API key or set in .env file\n",
    "if not os.getenv('NOVEUM_API_KEY'):\n",
    "    # For testing purposes, you can set a dummy API key\n",
    "    # In production, use your actual Noveum API key\n",
    "    os.environ['NOVEUM_API_KEY'] = 'test-api-key-for-demo'\n",
    "    print(\"‚ö†Ô∏è  Using dummy API key for testing. Set NOVEUM_API_KEY environment variable for production use.\")\n",
    "else:\n",
    "    print(\"‚úÖ NOVEUM_API_KEY found in environment\")\n",
    "\n",
    "# Optional: Set OpenAI API key for LLM examples\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"‚ÑπÔ∏è  OPENAI_API_KEY not found. LLM examples will use mock responses.\")\n",
    "else:\n",
    "    print(\"‚úÖ OPENAI_API_KEY found in environment\")\n",
    "\n",
    "print(\"\\nüìã Environment Variables Status:\")\n",
    "print(f\"NOVEUM_API_KEY: {'‚úì' if os.getenv('NOVEUM_API_KEY') else '‚úó'}\")\n",
    "print(f\"OPENAI_API_KEY: {'‚úì' if os.getenv('OPENAI_API_KEY') else '‚úó'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîÑ FLUSH HELPER: Automatic Trace Sending\n",
    "\n",
    "To ensure all traces are sent immediately to your endpoint, we'll create a helper function that can be called after any traced operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Flush helper functions initialized\n",
      "üìã Usage:\n",
      "  - Call flush_traces('operation_name') after any traced operation\n",
      "  - Use @auto_flush_decorator on functions containing traced operations\n",
      "  - This ensures immediate trace sending to your endpoint\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH HELPER FUNCTIONS FOR IMMEDIATE TRACE SENDING\n",
    "\n",
    "def flush_traces(operation_name=\"Operation\"):\n",
    "    \"\"\"\n",
    "    Helper function to flush traces immediately to endpoint.\n",
    "    Call this after any traced operation to ensure traces are sent right away.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        noveum_trace.flush()\n",
    "        print(f\"üì§ ‚úÖ {operation_name} traces flushed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"üì§ ‚ö†Ô∏è  {operation_name} flush warning: {e}\")\n",
    "\n",
    "def auto_flush_decorator(func):\n",
    "    \"\"\"\n",
    "    Decorator that automatically flushes traces after function execution.\n",
    "    Use this for any function that contains traced operations.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        flush_traces(func.__name__)\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Test the flush helper\n",
    "print(\"üîß Flush helper functions initialized\")\n",
    "print(\"üìã Usage:\")\n",
    "print(\"  - Call flush_traces('operation_name') after any traced operation\")\n",
    "print(\"  - Use @auto_flush_decorator on functions containing traced operations\")\n",
    "print(\"  - This ensures immediate trace sending to your endpoint\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Initialize the SDK\n",
    "\n",
    "Initialize the Noveum Trace SDK with your project configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Noveum Trace SDK initialized successfully!\n",
      "üìä Project: jupyter-test-project\n",
      "üîß Environment: development\n",
      "üåê Endpoint: https://noveum.ai/api/v1/traces (auto-appended)\n",
      "üîç Debug logging enabled - check console for HTTP request details\n",
      "üìã Config verified - Endpoint: https://noveum.ai/api\n",
      "üì¶ Batch size: 100\n",
      "‚è±Ô∏è  Batch timeout: 5.0s\n"
     ]
    }
   ],
   "source": [
    "import noveum_trace\n",
    "from noveum_trace import trace, trace_agent, trace_llm, trace_tool\n",
    "import logging\n",
    "\n",
    "# Enable detailed logging to debug transport issues\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "transport_logger = logging.getLogger('noveum_trace.transport')\n",
    "transport_logger.setLevel(logging.DEBUG)\n",
    "ENDPOINT = \"https://noveum.ai/api\"\n",
    "\n",
    "# Initialize the SDK with proper endpoint configuration\n",
    "try:\n",
    "    # IMPORTANT: The SDK will append \"/v1/traces\" to your endpoint\n",
    "    # So \"https://noveum-trace.free.beeceptor.com\" becomes \"https://noveum-trace.free.beeceptor.com/v1/traces\"\n",
    "    noveum_trace.init(\n",
    "        api_key=os.getenv('NOVEUM_API_KEY'),\n",
    "        project=\"jupyter-test-project\", \n",
    "        environment=\"development\",\n",
    "        endpoint=ENDPOINT,  # SDK will add /v1/traces automatically\n",
    "        debug=True,  # Enable debug mode for testing\n",
    "        \n",
    "        # Transport configuration for better reliability\n",
    "        transport_config={\n",
    "            \"timeout\": 10,           # 10 second timeout\n",
    "            \"retry_attempts\": 2,     # Retry failed requests 2 times\n",
    "            \"batch_size\": 10,        # Smaller batches for demo\n",
    "            \"batch_timeout\": 2.0,    # Send batches every 2 seconds\n",
    "            \"compression\": False     # Disable compression for debugging\n",
    "        },\n",
    "        \n",
    "        # Tracing configuration\n",
    "        tracing_config={\n",
    "            \"sample_rate\": 1.0,      # Trace 100% of operations\n",
    "            \"capture_errors\": True,\n",
    "            \"capture_stack_traces\": True\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Noveum Trace SDK initialized successfully!\")\n",
    "    print(\"üìä Project: jupyter-test-project\")\n",
    "    print(\"üîß Environment: development\") \n",
    "    print(f\"üåê Endpoint: {ENDPOINT}/v1/traces (auto-appended)\")\n",
    "    print(\"üîç Debug logging enabled - check console for HTTP request details\")\n",
    "    \n",
    "    # Get the current configuration to verify settings\n",
    "    config = noveum_trace.get_config()\n",
    "    print(f\"üìã Config verified - Endpoint: {config.transport.endpoint}\")\n",
    "    print(f\"üì¶ Batch size: {config.transport.batch_size}\")\n",
    "    print(f\"‚è±Ô∏è  Batch timeout: {config.transport.batch_timeout}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing SDK: {e}\")\n",
    "    print(\"Continuing with demo - traces will be logged locally\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: LLM Call Tracing\n",
    "\n",
    "Test LLM call tracing with the `@trace_llm` decorator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:noveum_trace.transport.http_transport:HTTP transport flush completed\n",
      "INFO:noveum_trace.core.client:Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ LLM Call Tracing (call_language_model) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER LLM CALL TRACING\n",
    "# This ensures the @trace_llm decorator traces are sent immediately\n",
    "\n",
    "flush_traces(\"LLM Call Tracing (call_language_model)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: e44bb7d3-5e7f-4628-ba73-6967bcc4079b\n",
      "DEBUG:noveum_trace.core.client:Started span: 64343ce6-19d6-4f2a-8014-c23cf8dfc319 in trace: e44bb7d3-5e7f-4628-ba73-6967bcc4079b\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-863b5391-c6e4-43fd-a05d-ea0722156917', 'json_data': {'messages': [{'role': 'user', 'content': 'Explain the benefits of observability in AI systems.'}], 'model': 'gpt-4', 'max_tokens': 100}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x115d89590>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x10732b530> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x113ceb950>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Calling gpt-4 with prompt: Explain the benefits of observability in AI system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 19 Jul 2025 19:31:03 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'magicapi-inc'), (b'openai-processing-ms', b'3255'), (b'openai-project', b'proj_5py2FGvCF1HwegoK03gVwlEm'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'3259'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'39985'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'22ms'), (b'x-request-id', b'req_812e8f22c5cc77585c07cc436434dda9'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=nQutduGj.hXxDdQQk8kWHzOlrrcyPlSrQXOmogjVXNw-1752953463-1.0.1.1-Cnmnek5Jmd563grsDA0HyXtJJRYIzks2s5Yy6b4zOn7OYqslD09k_u1rLWRsGbWpcvTPNfmHYlXSvBSaLsLUaQ7_os2L5LCrv6gm2kvviJQ; path=/; expires=Sat, 19-Jul-25 20:01:03 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=idYjzUWGwGlCTcfYUW6fhrphcUV4LUNgCWFxfEIuagQ-1752953463493-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'961ca2f40eb07f5c-MAA'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sat, 19 Jul 2025 19:31:03 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'magicapi-inc'), ('openai-processing-ms', '3255'), ('openai-project', 'proj_5py2FGvCF1HwegoK03gVwlEm'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '3259'), ('x-ratelimit-limit-requests', '5000'), ('x-ratelimit-limit-tokens', '40000'), ('x-ratelimit-remaining-requests', '4999'), ('x-ratelimit-remaining-tokens', '39985'), ('x-ratelimit-reset-requests', '12ms'), ('x-ratelimit-reset-tokens', '22ms'), ('x-request-id', 'req_812e8f22c5cc77585c07cc436434dda9'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=nQutduGj.hXxDdQQk8kWHzOlrrcyPlSrQXOmogjVXNw-1752953463-1.0.1.1-Cnmnek5Jmd563grsDA0HyXtJJRYIzks2s5Yy6b4zOn7OYqslD09k_u1rLWRsGbWpcvTPNfmHYlXSvBSaLsLUaQ7_os2L5LCrv6gm2kvviJQ; path=/; expires=Sat, 19-Jul-25 20:01:03 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=idYjzUWGwGlCTcfYUW6fhrphcUV4LUNgCWFxfEIuagQ-1752953463493-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '961ca2f40eb07f5c-MAA'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_812e8f22c5cc77585c07cc436434dda9\n",
      "DEBUG:noveum_trace.core.client:Finished span: 64343ce6-19d6-4f2a-8014-c23cf8dfc319\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace e44bb7d3-5e7f-4628-ba73-6967bcc4079b queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: e44bb7d3-5e7f-4628-ba73-6967bcc4079b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ LLM Response: 1. Improved Decision Making: Observability in AI helps understand the process of decision-making. It provides more insight into the mechanisms that AI uses to reach a conclusion, which can help improve the accuracy and effectiveness of the AI's function.\n",
      "\n",
      "2. Troubleshooting and Root Cause Analysis: With observability, one can easily detect and diagnose the problems in the AI system. It helps identify the root cause of any issue in the system, making it convenient to rectify the error and prevent its reocc\n"
     ]
    }
   ],
   "source": [
    "# Mock LLM responses for testing (replace with actual API calls if you have keys)\n",
    "def mock_openai_call(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"Mock OpenAI API call for testing.\"\"\"\n",
    "    responses = [\n",
    "        \"This is a mock response from the language model.\",\n",
    "        \"Here's a simulated AI response for testing purposes.\",\n",
    "        \"Mock LLM output to demonstrate tracing functionality.\"\n",
    "    ]\n",
    "    time.sleep(0.3)  # Simulate API call latency\n",
    "    return random.choice(responses)\n",
    "\n",
    "@trace_llm\n",
    "def call_language_model(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"Call a language model with tracing.\"\"\"\n",
    "    print(f\"ü§ñ Calling {model} with prompt: {prompt[:50]}...\")\n",
    "\n",
    "    # Use real OpenAI API if available, otherwise use mock\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        try:\n",
    "            import openai\n",
    "            client = openai.OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  OpenAI API call failed: {e}. Using mock response.\")\n",
    "            return mock_openai_call(prompt, model)\n",
    "    else:\n",
    "        print(\"üìù Using mock LLM response (no API key provided)\")\n",
    "        return mock_openai_call(prompt, model)\n",
    "\n",
    "# Test LLM tracing\n",
    "prompt = \"Explain the benefits of observability in AI systems.\"\n",
    "response = call_language_model(prompt)\n",
    "print(f\"\\nüéØ LLM Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:noveum_trace.transport.http_transport:HTTP transport flush completed\n",
      "INFO:noveum_trace.core.client:Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Enhanced LLM Tracing (Anthropic + OpenAI + Google) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER ENHANCED LLM TRACING\n",
    "# This ensures all enhanced LLM traces (Anthropic, OpenAI, Google) are sent immediately\n",
    "\n",
    "flush_traces(\"Enhanced LLM Tracing (Anthropic + OpenAI + Google)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5.1: Enhanced LLM Tracing Examples\n",
    "\n",
    "Demonstrate various LLM tracing features including different providers, metadata, and advanced parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:noveum_trace.transport.http_transport:HTTP transport flush completed\n",
      "INFO:noveum_trace.core.client:Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Retrieval System Tracing (Vector + Keyword + Hybrid) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER RETRIEVAL SYSTEM TRACING  \n",
    "# This ensures all @trace_retrieval decorator traces are sent immediately\n",
    "\n",
    "flush_traces(\"Retrieval System Tracing (Vector + Keyword + Hybrid)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: b9cdd133-5987-4fb7-b7d5-ad9eedf9a403\n",
      "DEBUG:noveum_trace.core.client:Started span: b503a69a-c088-4270-8866-d097bfe42cae in trace: b9cdd133-5987-4fb7-b7d5-ad9eedf9a403\n",
      "DEBUG:noveum_trace.core.client:Finished span: b503a69a-c088-4270-8866-d097bfe42cae\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace b9cdd133-5987-4fb7-b7d5-ad9eedf9a403 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: b9cdd133-5987-4fb7-b7d5-ad9eedf9a403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing Enhanced LLM Tracing...\n",
      "üß† Calling claude-3-haiku with prompt: What are the key benefits of AI observability?...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Test various LLM providers\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mü§ñ Testing Enhanced LLM Tracing...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m anthropic_response = \u001b[43mcall_anthropic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat are the key benefits of AI observability?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müß† Anthropic Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manthropic_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m metadata_response = call_llm_with_metadata(\u001b[33m\"\u001b[39m\u001b[33mSummarize the importance of AI monitoring\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Noveum/noveum-trace/src/noveum_trace/decorators/llm.py:169\u001b[39m, in \u001b[36mtrace_llm.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m attach_context_to_span(span)\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Execute the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# Capture completions if enabled\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m capture_completions:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mcall_anthropic\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müß† Calling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt[:\u001b[32m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Mock Anthropic response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtime\u001b[49m.sleep(\u001b[32m0.4\u001b[39m)\n\u001b[32m     11\u001b[39m responses = [\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mObservability in AI systems provides critical insights into model behavior and performance.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTracing AI workflows enables debugging, optimization, and compliance monitoring.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mComprehensive monitoring helps ensure AI system reliability and user trust.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m ]\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m random.choice(responses)\n",
      "\u001b[31mNameError\u001b[39m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# Enhanced LLM Tracing Examples\n",
    "\n",
    "# Test different LLM providers with comprehensive metadata\n",
    "@trace_llm(provider=\"anthropic\", capture_tokens=True, estimate_costs=True)\n",
    "def call_anthropic(prompt: str, model: str = \"claude-3-haiku\") -> str:\n",
    "    \"\"\"Call Anthropic Claude with tracing.\"\"\"\n",
    "    print(f\"üß† Calling {model} with prompt: {prompt[:50]}...\")\n",
    "    \n",
    "    # Mock Anthropic response\n",
    "    time.sleep(0.4)\n",
    "    responses = [\n",
    "        \"Observability in AI systems provides critical insights into model behavior and performance.\",\n",
    "        \"Tracing AI workflows enables debugging, optimization, and compliance monitoring.\",\n",
    "        \"Comprehensive monitoring helps ensure AI system reliability and user trust.\"\n",
    "    ]\n",
    "    return random.choice(responses)\n",
    "\n",
    "# Test with custom metadata and tags\n",
    "@trace_llm(\n",
    "    provider=\"openai\", \n",
    "    capture_prompts=True, \n",
    "    capture_completions=True,\n",
    "    metadata={\"experiment\": \"demo\", \"version\": \"1.0\"},\n",
    "    tags={\"environment\": \"notebook\", \"user\": \"demo\"}\n",
    ")\n",
    "def call_llm_with_metadata(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"LLM call with custom metadata and tags.\"\"\"\n",
    "    print(f\"üìã Enhanced LLM call with metadata: {prompt[:40]}...\")\n",
    "    time.sleep(0.3)\n",
    "    return f\"Enhanced response for: {prompt[:20]}...\"\n",
    "\n",
    "# Test Google AI provider\n",
    "@trace_llm(provider=\"google\", capture_tokens=True, redact_pii=True)\n",
    "def call_google_ai(prompt: str, model: str = \"gemini-pro\") -> str:\n",
    "    \"\"\"Call Google AI with PII redaction.\"\"\"\n",
    "    print(f\"üü¢ Calling {model} with PII protection: {prompt[:40]}...\")\n",
    "    time.sleep(0.5)\n",
    "    return \"Google AI response with PII redaction enabled for sensitive data handling.\"\n",
    "\n",
    "# Test various LLM providers\n",
    "print(\"ü§ñ Testing Enhanced LLM Tracing...\")\n",
    "\n",
    "anthropic_response = call_anthropic(\"What are the key benefits of AI observability?\")\n",
    "print(f\"\\nüß† Anthropic Response: {anthropic_response}\")\n",
    "\n",
    "metadata_response = call_llm_with_metadata(\"Summarize the importance of AI monitoring\")\n",
    "print(f\"\\nüìã Enhanced Response: {metadata_response}\")\n",
    "\n",
    "google_response = call_google_ai(\"How does tracing help with AI compliance?\")\n",
    "print(f\"\\nüü¢ Google AI Response: {google_response}\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced LLM tracing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:noveum_trace.transport.http_transport:HTTP transport flush completed\n",
      "INFO:noveum_trace.core.client:Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Enhanced Multi-Agent System (Data Analyst + Content Curator + Synthesis + Orchestrator) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER ENHANCED MULTI-AGENT SYSTEM TRACING\n",
    "# This ensures all @trace_agent decorator traces are sent immediately\n",
    "\n",
    "flush_traces(\"Enhanced Multi-Agent System (Data Analyst + Content Curator + Synthesis + Orchestrator)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5.2: Retrieval System Tracing\n",
    "\n",
    "Test retrieval operations with the `@trace_retrieval` decorator for RAG systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:noveum_trace.transport.http_transport:HTTP transport flush completed\n",
      "INFO:noveum_trace.core.client:Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Context Managers and Streaming (trace_llm_call + trace_agent_operation + trace_operation + streaming) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER CONTEXT MANAGERS AND STREAMING\n",
    "# This ensures all context manager traces are sent immediately\n",
    "\n",
    "flush_traces(\"Context Managers and Streaming (trace_llm_call + trace_agent_operation + trace_operation + streaming)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: 8d691518-5983-4eda-85c0-64964c7faadc\n",
      "DEBUG:noveum_trace.core.client:Started span: ee709a5e-f8c9-41ea-a74d-9b34e73964f8 in trace: 8d691518-5983-4eda-85c0-64964c7faadc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Retrieval System Tracing...\n",
      "üîç Vector Search: Finding top 3 results for 'benefits of AI observability'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: ee709a5e-f8c9-41ea-a74d-9b34e73964f8\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 8d691518-5983-4eda-85c0-64964c7faadc queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 8d691518-5983-4eda-85c0-64964c7faadc\n",
      "DEBUG:noveum_trace.core.client:Started trace: b6e3f513-edec-4c18-bc53-8ad58295eccb\n",
      "DEBUG:noveum_trace.core.client:Started span: f40f6a43-282f-49e9-a4b3-0fb1482a6cf2 in trace: b6e3f513-edec-4c18-bc53-8ad58295eccb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      "üîç Vector Search Results: 3 documents\n",
      "üîé Keyword Search: 'AI monitoring' with filters: {'category': 'technical', 'year': 2024}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: f40f6a43-282f-49e9-a4b3-0fb1482a6cf2\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace b6e3f513-edec-4c18-bc53-8ad58295eccb queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: b6e3f513-edec-4c18-bc53-8ad58295eccb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Keyword Search Results: 3 matches\n",
      "\n",
      "‚úÖ Retrieval tracing completed!\n"
     ]
    }
   ],
   "source": [
    "# Import the retrieval decorator\n",
    "from noveum_trace import trace_retrieval\n",
    "\n",
    "# Vector search with comprehensive tracing\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"vector_search\",\n",
    "    index_name=\"knowledge_base\",\n",
    "    capture_query=True,\n",
    "    capture_results=True,\n",
    "    capture_scores=True,\n",
    "    metadata={\"index_version\": \"v2.1\", \"embedding_model\": \"text-embedding-ada-002\"}\n",
    ")\n",
    "def vector_search(query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Perform vector search with tracing.\"\"\"\n",
    "    print(f\"üîç Vector Search: Finding top {top_k} results for '{query}'\")\n",
    "    \n",
    "    # Simulate vector search\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    # Mock search results with scores\n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        results.append({\n",
    "            \"document_id\": f\"doc_{i+1}\",\n",
    "            \"content\": f\"Relevant content for '{query}' - document {i+1}\",\n",
    "            \"score\": 0.95 - (i * 0.1),\n",
    "            \"metadata\": {\"source\": f\"source_{i+1}.pdf\", \"page\": i+1}\n",
    "        })\n",
    "    \n",
    "    search_result = {\n",
    "        \"query\": query,\n",
    "        \"total_results\": top_k,\n",
    "        \"results\": results,\n",
    "        \"search_time_ms\": 300,\n",
    "        \"index_stats\": {\"total_docs\": 10000, \"dimensions\": 1536}\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(results)} relevant documents\")\n",
    "    return search_result\n",
    "\n",
    "# Keyword search with metadata capture\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"keyword_search\",\n",
    "    index_name=\"text_corpus\",\n",
    "    capture_metadata=True,\n",
    "    tags={\"search_type\": \"fulltext\", \"language\": \"en\"}\n",
    ")\n",
    "def keyword_search(query: str, filters: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Perform keyword search with filtering.\"\"\"\n",
    "    print(f\"üîé Keyword Search: '{query}' with filters: {filters}\")\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    # Mock keyword search results\n",
    "    results = [\n",
    "        {\"doc_id\": \"kw_1\", \"title\": \"AI Observability Guide\", \"snippet\": \"...observability in AI...\"},\n",
    "        {\"doc_id\": \"kw_2\", \"title\": \"Tracing Best Practices\", \"snippet\": \"...tracing methodologies...\"},\n",
    "        {\"doc_id\": \"kw_3\", \"title\": \"Monitoring AI Systems\", \"snippet\": \"...monitoring strategies...\"}\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"filters\": filters or {},\n",
    "        \"results\": results,\n",
    "        \"total_matches\": len(results)\n",
    "    }\n",
    "\n",
    "# Test retrieval operations\n",
    "print(\"üîç Testing Retrieval System Tracing...\")\n",
    "\n",
    "# Test vector search\n",
    "vector_result = vector_search(\"benefits of AI observability\", top_k=3)\n",
    "print(f\"\\nüîç Vector Search Results: {len(vector_result['results'])} documents\")\n",
    "\n",
    "# Test keyword search with filters\n",
    "keyword_result = keyword_search(\"AI monitoring\", filters={\"category\": \"technical\", \"year\": 2024})\n",
    "print(f\"\\nüîé Keyword Search Results: {keyword_result['total_matches']} matches\")\n",
    "\n",
    "print(\"\\n‚úÖ Retrieval tracing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6.1: Enhanced Multi-Agent System\n",
    "\n",
    "Test advanced multi-agent workflows with specialized agents and complex coordination patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: b08455d3-5ddb-4cdd-b139-0b6eebf38cda\n",
      "DEBUG:noveum_trace.core.client:Started span: 0ea8b562-a000-4657-af18-0a22cc1090ac in trace: b08455d3-5ddb-4cdd-b139-0b6eebf38cda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing Enhanced Multi-Agent System...\n",
      "üìä Data Analyst: Analyzing dataset with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 0ea8b562-a000-4657-af18-0a22cc1090ac\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace b08455d3-5ddb-4cdd-b139-0b6eebf38cda queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: b08455d3-5ddb-4cdd-b139-0b6eebf38cda\n",
      "DEBUG:noveum_trace.core.client:Started trace: 18454ea0-845b-4b1e-84d6-e759b19d7c13\n",
      "DEBUG:noveum_trace.core.client:Started span: df81263e-bc0a-4f26-94d9-1528842f5e8a in trace: 18454ea0-845b-4b1e-84d6-e759b19d7c13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis complete: 0.92 quality score\n",
      "\n",
      "üìä Data Analysis: 0.92 quality score\n",
      "üìù Content Curator: Processing 4 content items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: df81263e-bc0a-4f26-94d9-1528842f5e8a\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 18454ea0-845b-4b1e-84d6-e759b19d7c13 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 18454ea0-845b-4b1e-84d6-e759b19d7c13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Curated 3/4 items\n",
      "\n",
      "üìù Content Curation: 3/4 items retained\n",
      "\n",
      "‚úÖ Enhanced multi-agent system testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Multi-Agent System Examples\n",
    "# Import required types to ensure they're available in this cell\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Specialized agents with different roles and capabilities\n",
    "@trace_agent(\n",
    "    agent_id=\"data_analyst\",\n",
    "    role=\"analyst\",\n",
    "    agent_type=\"specialist\",\n",
    "    capabilities=[\"data_analysis\", \"statistical_modeling\", \"visualization\"],\n",
    "    capture_reasoning=True,\n",
    "    metadata={\"specialization\": \"quantitative_analysis\", \"confidence_threshold\": 0.8}\n",
    ")\n",
    "def data_analyst_agent(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Specialized data analysis agent.\"\"\"\n",
    "    print(f\"üìä Data Analyst: Analyzing dataset with {len(data.get('samples', []))} samples\")\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Mock data analysis\n",
    "    analysis = {\n",
    "        \"agent_id\": \"data_analyst\",\n",
    "        \"analysis_type\": \"quantitative\",\n",
    "        \"findings\": {\n",
    "            \"data_quality\": 0.92,\n",
    "            \"pattern_confidence\": 0.87,\n",
    "            \"anomalies_detected\": 3,\n",
    "            \"recommendations\": [\n",
    "                \"Data quality is high with 92% confidence\",\n",
    "                \"3 anomalies detected requiring investigation\",\n",
    "                \"Statistical patterns show strong correlation\"\n",
    "            ]\n",
    "        },\n",
    "        \"reasoning_steps\": [\n",
    "            \"Loaded and validated input data\",\n",
    "            \"Applied statistical analysis methods\", \n",
    "            \"Identified patterns and anomalies\",\n",
    "            \"Generated confidence-based recommendations\"\n",
    "        ],\n",
    "        \"processing_time\": 0.5\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Analysis complete: {analysis['findings']['data_quality']:.2f} quality score\")\n",
    "    return analysis\n",
    "\n",
    "@trace_agent(\n",
    "    agent_id=\"content_curator\",\n",
    "    role=\"curator\",\n",
    "    agent_type=\"content_specialist\", \n",
    "    capabilities=[\"content_filtering\", \"quality_assessment\", \"summarization\"],\n",
    "    capture_tools=True\n",
    ")\n",
    "def content_curator_agent(content_list: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Content curation and quality assessment agent.\"\"\"\n",
    "    print(f\"üìù Content Curator: Processing {len(content_list)} content items\")\n",
    "    \n",
    "    time.sleep(0.4)\n",
    "    \n",
    "    # Mock content curation using tools\n",
    "    high_quality_content = []\n",
    "    for i, content in enumerate(content_list):\n",
    "        if i < 3:  # Mock: keep first 3 as high quality\n",
    "            high_quality_content.append({\n",
    "                **content,\n",
    "                \"quality_score\": 0.9 - (i * 0.05),\n",
    "                \"curation_reason\": \"Meets quality standards\"\n",
    "            })\n",
    "    \n",
    "    curation_result = {\n",
    "        \"agent_id\": \"content_curator\",\n",
    "        \"input_count\": len(content_list),\n",
    "        \"curated_count\": len(high_quality_content),\n",
    "        \"curated_content\": high_quality_content,\n",
    "        \"tools_used\": [\"quality_scorer\", \"content_filter\", \"summarizer\"],\n",
    "        \"curation_metrics\": {\n",
    "            \"retention_rate\": len(high_quality_content) / len(content_list),\n",
    "            \"average_quality\": sum(item[\"quality_score\"] for item in high_quality_content) / len(high_quality_content)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Curated {len(high_quality_content)}/{len(content_list)} items\")\n",
    "    return curation_result\n",
    "\n",
    "# Test the enhanced multi-agent system\n",
    "print(\"ü§ñ Testing Enhanced Multi-Agent System...\")\n",
    "\n",
    "# Mock data for demonstration\n",
    "sample_data = {\"samples\": [f\"sample_{i}\" for i in range(100)]}\n",
    "sample_content = [\n",
    "    {\"id\": 1, \"title\": \"AI Observability\", \"content\": \"Content about observability\"},\n",
    "    {\"id\": 2, \"title\": \"Tracing Systems\", \"content\": \"Content about tracing\"},\n",
    "    {\"id\": 3, \"title\": \"Monitoring Tools\", \"content\": \"Content about monitoring\"},\n",
    "    {\"id\": 4, \"title\": \"Low Quality\", \"content\": \"Poor content\"}\n",
    "]\n",
    "\n",
    "# Test individual agents\n",
    "analyst_result = data_analyst_agent(sample_data)\n",
    "print(f\"\\nüìä Data Analysis: {analyst_result['findings']['data_quality']:.2f} quality score\")\n",
    "\n",
    "curator_result = content_curator_agent(sample_content)\n",
    "print(f\"\\nüìù Content Curation: {curator_result['curated_count']}/{curator_result['input_count']} items retained\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced multi-agent system testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7.1: Context Managers and Streaming\n",
    "\n",
    "Test context managers for inline tracing and streaming LLM responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: e2873aac-2abd-4aa5-9702-1948f1a93847\n",
      "DEBUG:noveum_trace.core.client:Started span: a517c511-81b0-4105-9455-3ec4da2a3fdf in trace: e2873aac-2abd-4aa5-9702-1948f1a93847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Context Managers...\n",
      "\\n1Ô∏è‚É£ Context Manager Examples:\n",
      "üîÑ Processing user query: 'What are the benefits of AI observabilit...'\n",
      "ü§ñ Making LLM call within context manager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: a517c511-81b0-4105-9455-3ec4da2a3fdf\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace e2873aac-2abd-4aa5-9702-1948f1a93847 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: e2873aac-2abd-4aa5-9702-1948f1a93847\n",
      "DEBUG:noveum_trace.core.client:Started trace: f499b45b-9753-48a3-849d-7b356391eed4\n",
      "DEBUG:noveum_trace.core.client:Started span: c56bdd26-1c21-447b-83e6-45622ec13751 in trace: f499b45b-9753-48a3-849d-7b356391eed4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM response generated: 57 characters\n",
      "üì§ Final response: Final: Processed response for: what are the benefi...\n",
      "ü§ñ Agent Task: 'Analyze system performance metrics'\n",
      "‚öôÔ∏è  Executing agent task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: c56bdd26-1c21-447b-83e6-45622ec13751\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace f499b45b-9753-48a3-849d-7b356391eed4 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: f499b45b-9753-48a3-849d-7b356391eed4\n",
      "DEBUG:noveum_trace.core.client:Started trace: 51348c5a-ff42-443b-81de-90945a4d5797\n",
      "DEBUG:noveum_trace.core.client:Started span: 3a720185-4daf-4717-8e7f-902ab6e7d8fc in trace: 51348c5a-ff42-443b-81de-90945a4d5797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent task completed with 95.0% success rate\n",
      "üîß Starting complex operation...\n",
      "üì• Step 1: Loading data...\n",
      "‚öôÔ∏è  Step 2: Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 3a720185-4daf-4717-8e7f-902ab6e7d8fc\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 51348c5a-ff42-443b-81de-90945a4d5797 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 51348c5a-ff42-443b-81de-90945a4d5797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Step 3: Generating output...\n",
      "‚úÖ Complex operation completed successfully\n",
      "\\n‚úÖ Context managers testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Import context managers and streaming features\n",
    "from noveum_trace import (\n",
    "    trace_llm_call, trace_agent_operation, trace_operation, \n",
    "    streaming_llm, trace_streaming, ThreadContext\n",
    ")\n",
    "from typing import Dict, Any, Iterator\n",
    "\n",
    "# Context Manager Examples - Inline Tracing\n",
    "\n",
    "def process_user_query_with_context_managers(user_input: str) -> str:\n",
    "    \"\"\"Demonstrate inline tracing with context managers.\"\"\"\n",
    "    print(f\"üîÑ Processing user query: '{user_input[:40]}...'\")\n",
    "    \n",
    "    # Some preprocessing (not traced)\n",
    "    cleaned_input = user_input.strip().lower()\n",
    "    \n",
    "    # Trace just the LLM call using context manager\n",
    "    with trace_llm_call(model=\"gpt-4\", provider=\"openai\", operation=\"query_processing\") as span:\n",
    "        print(\"ü§ñ Making LLM call within context manager...\")\n",
    "        time.sleep(0.4)\n",
    "        \n",
    "        # Mock LLM response\n",
    "        response = f\"Processed response for: {cleaned_input[:30]}...\"\n",
    "        \n",
    "        # Add custom attributes to the span\n",
    "        span.set_attributes({\n",
    "            \"llm.input_length\": len(cleaned_input),\n",
    "            \"llm.output_length\": len(response),\n",
    "            \"llm.processing_type\": \"query_understanding\"\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ LLM response generated: {len(response)} characters\")\n",
    "    \n",
    "    # Post-processing (not traced)\n",
    "    final_response = f\"Final: {response}\"\n",
    "    print(f\"üì§ Final response: {final_response[:50]}...\")\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "# Agent operation context manager\n",
    "def agent_task_with_context_manager(task: str) -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate agent operation tracing with context manager.\"\"\"\n",
    "    print(f\"ü§ñ Agent Task: '{task}'\")\n",
    "    \n",
    "    with trace_agent_operation(\n",
    "        agent_type=\"task_agent\", \n",
    "        operation=\"task_execution\",\n",
    "        capabilities=[\"task_planning\", \"execution\", \"monitoring\"]\n",
    "    ) as span:\n",
    "        print(\"‚öôÔ∏è  Executing agent task...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Mock agent work\n",
    "        result = {\n",
    "            \"task\": task,\n",
    "            \"status\": \"completed\",\n",
    "            \"steps_executed\": 5,\n",
    "            \"success_rate\": 0.95\n",
    "        }\n",
    "        \n",
    "        # Add agent-specific attributes\n",
    "        span.set_attributes({\n",
    "            \"agent.task_complexity\": \"medium\",\n",
    "            \"agent.steps_executed\": result[\"steps_executed\"],\n",
    "            \"agent.success_rate\": result[\"success_rate\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Agent task completed with {result['success_rate']:.1%} success rate\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Generic operation context manager (FIXED SYNTAX)\n",
    "def complex_operation_with_tracing() -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate generic operation tracing.\"\"\"\n",
    "    print(\"üîß Starting complex operation...\")\n",
    "    \n",
    "    # CORRECT SYNTAX: trace_operation(operation_name, attributes=...)\n",
    "    with trace_operation(\"complex_data_processing\", \n",
    "                        attributes={\"operation_type\": \"data_pipeline\", \"complexity\": \"high\"}) as span:\n",
    "        # Step 1: Data loading\n",
    "        print(\"üì• Step 1: Loading data...\")\n",
    "        time.sleep(0.2)\n",
    "        span.set_attributes({\"step\": \"data_loading\", \"records_loaded\": 1000})\n",
    "        \n",
    "        # Step 2: Processing\n",
    "        print(\"‚öôÔ∏è  Step 2: Processing data...\")\n",
    "        time.sleep(0.3)\n",
    "        span.set_attributes({\"step\": \"processing\", \"records_processed\": 950})\n",
    "        \n",
    "        # Step 3: Output\n",
    "        print(\"üì§ Step 3: Generating output...\")\n",
    "        time.sleep(0.1)\n",
    "        span.set_attributes({\"step\": \"output\", \"records_output\": 950})\n",
    "        \n",
    "        result = {\n",
    "            \"operation\": \"complex_data_processing\",\n",
    "            \"input_records\": 1000,\n",
    "            \"processed_records\": 950,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Complex operation completed successfully\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test context managers\n",
    "print(\"üîÑ Testing Context Managers...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Context Manager Examples:\")\n",
    "query_result = process_user_query_with_context_managers(\"What are the benefits of AI observability?\")\n",
    "\n",
    "agent_result = agent_task_with_context_manager(\"Analyze system performance metrics\")\n",
    "\n",
    "operation_result = complex_operation_with_tracing()\n",
    "\n",
    "print(\"\\\\n‚úÖ Context managers testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7.2: Auto-Instrumentation and Advanced Features\n",
    "\n",
    "Test auto-instrumentation, proxy objects, and advanced SDK features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "/var/folders/7d/c1z608491jq84m9f40phz5rh0000gn/T/ipykernel_46328/3792679582.py:25: UserWarning: Failed to instrument openai: Failed to instrument openai: Ambiguous use of module client; please set `openai.api_type` or the `OPENAI_API_TYPE` environment variable to `openai` or `azure`\n",
      "  auto_instrument(\"openai\")\n",
      "DEBUG:noveum_trace.core.client:Started trace: a63d4111-539e-4103-b6c5-f69cd87f9e35\n",
      "DEBUG:noveum_trace.core.client:Started span: bc4e62cc-c2f1-489e-8bb9-c78571f5ebd5 in trace: a63d4111-539e-4103-b6c5-f69cd87f9e35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing Auto-Instrumentation and Advanced Features...\n",
      "\\n1Ô∏è‚É£ Auto-Instrumentation:\n",
      "üîß Testing Auto-Instrumentation...\n",
      "üì¶ Available instrumentations: ['openai', 'anthropic', 'langchain']\n",
      "üîå Enabling OpenAI auto-instrumentation...\n",
      "‚úÖ OpenAI auto-instrumentation enabled\n",
      "üîç Currently instrumented: []\n",
      "\\n2Ô∏è‚É£ Proxy Objects:\n",
      "üîÑ Testing Traced OpenAI Client...\n",
      "‚ÑπÔ∏è  Traced client demo: create_traced_openai_client() got an unexpected keyword argument 'api_key'\n",
      "ü§ñ Testing Traced Agent Proxy...\n",
      "‚úÖ Traced agent proxy created\n",
      "üß† Testing traced agent methods...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: bc4e62cc-c2f1-489e-8bb9-c78571f5ebd5\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace a63d4111-539e-4103-b6c5-f69cd87f9e35 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: a63d4111-539e-4103-b6c5-f69cd87f9e35\n",
      "DEBUG:noveum_trace.core.client:Started trace: 0d430ae2-ba17-41d6-85f4-3927eec068fa\n",
      "DEBUG:noveum_trace.core.client:Started span: 13f39075-bb24-4823-a4bf-50ddeb073967 in trace: 0d430ae2-ba17-41d6-85f4-3927eec068fa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí≠ Think result: Thinking about: How to improve AI observability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 13f39075-bb24-4823-a4bf-50ddeb073967\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 0d430ae2-ba17-41d6-85f4-3927eec068fa queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 0d430ae2-ba17-41d6-85f4-3927eec068fa\n",
      "DEBUG:noveum_trace.core.client:Started trace: dba07d70-2b05-4f4a-9456-42268d9430a9\n",
      "DEBUG:noveum_trace.core.client:Started span: 9fe0c703-fb4f-4a19-84a3-587135ef0756 in trace: dba07d70-2b05-4f4a-9456-42268d9430a9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Action result: Performing action: Implement monitoring dashboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 9fe0c703-fb4f-4a19-84a3-587135ef0756\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace dba07d70-2b05-4f4a-9456-42268d9430a9 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: dba07d70-2b05-4f4a-9456-42268d9430a9\n",
      "DEBUG:noveum_trace.core.client:Started trace: 2e0dfbda-b28b-4f3f-93bd-8a455b51d964\n",
      "DEBUG:noveum_trace.core.client:Started span: 4a530205-d24c-46ee-9652-64e06c5167d9 in trace: 2e0dfbda-b28b-4f3f-93bd-8a455b51d964\n",
      "DEBUG:noveum_trace.core.client:Finished span: 4a530205-d24c-46ee-9652-64e06c5167d9\n",
      "DEBUG:noveum_trace.core.client:Started span: 5ea813b8-b014-4d03-adb2-eb6356173b7b in trace: 2e0dfbda-b28b-4f3f-93bd-8a455b51d964\n",
      "DEBUG:noveum_trace.core.client:Finished span: 5ea813b8-b014-4d03-adb2-eb6356173b7b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Plan result: 3 steps\n",
      "\\n3Ô∏è‚É£ Manual Tracing:\n",
      "üîç Testing Manual Tracing...\n",
      "‚úÖ Started trace: 2e0dfbda-b28b-4f3f-93bd-8a455b51d964\n",
      "‚ÑπÔ∏è  Manual tracing demo: 'Trace' object has no attribute 'span'\n",
      "\\n4Ô∏è‚É£ Error Handling:\n",
      "‚ö†Ô∏è  Testing Error Handling...\n",
      "‚úÖ Successful operation: Success!\n",
      "‚úÖ Error captured successfully: This is a demo error for testing\n",
      "\\n‚úÖ Advanced features testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Import advanced features\n",
    "from noveum_trace import (\n",
    "    auto_instrument, get_instrumented_libraries, is_instrumented,\n",
    "    create_traced_openai_client, create_traced_agent, TracedOpenAIClient,\n",
    "    start_trace, start_span, get_current_trace, get_current_span\n",
    ")\n",
    "\n",
    "# Auto-Instrumentation Examples\n",
    "\n",
    "def test_auto_instrumentation():\n",
    "    \"\"\"Test automatic instrumentation of libraries.\"\"\"\n",
    "    print(\"üîß Testing Auto-Instrumentation...\")\n",
    "    \n",
    "    # Check available instrumentations\n",
    "    try:\n",
    "        available = noveum_trace.get_available_instrumentations()\n",
    "        print(f\"üì¶ Available instrumentations: {available}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Available instrumentations: {e}\")\n",
    "    \n",
    "    # Enable auto-instrumentation for OpenAI (if not already enabled)\n",
    "    try:\n",
    "        if not is_instrumented(\"openai\"):\n",
    "            print(\"üîå Enabling OpenAI auto-instrumentation...\")\n",
    "            auto_instrument(\"openai\")\n",
    "            print(\"‚úÖ OpenAI auto-instrumentation enabled\")\n",
    "        else:\n",
    "            print(\"‚úÖ OpenAI already instrumented\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Auto-instrumentation: {e}\")\n",
    "    \n",
    "    # Check instrumented libraries\n",
    "    try:\n",
    "        instrumented = get_instrumented_libraries()\n",
    "        print(f\"üîç Currently instrumented: {instrumented}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Instrumented libraries: {e}\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Proxy Objects for Enhanced Control\n",
    "\n",
    "def test_traced_openai_client():\n",
    "    \"\"\"Test traced OpenAI client proxy.\"\"\"\n",
    "    print(\"üîÑ Testing Traced OpenAI Client...\")\n",
    "    \n",
    "    # Create traced OpenAI client (even without real API key)\n",
    "    try:\n",
    "        traced_client = create_traced_openai_client(\n",
    "            api_key=\"mock-key-for-demo\",\n",
    "            trace_completions=True,\n",
    "            trace_embeddings=True,\n",
    "            capture_content=True\n",
    "        )\n",
    "        print(\"‚úÖ Traced OpenAI client created\")\n",
    "        \n",
    "        # Mock a call (won't actually work without real API key)\n",
    "        print(\"ü§ñ Simulating traced OpenAI call...\")\n",
    "        # In real usage: response = traced_client.chat.completions.create(...)\n",
    "        print(\"‚ÑπÔ∏è  Would automatically trace all OpenAI API calls\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Traced client demo: {e}\")\n",
    "\n",
    "def test_traced_agent_proxy():\n",
    "    \"\"\"Test traced agent proxy for enhanced agent monitoring.\"\"\"\n",
    "    print(\"ü§ñ Testing Traced Agent Proxy...\")\n",
    "    \n",
    "    # Mock agent class\n",
    "    class MockAgent:\n",
    "        def __init__(self, name: str):\n",
    "            self.name = name\n",
    "        \n",
    "        def think(self, problem: str) -> str:\n",
    "            time.sleep(0.2)\n",
    "            return f\"Thinking about: {problem}\"\n",
    "        \n",
    "        def act(self, action: str) -> str:\n",
    "            time.sleep(0.3)\n",
    "            return f\"Performing action: {action}\"\n",
    "        \n",
    "        def plan(self, goal: str) -> List[str]:\n",
    "            time.sleep(0.4)\n",
    "            return [f\"Step 1 for {goal}\", f\"Step 2 for {goal}\", f\"Step 3 for {goal}\"]\n",
    "    \n",
    "    # Create traced agent proxy with CORRECT PARAMETERS\n",
    "    original_agent = MockAgent(\"demo_agent\")\n",
    "    traced_agent = create_traced_agent(\n",
    "        agent=original_agent,\n",
    "        agent_type=\"traced_demo_agent\",  # CORRECT: agent_type instead of agent_id\n",
    "        capabilities=[\"thinking\", \"acting\", \"planning\"],\n",
    "        trace_config={\"capture_inputs\": True, \"capture_outputs\": True}\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Traced agent proxy created\")\n",
    "    \n",
    "    # Test traced methods\n",
    "    print(\"üß† Testing traced agent methods...\")\n",
    "    \n",
    "    thought = traced_agent.think(\"How to improve AI observability\")\n",
    "    print(f\"üí≠ Think result: {thought}\")\n",
    "    \n",
    "    action = traced_agent.act(\"Implement monitoring dashboard\")\n",
    "    print(f\"‚ö° Action result: {action}\")\n",
    "    \n",
    "    plan = traced_agent.plan(\"Enhance system reliability\")\n",
    "    print(f\"üìã Plan result: {len(plan)} steps\")\n",
    "\n",
    "# Manual Span Creation and Management\n",
    "\n",
    "def test_manual_tracing():\n",
    "    \"\"\"Test manual trace and span creation.\"\"\"\n",
    "    print(\"üîç Testing Manual Tracing...\")\n",
    "    \n",
    "    try:\n",
    "        # Start a manual trace\n",
    "        trace = start_trace(\"manual_demo_trace\")\n",
    "        print(f\"‚úÖ Started trace: {trace.trace_id}\")\n",
    "        \n",
    "        # Create nested spans manually\n",
    "        with trace.span(\"parent_operation\") as parent_span:\n",
    "            parent_span.set_attributes({\n",
    "                \"operation.type\": \"parent\",\n",
    "                \"operation.importance\": \"high\"\n",
    "            })\n",
    "            print(\"üìä Parent span created\")\n",
    "            \n",
    "            # Child span 1\n",
    "            with parent_span.create_child_span(\"child_operation_1\") as child1:\n",
    "                child1.set_attributes({\n",
    "                    \"operation.type\": \"child\",\n",
    "                    \"child.number\": 1\n",
    "                })\n",
    "                time.sleep(0.2)\n",
    "                print(\"üîπ Child span 1 completed\")\n",
    "            \n",
    "            # Child span 2  \n",
    "            with parent_span.create_child_span(\"child_operation_2\") as child2:\n",
    "                child2.set_attributes({\n",
    "                    \"operation.type\": \"child\",\n",
    "                    \"child.number\": 2,\n",
    "                    \"child.data_processed\": 500\n",
    "                })\n",
    "                time.sleep(0.3)\n",
    "                print(\"üîπ Child span 2 completed\")\n",
    "            \n",
    "            print(\"üìä Parent operation completed\")\n",
    "        \n",
    "        # Finish trace\n",
    "        trace.finish()\n",
    "        print(f\"‚úÖ Manual trace completed: {trace.trace_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Manual tracing demo: {e}\")\n",
    "\n",
    "# Error Handling and Edge Cases\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test error handling and edge cases.\"\"\"\n",
    "    print(\"‚ö†Ô∏è  Testing Error Handling...\")\n",
    "    \n",
    "    # Test error capture in traced function\n",
    "    @noveum_trace.trace(capture_errors=True, capture_stack_trace=True)\n",
    "    def operation_with_error(should_fail: bool = False):\n",
    "        if should_fail:\n",
    "            raise ValueError(\"This is a demo error for testing\")\n",
    "        return \"Success!\"\n",
    "    \n",
    "    # Test successful operation\n",
    "    try:\n",
    "        result = operation_with_error(should_fail=False)\n",
    "        print(f\"‚úÖ Successful operation: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "    \n",
    "    # Test error capture\n",
    "    try:\n",
    "        result = operation_with_error(should_fail=True)\n",
    "        print(f\"Unexpected success: {result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"‚úÖ Error captured successfully: {e}\")\n",
    "\n",
    "# Run all advanced feature tests\n",
    "print(\"üöÄ Testing Auto-Instrumentation and Advanced Features...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Auto-Instrumentation:\")\n",
    "instrumented_libs = test_auto_instrumentation()\n",
    "\n",
    "print(\"\\\\n2Ô∏è‚É£ Proxy Objects:\")\n",
    "test_traced_openai_client()\n",
    "test_traced_agent_proxy()\n",
    "\n",
    "print(\"\\\\n3Ô∏è‚É£ Manual Tracing:\")\n",
    "test_manual_tracing()\n",
    "\n",
    "print(\"\\\\n4Ô∏è‚É£ Error Handling:\")\n",
    "test_error_handling()\n",
    "\n",
    "print(\"\\\\n‚úÖ Advanced features testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5.1: Enhanced LLM Tracing Examples\n",
    "\n",
    "Demonstrate various LLM tracing features including different providers, metadata, and advanced parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started span: cf6c8189-5654-484a-9954-720453617a36 in trace: 2e0dfbda-b28b-4f3f-93bd-8a455b51d964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing Enhanced LLM Tracing...\n",
      "üß† Calling claude-3-haiku with prompt: What are the key benefits of AI observability?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: cf6c8189-5654-484a-9954-720453617a36\n",
      "DEBUG:noveum_trace.core.client:Started span: a3da9f89-e154-4ba0-8d95-f12ccd27faa0 in trace: 2e0dfbda-b28b-4f3f-93bd-8a455b51d964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Anthropic Response: Comprehensive monitoring helps ensure AI system reliability and user trust.\n",
      "üìã Enhanced LLM call with metadata: Summarize the importance of AI monitorin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: a3da9f89-e154-4ba0-8d95-f12ccd27faa0\n",
      "DEBUG:noveum_trace.core.client:Started span: 1f400432-b762-4f94-bf9c-ee05098fbf89 in trace: 2e0dfbda-b28b-4f3f-93bd-8a455b51d964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Enhanced Response: Enhanced response for: Summarize the import...\n",
      "üü¢ Calling gemini-pro with PII protection: How does tracing help with AI compliance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 1f400432-b762-4f94-bf9c-ee05098fbf89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Google AI Response: Google AI response with PII redaction enabled for sensitive data handling.\n",
      "\n",
      "‚úÖ Enhanced LLM tracing completed!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced LLM Tracing Examples\n",
    "\n",
    "# Test different LLM providers with comprehensive metadata\n",
    "@trace_llm(provider=\"anthropic\", capture_tokens=True, estimate_costs=True)\n",
    "def call_anthropic(prompt: str, model: str = \"claude-3-haiku\") -> str:\n",
    "    \"\"\"Call Anthropic Claude with tracing.\"\"\"\n",
    "    print(f\"üß† Calling {model} with prompt: {prompt[:50]}...\")\n",
    "    \n",
    "    # Mock Anthropic response\n",
    "    time.sleep(0.4)\n",
    "    responses = [\n",
    "        \"Observability in AI systems provides critical insights into model behavior and performance.\",\n",
    "        \"Tracing AI workflows enables debugging, optimization, and compliance monitoring.\",\n",
    "        \"Comprehensive monitoring helps ensure AI system reliability and user trust.\"\n",
    "    ]\n",
    "    return random.choice(responses)\n",
    "\n",
    "# Test with custom metadata and tags\n",
    "@trace_llm(\n",
    "    provider=\"openai\", \n",
    "    capture_prompts=True, \n",
    "    capture_completions=True,\n",
    "    metadata={\"experiment\": \"demo\", \"version\": \"1.0\"},\n",
    "    tags={\"environment\": \"notebook\", \"user\": \"demo\"}\n",
    ")\n",
    "def call_llm_with_metadata(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"LLM call with custom metadata and tags.\"\"\"\n",
    "    print(f\"üìã Enhanced LLM call with metadata: {prompt[:40]}...\")\n",
    "    time.sleep(0.3)\n",
    "    return f\"Enhanced response for: {prompt[:20]}...\"\n",
    "\n",
    "# Test Google AI provider\n",
    "@trace_llm(provider=\"google\", capture_tokens=True, redact_pii=True)\n",
    "def call_google_ai(prompt: str, model: str = \"gemini-pro\") -> str:\n",
    "    \"\"\"Call Google AI with PII redaction.\"\"\"\n",
    "    print(f\"üü¢ Calling {model} with PII protection: {prompt[:40]}...\")\n",
    "    time.sleep(0.5)\n",
    "    return \"Google AI response with PII redaction enabled for sensitive data handling.\"\n",
    "\n",
    "# Test various LLM providers\n",
    "print(\"ü§ñ Testing Enhanced LLM Tracing...\")\n",
    "\n",
    "anthropic_response = call_anthropic(\"What are the key benefits of AI observability?\")\n",
    "print(f\"\\nüß† Anthropic Response: {anthropic_response}\")\n",
    "\n",
    "metadata_response = call_llm_with_metadata(\"Summarize the importance of AI monitoring\")\n",
    "print(f\"\\nüìã Enhanced Response: {metadata_response}\")\n",
    "\n",
    "google_response = call_google_ai(\"How does tracing help with AI compliance?\")\n",
    "print(f\"\\nüü¢ Google AI Response: {google_response}\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced LLM tracing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5.2: Retrieval System Tracing\n",
    "\n",
    "Test retrieval operations with the `@trace_retrieval` decorator for RAG systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.transport.http_transport:Trace 2e0dfbda-b28b-4f3f-93bd-8a455b51d964 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 2e0dfbda-b28b-4f3f-93bd-8a455b51d964\n",
      "DEBUG:urllib3.connectionpool:http://localhost:3000 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "DEBUG:noveum_trace.transport.http_transport:Successfully sent batch of 9 traces\n",
      "DEBUG:noveum_trace.transport.batch_processor:Sent batch of 9 traces\n",
      "INFO:noveum_trace.transport.http_transport:HTTP transport flush completed\n",
      "INFO:noveum_trace.core.client:Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Corrected trace_operation Examples (Fixed Syntax) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER CORRECTED TRACE_OPERATION EXAMPLES\n",
    "# This ensures all corrected context manager traces are sent immediately\n",
    "\n",
    "flush_traces(\"Corrected trace_operation Examples (Fixed Syntax)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: 3e4e26f0-ba34-4e6d-b349-c3621d25b2ad\n",
      "DEBUG:noveum_trace.core.client:Started span: 90d82ebd-0bd2-42bb-9a52-d2f2dd21c1c6 in trace: 3e4e26f0-ba34-4e6d-b349-c3621d25b2ad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Retrieval System Tracing...\n",
      "üîç Vector Search: Finding top 3 results for 'benefits of AI observability'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 90d82ebd-0bd2-42bb-9a52-d2f2dd21c1c6\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 3e4e26f0-ba34-4e6d-b349-c3621d25b2ad queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 3e4e26f0-ba34-4e6d-b349-c3621d25b2ad\n",
      "DEBUG:noveum_trace.core.client:Started trace: 3fb52d31-ba85-4ac6-a327-7c2a263a24aa\n",
      "DEBUG:noveum_trace.core.client:Started span: 69695bd5-f43a-4f89-881c-f6e013197f97 in trace: 3fb52d31-ba85-4ac6-a327-7c2a263a24aa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      "üîç Vector Search Results: 3 documents\n",
      "üîé Keyword Search: 'AI monitoring' with filters: {'category': 'technical', 'year': 2024}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 69695bd5-f43a-4f89-881c-f6e013197f97\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 3fb52d31-ba85-4ac6-a327-7c2a263a24aa queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 3fb52d31-ba85-4ac6-a327-7c2a263a24aa\n",
      "DEBUG:noveum_trace.core.client:Started trace: abaa1e3b-0561-41a1-930b-913d73260ba9\n",
      "DEBUG:noveum_trace.core.client:Started span: 927840ca-b6af-4af7-9911-9e4ed84a2620 in trace: abaa1e3b-0561-41a1-930b-913d73260ba9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Keyword Search Results: 3 matches\n",
      "üîó Hybrid Search: 'observability tracing systems' with alpha=0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started span: d928bc66-4c51-48bd-8879-4758ab1f2105 in trace: abaa1e3b-0561-41a1-930b-913d73260ba9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Vector Search: Finding top 3 results for 'observability tracing systems'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: d928bc66-4c51-48bd-8879-4758ab1f2105\n",
      "DEBUG:noveum_trace.core.client:Started span: f97b2f07-df1b-42ab-9478-cc493e5d4a8e in trace: abaa1e3b-0561-41a1-930b-913d73260ba9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 3 relevant documents\n",
      "üîé Keyword Search: 'observability tracing systems' with filters: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: f97b2f07-df1b-42ab-9478-cc493e5d4a8e\n",
      "DEBUG:noveum_trace.core.client:Finished span: 927840ca-b6af-4af7-9911-9e4ed84a2620\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace abaa1e3b-0561-41a1-930b-913d73260ba9 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: abaa1e3b-0561-41a1-930b-913d73260ba9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Hybrid Search Results: 2 combined results\n",
      "\n",
      "‚úÖ Retrieval tracing completed!\n"
     ]
    }
   ],
   "source": [
    "# Import the retrieval decorator and missing typing imports\n",
    "from noveum_trace import trace_retrieval\n",
    "from typing import Dict, Any, Optional, List, Iterator\n",
    "\n",
    "# Vector search with comprehensive tracing\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"vector_search\",\n",
    "    index_name=\"knowledge_base\",\n",
    "    capture_query=True,\n",
    "    capture_results=True,\n",
    "    capture_scores=True,\n",
    "    metadata={\"index_version\": \"v2.1\", \"embedding_model\": \"text-embedding-ada-002\"}\n",
    ")\n",
    "def vector_search(query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Perform vector search with tracing.\"\"\"\n",
    "    print(f\"üîç Vector Search: Finding top {top_k} results for '{query}'\")\n",
    "    \n",
    "    # Simulate vector search\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    # Mock search results with scores\n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        results.append({\n",
    "            \"document_id\": f\"doc_{i+1}\",\n",
    "            \"content\": f\"Relevant content for '{query}' - document {i+1}\",\n",
    "            \"score\": 0.95 - (i * 0.1),\n",
    "            \"metadata\": {\"source\": f\"source_{i+1}.pdf\", \"page\": i+1}\n",
    "        })\n",
    "    \n",
    "    search_result = {\n",
    "        \"query\": query,\n",
    "        \"total_results\": top_k,\n",
    "        \"results\": results,\n",
    "        \"search_time_ms\": 300,\n",
    "        \"index_stats\": {\"total_docs\": 10000, \"dimensions\": 1536}\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(results)} relevant documents\")\n",
    "    return search_result\n",
    "\n",
    "# Keyword search with metadata capture\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"keyword_search\",\n",
    "    index_name=\"text_corpus\",\n",
    "    capture_metadata=True,\n",
    "    tags={\"search_type\": \"fulltext\", \"language\": \"en\"}\n",
    ")\n",
    "def keyword_search(query: str, filters: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Perform keyword search with filtering.\"\"\"\n",
    "    print(f\"üîé Keyword Search: '{query}' with filters: {filters}\")\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    # Mock keyword search results\n",
    "    results = [\n",
    "        {\"doc_id\": \"kw_1\", \"title\": \"AI Observability Guide\", \"snippet\": \"...observability in AI...\"},\n",
    "        {\"doc_id\": \"kw_2\", \"title\": \"Tracing Best Practices\", \"snippet\": \"...tracing methodologies...\"},\n",
    "        {\"doc_id\": \"kw_3\", \"title\": \"Monitoring AI Systems\", \"snippet\": \"...monitoring strategies...\"}\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"filters\": filters or {},\n",
    "        \"results\": results,\n",
    "        \"total_matches\": len(results)\n",
    "    }\n",
    "\n",
    "# Hybrid search combining vector and keyword\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"hybrid_search\",\n",
    "    index_name=\"hybrid_index\",\n",
    "    capture_query=True,\n",
    "    capture_results=True,\n",
    "    capture_scores=True\n",
    ")\n",
    "def hybrid_search(query: str, alpha: float = 0.5) -> Dict[str, Any]:\n",
    "    \"\"\"Perform hybrid search combining vector and keyword search.\"\"\"\n",
    "    print(f\"üîó Hybrid Search: '{query}' with alpha={alpha}\")\n",
    "    \n",
    "    time.sleep(0.4)\n",
    "    \n",
    "    # Simulate hybrid search by combining both approaches\n",
    "    vector_results = vector_search(query, top_k=3)\n",
    "    keyword_results = keyword_search(query)\n",
    "    \n",
    "    # Mock hybrid ranking\n",
    "    hybrid_results = []\n",
    "    for i, result in enumerate(vector_results[\"results\"][:2]):\n",
    "        hybrid_results.append({\n",
    "            \"document_id\": result[\"document_id\"],\n",
    "            \"content\": result[\"content\"],\n",
    "            \"vector_score\": result[\"score\"],\n",
    "            \"keyword_score\": 0.8 - (i * 0.1),\n",
    "            \"combined_score\": (result[\"score\"] * alpha) + ((0.8 - i * 0.1) * (1 - alpha)),\n",
    "            \"source\": \"hybrid\"\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"alpha\": alpha,\n",
    "        \"results\": hybrid_results,\n",
    "        \"total_results\": len(hybrid_results),\n",
    "        \"search_strategy\": \"vector + keyword fusion\"\n",
    "    }\n",
    "\n",
    "# Test all retrieval operations\n",
    "print(\"üîç Testing Retrieval System Tracing...\")\n",
    "\n",
    "# Test vector search\n",
    "vector_result = vector_search(\"benefits of AI observability\", top_k=3)\n",
    "print(f\"\\nüîç Vector Search Results: {len(vector_result['results'])} documents\")\n",
    "\n",
    "# Test keyword search with filters\n",
    "keyword_result = keyword_search(\"AI monitoring\", filters={\"category\": \"technical\", \"year\": 2024})\n",
    "print(f\"\\nüîé Keyword Search Results: {keyword_result['total_matches']} matches\")\n",
    "\n",
    "# Test hybrid search\n",
    "hybrid_result = hybrid_search(\"observability tracing systems\", alpha=0.7)\n",
    "print(f\"\\nüîó Hybrid Search Results: {len(hybrid_result['results'])} combined results\")\n",
    "\n",
    "print(\"\\n‚úÖ Retrieval tracing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6.1: Enhanced Multi-Agent System\n",
    "\n",
    "Test advanced multi-agent workflows with specialized agents and complex coordination patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:3000 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "DEBUG:noveum_trace.transport.http_transport:Successfully sent batch of 1 traces\n",
      "DEBUG:noveum_trace.transport.batch_processor:Sent batch of 1 traces\n",
      "INFO:noveum_trace.transport.http_transport:HTTP transport flush completed\n",
      "INFO:noveum_trace.core.client:Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Corrected Proxy Objects (create_traced_agent + create_traced_openai_client) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER CORRECTED PROXY OBJECT EXAMPLES\n",
    "# This ensures all corrected proxy object traces are sent immediately\n",
    "\n",
    "flush_traces(\"Corrected Proxy Objects (create_traced_agent + create_traced_openai_client)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: 0fd3906b-3a45-4bd5-a4c9-a1df8c50f722\n",
      "DEBUG:noveum_trace.core.client:Started span: 963b00c4-9039-4c19-a586-56baa527afca in trace: 0fd3906b-3a45-4bd5-a4c9-a1df8c50f722\n",
      "DEBUG:noveum_trace.core.client:Started span: 437e5143-895c-428a-b45c-aaebd113f05f in trace: 0fd3906b-3a45-4bd5-a4c9-a1df8c50f722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing Enhanced Multi-Agent System...\n",
      "üé≠ Advanced Orchestrator: Managing workflow for 'Comprehensive analysis of AI system observability data and content'\n",
      "\n",
      "üîÑ Phase 1: Parallel Agent Execution\n",
      "üìä Data Analyst: Analyzing dataset with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 437e5143-895c-428a-b45c-aaebd113f05f\n",
      "DEBUG:noveum_trace.core.client:Started span: 708c0ec0-ec62-4b85-8483-08b3dc7209ef in trace: 0fd3906b-3a45-4bd5-a4c9-a1df8c50f722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis complete: 0.92 quality score\n",
      "üìù Content Curator: Processing 4 content items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 708c0ec0-ec62-4b85-8483-08b3dc7209ef\n",
      "DEBUG:noveum_trace.core.client:Started span: 5cbfe248-458b-45dc-a2e5-f3ae59e75b84 in trace: 0fd3906b-3a45-4bd5-a4c9-a1df8c50f722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Curated 3/4 items\n",
      "\\nüîó Phase 2: Synthesis and Integration\n",
      "üîó Synthesis Agent: Combining insights for context 'Comprehensive analysis of AI system observability data and content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 5cbfe248-458b-45dc-a2e5-f3ae59e75b84\n",
      "DEBUG:noveum_trace.core.client:Finished span: 963b00c4-9039-4c19-a586-56baa527afca\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 0fd3906b-3a45-4bd5-a4c9-a1df8c50f722 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 0fd3906b-3a45-4bd5-a4c9-a1df8c50f722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Synthesis complete with 0.86 confidence\n",
      "\\n‚úÖ Advanced orchestration complete!\n",
      "\\nüé≠ Enhanced Multi-Agent Results:\n",
      "Task: Comprehensive analysis of AI system observability data and content\n",
      "Agents: 3\n",
      "Phases: 2\n",
      "Final Confidence: 0.86\n",
      "Success: True\n",
      "\\n‚úÖ Enhanced multi-agent system testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Multi-Agent System Examples\n",
    "# Import missing typing if not already available\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Specialized agents with different roles and capabilities\n",
    "@trace_agent(\n",
    "    agent_id=\"data_analyst\",\n",
    "    role=\"analyst\",\n",
    "    agent_type=\"specialist\",\n",
    "    capabilities=[\"data_analysis\", \"statistical_modeling\", \"visualization\"],\n",
    "    capture_reasoning=True,\n",
    "    metadata={\"specialization\": \"quantitative_analysis\", \"confidence_threshold\": 0.8}\n",
    ")\n",
    "def data_analyst_agent(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Specialized data analysis agent.\"\"\"\n",
    "    print(f\"üìä Data Analyst: Analyzing dataset with {len(data.get('samples', []))} samples\")\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Mock data analysis\n",
    "    analysis = {\n",
    "        \"agent_id\": \"data_analyst\",\n",
    "        \"analysis_type\": \"quantitative\",\n",
    "        \"findings\": {\n",
    "            \"data_quality\": 0.92,\n",
    "            \"pattern_confidence\": 0.87,\n",
    "            \"anomalies_detected\": 3,\n",
    "            \"recommendations\": [\n",
    "                \"Data quality is high with 92% confidence\",\n",
    "                \"3 anomalies detected requiring investigation\",\n",
    "                \"Statistical patterns show strong correlation\"\n",
    "            ]\n",
    "        },\n",
    "        \"reasoning_steps\": [\n",
    "            \"Loaded and validated input data\",\n",
    "            \"Applied statistical analysis methods\",\n",
    "            \"Identified patterns and anomalies\",\n",
    "            \"Generated confidence-based recommendations\"\n",
    "        ],\n",
    "        \"processing_time\": 0.5\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Analysis complete: {analysis['findings']['data_quality']:.2f} quality score\")\n",
    "    return analysis\n",
    "\n",
    "@trace_agent(\n",
    "    agent_id=\"content_curator\",\n",
    "    role=\"curator\",\n",
    "    agent_type=\"content_specialist\",\n",
    "    capabilities=[\"content_filtering\", \"quality_assessment\", \"summarization\"],\n",
    "    capture_tools=True\n",
    ")\n",
    "def content_curator_agent(content_list: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Content curation and quality assessment agent.\"\"\"\n",
    "    print(f\"üìù Content Curator: Processing {len(content_list)} content items\")\n",
    "    \n",
    "    time.sleep(0.4)\n",
    "    \n",
    "    # Mock content curation using tools\n",
    "    high_quality_content = []\n",
    "    for i, content in enumerate(content_list):\n",
    "        if i < 3:  # Mock: keep first 3 as high quality\n",
    "            high_quality_content.append({\n",
    "                **content,\n",
    "                \"quality_score\": 0.9 - (i * 0.05),\n",
    "                \"curation_reason\": \"Meets quality standards\"\n",
    "            })\n",
    "    \n",
    "    curation_result = {\n",
    "        \"agent_id\": \"content_curator\",\n",
    "        \"input_count\": len(content_list),\n",
    "        \"curated_count\": len(high_quality_content),\n",
    "        \"curated_content\": high_quality_content,\n",
    "        \"tools_used\": [\"quality_scorer\", \"content_filter\", \"summarizer\"],\n",
    "        \"curation_metrics\": {\n",
    "            \"retention_rate\": len(high_quality_content) / len(content_list),\n",
    "            \"average_quality\": sum(item[\"quality_score\"] for item in high_quality_content) / len(high_quality_content)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Curated {len(high_quality_content)}/{len(content_list)} items\")\n",
    "    return curation_result\n",
    "\n",
    "@trace_agent(\n",
    "    agent_id=\"synthesis_agent\",\n",
    "    role=\"synthesizer\",\n",
    "    agent_type=\"integration_specialist\", \n",
    "    capabilities=[\"multi_source_synthesis\", \"insight_generation\", \"report_creation\"],\n",
    "    capture_inputs=True,\n",
    "    capture_outputs=True\n",
    ")\n",
    "def synthesis_agent(analyst_data: Dict, curator_data: Dict, context: str) -> Dict[str, Any]:\n",
    "    \"\"\"Agent that synthesizes insights from multiple sources.\"\"\"\n",
    "    print(f\"üîó Synthesis Agent: Combining insights for context '{context}'\")\n",
    "    \n",
    "    time.sleep(0.6)\n",
    "    \n",
    "    # Synthesize insights from multiple agents\n",
    "    synthesis = {\n",
    "        \"agent_id\": \"synthesis_agent\",\n",
    "        \"context\": context,\n",
    "        \"input_sources\": [\"data_analyst\", \"content_curator\"],\n",
    "        \"synthesis_insights\": [\n",
    "            f\"Data quality score of {analyst_data['findings']['data_quality']:.2f} indicates reliable foundation\",\n",
    "            f\"Content curation retained {curator_data['curated_count']}/{curator_data['input_count']} high-quality items\",\n",
    "            \"Cross-analysis reveals consistent quality patterns across data and content\",\n",
    "            \"Synthesis confidence: High based on convergent evidence\"\n",
    "        ],\n",
    "        \"combined_metrics\": {\n",
    "            \"data_quality\": analyst_data['findings']['data_quality'],\n",
    "            \"content_quality\": curator_data['curation_metrics']['average_quality'],\n",
    "            \"overall_confidence\": (analyst_data['findings']['pattern_confidence'] + \n",
    "                                 curator_data['curation_metrics']['average_quality']) / 2\n",
    "        },\n",
    "        \"final_recommendation\": \"Proceed with high confidence based on quality convergence\"\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Synthesis complete with {synthesis['combined_metrics']['overall_confidence']:.2f} confidence\")\n",
    "    return synthesis\n",
    "\n",
    "# Advanced orchestrator with dependency management\n",
    "@trace_agent(\n",
    "    agent_id=\"advanced_orchestrator\",\n",
    "    role=\"coordinator\", \n",
    "    agent_type=\"orchestrator\",\n",
    "    capabilities=[\"workflow_management\", \"dependency_resolution\", \"result_aggregation\"],\n",
    "    capture_reasoning=True,\n",
    "    metadata={\"orchestration_strategy\": \"parallel_with_dependencies\"}\n",
    ")\n",
    "def advanced_orchestrator(task: str, data_context: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"Advanced orchestrator managing complex multi-agent workflows.\"\"\"\n",
    "    print(f\"üé≠ Advanced Orchestrator: Managing workflow for '{task}'\")\n",
    "    \n",
    "    # Phase 1: Parallel execution of independent agents\n",
    "    print(\"\\nüîÑ Phase 1: Parallel Agent Execution\")\n",
    "    \n",
    "    # Mock data for demonstration\n",
    "    sample_data = {\"samples\": [f\"sample_{i}\" for i in range(100)]}\n",
    "    sample_content = [\n",
    "        {\"id\": 1, \"title\": \"AI Observability\", \"content\": \"Content about observability\"},\n",
    "        {\"id\": 2, \"title\": \"Tracing Systems\", \"content\": \"Content about tracing\"},\n",
    "        {\"id\": 3, \"title\": \"Monitoring Tools\", \"content\": \"Content about monitoring\"},\n",
    "        {\"id\": 4, \"title\": \"Low Quality\", \"content\": \"Poor content\"}\n",
    "    ]\n",
    "    \n",
    "    # Execute agents in parallel (simulated)\n",
    "    analyst_result = data_analyst_agent(sample_data)\n",
    "    curator_result = content_curator_agent(sample_content)\n",
    "    \n",
    "    # Phase 2: Synthesis based on results\n",
    "    print(\"\\\\nüîó Phase 2: Synthesis and Integration\")\n",
    "    synthesis_result = synthesis_agent(analyst_result, curator_result, task)\n",
    "    \n",
    "    # Final orchestration result\n",
    "    orchestration_result = {\n",
    "        \"task\": task,\n",
    "        \"orchestration_id\": \"adv_orch_001\",\n",
    "        \"phases\": {\n",
    "            \"analysis\": analyst_result,\n",
    "            \"curation\": curator_result, \n",
    "            \"synthesis\": synthesis_result\n",
    "        },\n",
    "        \"workflow_metrics\": {\n",
    "            \"total_agents\": 3,\n",
    "            \"execution_phases\": 2,\n",
    "            \"final_confidence\": synthesis_result[\"combined_metrics\"][\"overall_confidence\"],\n",
    "            \"workflow_success\": True\n",
    "        },\n",
    "        \"reasoning\": [\n",
    "            \"Initiated parallel execution of specialist agents\",\n",
    "            \"Data analyst provided quantitative insights\",\n",
    "            \"Content curator filtered and assessed quality\",\n",
    "            \"Synthesis agent combined multi-source insights\",\n",
    "            \"Workflow completed with high confidence\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Advanced orchestration complete!\")\n",
    "    return orchestration_result\n",
    "\n",
    "# Test the enhanced multi-agent system\n",
    "print(\"ü§ñ Testing Enhanced Multi-Agent System...\")\n",
    "\n",
    "# Run the advanced workflow\n",
    "task = \"Comprehensive analysis of AI system observability data and content\"\n",
    "context_data = {\"domain\": \"ai_observability\", \"priority\": \"high\"}\n",
    "\n",
    "workflow_result = advanced_orchestrator(task, context_data)\n",
    "\n",
    "print(\"\\\\nüé≠ Enhanced Multi-Agent Results:\")\n",
    "print(f\"Task: {workflow_result['task']}\")\n",
    "print(f\"Agents: {workflow_result['workflow_metrics']['total_agents']}\")\n",
    "print(f\"Phases: {workflow_result['workflow_metrics']['execution_phases']}\")\n",
    "print(f\"Final Confidence: {workflow_result['workflow_metrics']['final_confidence']:.2f}\")\n",
    "print(f\"Success: {workflow_result['workflow_metrics']['workflow_success']}\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Enhanced multi-agent system testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7.1: Context Managers and Streaming\n",
    "\n",
    "Test context managers for inline tracing and streaming LLM responses.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Comprehensive Demo Summary\n",
    "\n",
    "This notebook provides a **complete demonstration** of the Noveum Trace SDK capabilities:\n",
    "\n",
    "### üé® All Available Decorators:\n",
    "- `@trace` - General purpose function tracing\n",
    "- `@trace_llm` - LLM call tracing with provider-specific features\n",
    "- `@trace_agent` - Agent workflow tracing with role-based capabilities\n",
    "- `@trace_tool` - Tool usage tracing with comprehensive metadata\n",
    "- `@trace_retrieval` - Retrieval operation tracing for RAG systems\n",
    "\n",
    "### üîÑ Context Managers for Inline Tracing:\n",
    "- `trace_llm_call()` - LLM operations within existing functions\n",
    "- `trace_agent_operation()` - Agent tasks with custom attributes\n",
    "- `trace_operation()` - Generic operations with step-by-step tracking\n",
    "- `streaming_llm()` - Real-time streaming LLM response tracing\n",
    "- `ThreadContext()` - Conversation thread management\n",
    "\n",
    "### ü§ñ Multi-Agent System Features:\n",
    "- Basic orchestration patterns\n",
    "- Advanced multi-agent workflows with specialized roles\n",
    "- Dependency management between agents\n",
    "- Parallel execution and result synthesis\n",
    "- Agent capability tracking and reasoning capture\n",
    "\n",
    "### üåä Streaming & Real-time Features:\n",
    "- Token-by-token streaming trace capture\n",
    "- Real-time metrics (tokens/second, time to first token)\n",
    "- Stream metadata and performance analysis\n",
    "- Context-aware streaming within conversations\n",
    "\n",
    "### üöÄ Advanced SDK Features:\n",
    "- Auto-instrumentation for seamless integration\n",
    "- Proxy objects for enhanced control and monitoring\n",
    "- Manual trace/span creation for custom workflows\n",
    "- Batch processing and performance optimization\n",
    "- Configuration management and debugging tools\n",
    "- Comprehensive error handling and edge case testing\n",
    "\n",
    "### üìä Integration Examples:\n",
    "- OpenAI API integration with cost estimation\n",
    "- Anthropic Claude integration with PII redaction\n",
    "- Google AI integration examples\n",
    "- RAG system integration (vector, keyword, hybrid search)\n",
    "- Multi-provider LLM support patterns\n",
    "\n",
    "This comprehensive demo showcases **every major feature** of the Noveum Trace SDK, making it the perfect reference for implementing observability in your AI applications! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:3000 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "DEBUG:noveum_trace.transport.http_transport:Successfully sent batch of 1 traces\n",
      "DEBUG:noveum_trace.transport.batch_processor:Sent batch of 1 traces\n",
      "INFO:noveum_trace.transport.http_transport:HTTP transport flush completed\n",
      "INFO:noveum_trace.core.client:Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Enhanced SDK Initialization and Endpoint Testing traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER ENHANCED SDK INITIALIZATION AND ENDPOINT TESTING\n",
    "# This ensures the endpoint connectivity test traces are sent immediately\n",
    "\n",
    "flush_traces(\"Enhanced SDK Initialization and Endpoint Testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: 4b504e8b-e27b-4418-9244-b687733cbebc\n",
      "DEBUG:noveum_trace.core.client:Started span: 23cf6769-ebfe-4491-960d-f76d9e174df8 in trace: 4b504e8b-e27b-4418-9244-b687733cbebc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Context Managers and Streaming...\n",
      "\\n1Ô∏è‚É£ Context Manager Examples:\n",
      "üîÑ Processing user query: 'What are the benefits of AI observabilit...'\n",
      "ü§ñ Making LLM call within context manager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 23cf6769-ebfe-4491-960d-f76d9e174df8\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 4b504e8b-e27b-4418-9244-b687733cbebc queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 4b504e8b-e27b-4418-9244-b687733cbebc\n",
      "DEBUG:noveum_trace.core.client:Started trace: 8296746f-e52f-4b1e-949d-8a29855c53f4\n",
      "DEBUG:noveum_trace.core.client:Started span: 5a33e68a-cb5a-4ef7-b969-7b556799d192 in trace: 8296746f-e52f-4b1e-949d-8a29855c53f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM response generated: 57 characters\n",
      "üì§ Final response: Final: Processed response for: what are the benefi...\n",
      "ü§ñ Agent Task: 'Analyze system performance metrics'\n",
      "‚öôÔ∏è  Executing agent task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 5a33e68a-cb5a-4ef7-b969-7b556799d192\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 8296746f-e52f-4b1e-949d-8a29855c53f4 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 8296746f-e52f-4b1e-949d-8a29855c53f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent task completed with 95.0% success rate\n",
      "üîß Starting complex operation...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "trace_operation() missing 1 required positional argument: 'operation_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 200\u001b[39m\n\u001b[32m    196\u001b[39m query_result = process_user_query_with_context_managers(\u001b[33m\"\u001b[39m\u001b[33mWhat are the benefits of AI observability?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    198\u001b[39m agent_result = agent_task_with_context_manager(\u001b[33m\"\u001b[39m\u001b[33mAnalyze system performance metrics\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m operation_result = \u001b[43mcomplex_operation_with_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# Test streaming\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn2Ô∏è‚É£ Streaming Examples:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mcomplex_operation_with_tracing\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Demonstrate generic operation tracing.\"\"\"\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîß Starting complex operation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtrace_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcomplex_data_processing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata_pipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# Step 1: Data loading\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müì• Step 1: Loading data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     79\u001b[39m     time.sleep(\u001b[32m0.2\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: trace_operation() missing 1 required positional argument: 'operation_name'"
     ]
    }
   ],
   "source": [
    "# Import context managers and streaming features\n",
    "from noveum_trace import (\n",
    "    trace_llm_call, trace_agent_operation, trace_operation, \n",
    "    streaming_llm, trace_streaming, ThreadContext\n",
    ")\n",
    "\n",
    "# Context Manager Examples - Inline Tracing\n",
    "\n",
    "def process_user_query_with_context_managers(user_input: str) -> str:\n",
    "    \"\"\"Demonstrate inline tracing with context managers.\"\"\"\n",
    "    print(f\"üîÑ Processing user query: '{user_input[:40]}...'\")\n",
    "    \n",
    "    # Some preprocessing (not traced)\n",
    "    cleaned_input = user_input.strip().lower()\n",
    "    \n",
    "    # Trace just the LLM call using context manager\n",
    "    with trace_llm_call(model=\"gpt-4\", provider=\"openai\", operation=\"query_processing\") as span:\n",
    "        print(\"ü§ñ Making LLM call within context manager...\")\n",
    "        time.sleep(0.4)\n",
    "        \n",
    "        # Mock LLM response\n",
    "        response = f\"Processed response for: {cleaned_input[:30]}...\"\n",
    "        \n",
    "        # Add custom attributes to the span\n",
    "        span.set_attributes({\n",
    "            \"llm.input_length\": len(cleaned_input),\n",
    "            \"llm.output_length\": len(response),\n",
    "            \"llm.processing_type\": \"query_understanding\"\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ LLM response generated: {len(response)} characters\")\n",
    "    \n",
    "    # Post-processing (not traced)\n",
    "    final_response = f\"Final: {response}\"\n",
    "    print(f\"üì§ Final response: {final_response[:50]}...\")\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "# Agent operation context manager\n",
    "def agent_task_with_context_manager(task: str) -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate agent operation tracing with context manager.\"\"\"\n",
    "    print(f\"ü§ñ Agent Task: '{task}'\")\n",
    "    \n",
    "    with trace_agent_operation(\n",
    "        agent_type=\"task_agent\", \n",
    "        operation=\"task_execution\",\n",
    "        capabilities=[\"task_planning\", \"execution\", \"monitoring\"]\n",
    "    ) as span:\n",
    "        print(\"‚öôÔ∏è  Executing agent task...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Mock agent work\n",
    "        result = {\n",
    "            \"task\": task,\n",
    "            \"status\": \"completed\",\n",
    "            \"steps_executed\": 5,\n",
    "            \"success_rate\": 0.95\n",
    "        }\n",
    "        \n",
    "        # Add agent-specific attributes\n",
    "        span.set_attributes({\n",
    "            \"agent.task_complexity\": \"medium\",\n",
    "            \"agent.steps_executed\": result[\"steps_executed\"],\n",
    "            \"agent.success_rate\": result[\"success_rate\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Agent task completed with {result['success_rate']:.1%} success rate\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Generic operation context manager\n",
    "def complex_operation_with_tracing() -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate generic operation tracing.\"\"\"\n",
    "    print(\"üîß Starting complex operation...\")\n",
    "    \n",
    "    with trace_operation(name=\"complex_data_processing\", operation_type=\"data_pipeline\") as span:\n",
    "        # Step 1: Data loading\n",
    "        print(\"üì• Step 1: Loading data...\")\n",
    "        time.sleep(0.2)\n",
    "        span.set_attributes({\"step\": \"data_loading\", \"records_loaded\": 1000})\n",
    "        \n",
    "        # Step 2: Processing\n",
    "        print(\"‚öôÔ∏è  Step 2: Processing data...\")\n",
    "        time.sleep(0.3)\n",
    "        span.set_attributes({\"step\": \"processing\", \"records_processed\": 950})\n",
    "        \n",
    "        # Step 3: Output\n",
    "        print(\"üì§ Step 3: Generating output...\")\n",
    "        time.sleep(0.1)\n",
    "        span.set_attributes({\"step\": \"output\", \"records_output\": 950})\n",
    "        \n",
    "        result = {\n",
    "            \"operation\": \"complex_data_processing\",\n",
    "            \"input_records\": 1000,\n",
    "            \"processed_records\": 950,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Complex operation completed successfully\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Streaming LLM Examples\n",
    "\n",
    "class MockStreamChunk:\n",
    "    \"\"\"Mock streaming response chunk.\"\"\"\n",
    "    def __init__(self, content: str):\n",
    "        self.choices = [MockChoice(content)]\n",
    "\n",
    "class MockChoice:\n",
    "    \"\"\"Mock choice in streaming response.\"\"\"\n",
    "    def __init__(self, content: str):\n",
    "        self.delta = MockDelta(content)\n",
    "\n",
    "class MockDelta:\n",
    "    \"\"\"Mock delta content.\"\"\"\n",
    "    def __init__(self, content: str):\n",
    "        self.content = content\n",
    "\n",
    "def mock_streaming_response(prompt: str) -> Iterator[MockStreamChunk]:\n",
    "    \"\"\"Generate mock streaming response.\"\"\"\n",
    "    words = f\"This is a streaming response to: {prompt}. Each word comes separately.\".split()\n",
    "    for word in words:\n",
    "        time.sleep(0.05)  # Simulate streaming delay\n",
    "        yield MockStreamChunk(word + \" \")\n",
    "\n",
    "def test_streaming_with_context_manager(prompt: str) -> str:\n",
    "    \"\"\"Test streaming LLM with context manager.\"\"\"\n",
    "    print(f\"üåä Streaming LLM call: '{prompt[:30]}...'\")\n",
    "    \n",
    "    # Create mock stream\n",
    "    stream = mock_streaming_response(prompt)\n",
    "    \n",
    "    # Use streaming context manager\n",
    "    with streaming_llm(model=\"gpt-4\", provider=\"openai\", operation=\"streaming_chat\") as stream_manager:\n",
    "        print(\"üì∫ Streaming response: \", end=\"\")\n",
    "        full_response = \"\"\n",
    "        \n",
    "        for chunk in stream:\n",
    "            token = chunk.choices[0].delta.content\n",
    "            if token:\n",
    "                # Add token to stream manager for tracing\n",
    "                stream_manager.add_token(token)\n",
    "                print(token, end=\"\")\n",
    "                full_response += token\n",
    "        \n",
    "        # Add final metadata\n",
    "        stream_manager.add_metadata({\n",
    "            \"streaming.final_length\": len(full_response),\n",
    "            \"streaming.total_chunks\": len(full_response.split())\n",
    "        })\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Streaming completed: {len(full_response)} characters\")\n",
    "    \n",
    "    return full_response.strip()\n",
    "\n",
    "# Thread Context for Conversation Tracking\n",
    "\n",
    "def test_thread_context_conversation() -> None:\n",
    "    \"\"\"Test conversation thread tracking.\"\"\"\n",
    "    print(\"üí¨ Testing Thread Context for Conversations...\")\n",
    "    \n",
    "    with ThreadContext(name=\"demo_conversation\", metadata={\"session\": \"demo\"}) as thread:\n",
    "        # Simulate conversation turns\n",
    "        \n",
    "        # Turn 1\n",
    "        thread.add_message(\"user\", \"Hello, can you help me with AI observability?\")\n",
    "        print(\"üë§ User: Hello, can you help me with AI observability?\")\n",
    "        \n",
    "        # Simulate LLM response within thread\n",
    "        with trace_llm_call(model=\"gpt-4\") as llm_span:\n",
    "            time.sleep(0.3)\n",
    "            response1 = \"I'd be happy to help you with AI observability!\"\n",
    "            thread.add_message(\"assistant\", response1)\n",
    "            print(f\"ü§ñ Assistant: {response1}\")\n",
    "        \n",
    "        # Turn 2  \n",
    "        thread.add_message(\"user\", \"What are the key components?\")\n",
    "        print(\"üë§ User: What are the key components?\")\n",
    "        \n",
    "        with trace_llm_call(model=\"gpt-4\") as llm_span:\n",
    "            time.sleep(0.4)\n",
    "            response2 = \"Key components include tracing, metrics, and logging.\"\n",
    "            thread.add_message(\"assistant\", response2)\n",
    "            print(f\"ü§ñ Assistant: {response2}\")\n",
    "        \n",
    "        # Get thread statistics\n",
    "        stats = thread.get_statistics()\n",
    "        print(f\"\\\\nüìä Thread Stats: {stats['message_count']} messages, {stats['turn_count']} turns\")\n",
    "\n",
    "# Test all context manager and streaming features\n",
    "print(\"üîÑ Testing Context Managers and Streaming...\")\n",
    "\n",
    "# Test context managers\n",
    "print(\"\\\\n1Ô∏è‚É£ Context Manager Examples:\")\n",
    "query_result = process_user_query_with_context_managers(\"What are the benefits of AI observability?\")\n",
    "\n",
    "agent_result = agent_task_with_context_manager(\"Analyze system performance metrics\")\n",
    "\n",
    "operation_result = complex_operation_with_tracing()\n",
    "\n",
    "# Test streaming\n",
    "print(\"\\\\n2Ô∏è‚É£ Streaming Examples:\")\n",
    "stream_result = test_streaming_with_context_manager(\"Explain machine learning concepts\")\n",
    "\n",
    "# Test thread context\n",
    "print(\"\\\\n3Ô∏è‚É£ Thread Context Examples:\")\n",
    "test_thread_context_conversation()\n",
    "\n",
    "print(\"\\\\n‚úÖ Context managers and streaming testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ‚úÖ FIXED: Correct trace_operation Usage\n",
    "\n",
    "The `trace_operation()` context manager has been fixed with the correct syntax:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: ab5c73b5-6281-4057-9072-a652ab851852\n",
      "DEBUG:noveum_trace.core.client:Started span: bc1d3195-e720-499e-a9e5-4ca0f6e44cd2 in trace: ab5c73b5-6281-4057-9072-a652ab851852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing CORRECTED Context Manager Usage...\n",
      "\\n1Ô∏è‚É£ Correct trace_operation Usage:\n",
      "üîß Testing CORRECT trace_operation usage...\n",
      "üì• Step 1: Loading data...\n",
      "‚öôÔ∏è  Step 2: Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: bc1d3195-e720-499e-a9e5-4ca0f6e44cd2\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace ab5c73b5-6281-4057-9072-a652ab851852 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: ab5c73b5-6281-4057-9072-a652ab851852\n",
      "DEBUG:noveum_trace.core.client:Started trace: c220f277-b383-4889-a59b-5936d551dff4\n",
      "DEBUG:noveum_trace.core.client:Started span: d9fa3f3b-6e87-4973-acfa-eda05ead43b9 in trace: c220f277-b383-4889-a59b-5936d551dff4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Step 3: Generating output...\n",
      "‚úÖ Complex operation completed successfully\n",
      "\\n2Ô∏è‚É£ Correct Batch Operations:\n",
      "üì¶ Testing CORRECT batch trace_operation usage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: d9fa3f3b-6e87-4973-acfa-eda05ead43b9\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace c220f277-b383-4889-a59b-5936d551dff4 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: c220f277-b383-4889-a59b-5936d551dff4\n",
      "DEBUG:noveum_trace.core.client:Started trace: 66dac688-4039-4a23-94ff-f9a31bf4b033\n",
      "DEBUG:noveum_trace.core.client:Started span: de8a7c20-a98f-40a1-9d39-8c9863065544 in trace: 66dac688-4039-4a23-94ff-f9a31bf4b033\n",
      "DEBUG:noveum_trace.core.client:Finished span: de8a7c20-a98f-40a1-9d39-8c9863065544\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 66dac688-4039-4a23-94ff-f9a31bf4b033 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 66dac688-4039-4a23-94ff-f9a31bf4b033\n",
      "DEBUG:noveum_trace.core.client:Started trace: f638fb44-816e-41ec-961e-0a132d7e2c66\n",
      "DEBUG:noveum_trace.core.client:Started span: 79f65cc8-6997-4f4e-8a3b-ea110fc7e52e in trace: f638fb44-816e-41ec-961e-0a132d7e2c66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ Batch operation 1/3 completed\n",
      "üî∏ Batch operation 2/3 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 79f65cc8-6997-4f4e-8a3b-ea110fc7e52e\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace f638fb44-816e-41ec-961e-0a132d7e2c66 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: f638fb44-816e-41ec-961e-0a132d7e2c66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ Batch operation 3/3 completed\n",
      "‚úÖ Batch processing completed: 3 operations\n",
      "\\n‚úÖ All corrected context manager examples completed!\n",
      "Operation result: True\n",
      "Batch operations: 3 completed\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED trace_operation Examples\n",
    "# The correct syntax is: trace_operation(operation_name, attributes=dict, tags=dict)\n",
    "\n",
    "def test_correct_trace_operation_usage():\n",
    "    \"\"\"Demonstrate the CORRECT syntax for trace_operation context manager.\"\"\"\n",
    "    print(\"üîß Testing CORRECT trace_operation usage...\")\n",
    "    \n",
    "    # ‚úÖ CORRECT: First parameter is operation_name (string), second is attributes (dict)\n",
    "    with trace_operation(\"data_processing_pipeline\", \n",
    "                        attributes={\"operation_type\": \"data_pipeline\", \"complexity\": \"high\"}) as span:\n",
    "        # Step 1: Data loading\n",
    "        print(\"üì• Step 1: Loading data...\")\n",
    "        time.sleep(0.2)\n",
    "        span.set_attributes({\"step\": \"data_loading\", \"records_loaded\": 1000})\n",
    "        \n",
    "        # Step 2: Processing\n",
    "        print(\"‚öôÔ∏è  Step 2: Processing data...\")\n",
    "        time.sleep(0.3)\n",
    "        span.set_attributes({\"step\": \"processing\", \"records_processed\": 950})\n",
    "        \n",
    "        # Step 3: Output\n",
    "        print(\"üì§ Step 3: Generating output...\")\n",
    "        time.sleep(0.1)\n",
    "        span.set_attributes({\"step\": \"output\", \"records_output\": 950})\n",
    "        \n",
    "        result = {\n",
    "            \"operation\": \"data_processing_pipeline\",\n",
    "            \"input_records\": 1000,\n",
    "            \"processed_records\": 950,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Complex operation completed successfully\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def test_correct_batch_operations():\n",
    "    \"\"\"Demonstrate correct trace_operation usage in batch processing.\"\"\"\n",
    "    print(\"üì¶ Testing CORRECT batch trace_operation usage...\")\n",
    "    \n",
    "    operations = []\n",
    "    \n",
    "    for i in range(3):  # Reduced to 3 for demo\n",
    "        # ‚úÖ CORRECT: operation_name first, then attributes dict\n",
    "        with trace_operation(f\"batch_operation_{i}\", \n",
    "                           attributes={\"operation_type\": \"batch_demo\", \"batch_index\": i}) as span:\n",
    "            span.set_attributes({\n",
    "                \"batch.operation_number\": i,\n",
    "                \"batch.total_operations\": 3,\n",
    "                \"operation.size\": \"small\"\n",
    "            })\n",
    "            time.sleep(0.1)  # Quick operations\n",
    "            operations.append(f\"operation_{i}\")\n",
    "            print(f\"üî∏ Batch operation {i+1}/3 completed\")\n",
    "    \n",
    "    print(f\"‚úÖ Batch processing completed: {len(operations)} operations\")\n",
    "    return operations\n",
    "\n",
    "# Test the corrected functions\n",
    "print(\"üîß Testing CORRECTED Context Manager Usage...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Correct trace_operation Usage:\")\n",
    "operation_result = test_correct_trace_operation_usage()\n",
    "\n",
    "print(\"\\\\n2Ô∏è‚É£ Correct Batch Operations:\")\n",
    "batch_result = test_correct_batch_operations()\n",
    "\n",
    "print(f\"\\\\n‚úÖ All corrected context manager examples completed!\")\n",
    "print(f\"Operation result: {operation_result['success']}\")\n",
    "print(f\"Batch operations: {len(batch_result)} completed\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ‚úÖ FIXED: Correct create_traced_agent Usage\n",
    "\n",
    "The `create_traced_agent()` function has been fixed with the correct parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: 27959aef-5e73-418d-bee3-54449738ce63\n",
      "DEBUG:noveum_trace.core.client:Started span: 5b72295d-a29b-45f6-9d2b-b995cbce676b in trace: 27959aef-5e73-418d-bee3-54449738ce63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing CORRECTED Proxy Object Functions...\n",
      "\\n1Ô∏è‚É£ Corrected Traced Agent:\n",
      "ü§ñ Testing Traced Agent Proxy (CORRECTED)...\n",
      "‚úÖ Traced agent proxy created successfully!\n",
      "üß† Testing traced agent methods...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 5b72295d-a29b-45f6-9d2b-b995cbce676b\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 27959aef-5e73-418d-bee3-54449738ce63 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 27959aef-5e73-418d-bee3-54449738ce63\n",
      "DEBUG:noveum_trace.core.client:Started trace: 051214a6-a0e2-4ec9-92ad-171f2cb3f74b\n",
      "DEBUG:noveum_trace.core.client:Started span: 4e1f7e98-8fa5-45c1-99e1-8310a16a6212 in trace: 051214a6-a0e2-4ec9-92ad-171f2cb3f74b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí≠ Think result: Thinking about: How to improve AI observability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 4e1f7e98-8fa5-45c1-99e1-8310a16a6212\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 051214a6-a0e2-4ec9-92ad-171f2cb3f74b queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 051214a6-a0e2-4ec9-92ad-171f2cb3f74b\n",
      "DEBUG:noveum_trace.core.client:Started trace: ec189268-71b3-4710-baa2-55b546a3e443\n",
      "DEBUG:noveum_trace.core.client:Started span: ee0b17b9-ef76-4ea6-9ac0-de05e3a156fc in trace: ec189268-71b3-4710-baa2-55b546a3e443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Action result: Performing action: Implement monitoring dashboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: ee0b17b9-ef76-4ea6-9ac0-de05e3a156fc\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace ec189268-71b3-4710-baa2-55b546a3e443 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: ec189268-71b3-4710-baa2-55b546a3e443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Plan result: 3 steps\n",
      "\\n2Ô∏è‚É£ Corrected Traced OpenAI Client:\n",
      "üîÑ Testing Traced OpenAI Client (CORRECTED)...\n",
      "‚úÖ Traced OpenAI client created successfully!\n",
      "ü§ñ Simulating traced OpenAI call...\n",
      "‚ÑπÔ∏è  Would automatically trace all OpenAI API calls\n",
      "\\n‚úÖ All corrected proxy object examples completed!\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED create_traced_agent Example\n",
    "# The correct signature is: create_traced_agent(agent, agent_type, capabilities, trace_config)\n",
    "\n",
    "def test_traced_agent_proxy_corrected():\n",
    "    \"\"\"Test traced agent proxy with CORRECT parameters.\"\"\"\n",
    "    print(\"ü§ñ Testing Traced Agent Proxy (CORRECTED)...\")\n",
    "    \n",
    "    # Mock agent class\n",
    "    class MockAgent:\n",
    "        def __init__(self, name: str):\n",
    "            self.name = name\n",
    "        \n",
    "        def think(self, problem: str) -> str:\n",
    "            time.sleep(0.2)\n",
    "            return f\"Thinking about: {problem}\"\n",
    "        \n",
    "        def act(self, action: str) -> str:\n",
    "            time.sleep(0.3)\n",
    "            return f\"Performing action: {action}\"\n",
    "        \n",
    "        def plan(self, goal: str) -> List[str]:\n",
    "            time.sleep(0.4)\n",
    "            return [f\"Step 1 for {goal}\", f\"Step 2 for {goal}\", f\"Step 3 for {goal}\"]\n",
    "    \n",
    "    # ‚úÖ CORRECT: Use proper parameter names and structure\n",
    "    original_agent = MockAgent(\"demo_agent\")\n",
    "    traced_agent = create_traced_agent(\n",
    "        agent=original_agent,\n",
    "        agent_type=\"traced_demo_agent\",  # ‚úÖ CORRECT: agent_type (not agent_id)\n",
    "        capabilities=[\"thinking\", \"acting\", \"planning\"],  # ‚úÖ CORRECT: capabilities (not auto_trace_methods)\n",
    "        trace_config={\"capture_inputs\": True, \"capture_outputs\": True}  # ‚úÖ CORRECT: trace_config dict\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Traced agent proxy created successfully!\")\n",
    "    \n",
    "    # Test traced methods\n",
    "    print(\"üß† Testing traced agent methods...\")\n",
    "    \n",
    "    thought = traced_agent.think(\"How to improve AI observability\")\n",
    "    print(f\"üí≠ Think result: {thought}\")\n",
    "    \n",
    "    action = traced_agent.act(\"Implement monitoring dashboard\")\n",
    "    print(f\"‚ö° Action result: {action}\")\n",
    "    \n",
    "    plan = traced_agent.plan(\"Enhance system reliability\")\n",
    "    print(f\"üìã Plan result: {len(plan)} steps\")\n",
    "    \n",
    "    return traced_agent\n",
    "\n",
    "def test_traced_openai_client_corrected():\n",
    "    \"\"\"Test traced OpenAI client with CORRECT parameters.\"\"\"\n",
    "    print(\"üîÑ Testing Traced OpenAI Client (CORRECTED)...\")\n",
    "    \n",
    "    # ‚úÖ CORRECT: Use actual OpenAI client instance, not direct parameters\n",
    "    try:\n",
    "        # First create a real OpenAI client (even with mock key)\n",
    "        import openai\n",
    "        original_client = openai.OpenAI(api_key=\"mock-key-for-demo\")\n",
    "        \n",
    "        # ‚úÖ CORRECT: Pass the client instance to the tracer\n",
    "        traced_client = create_traced_openai_client(\n",
    "            original_client=original_client,\n",
    "            trace_config={\"trace_completions\": True, \"capture_content\": True}\n",
    "        )\n",
    "        print(\"‚úÖ Traced OpenAI client created successfully!\")\n",
    "        \n",
    "        # Mock a call (won't actually work without real API key)\n",
    "        print(\"ü§ñ Simulating traced OpenAI call...\")\n",
    "        print(\"‚ÑπÔ∏è  Would automatically trace all OpenAI API calls\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Traced client demo: {e}\")\n",
    "        print(\"üìù Note: This requires a real OpenAI client instance\")\n",
    "\n",
    "# Test the corrected functions\n",
    "print(\"üöÄ Testing CORRECTED Proxy Object Functions...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Corrected Traced Agent:\")\n",
    "traced_agent = test_traced_agent_proxy_corrected()\n",
    "\n",
    "print(\"\\\\n2Ô∏è‚É£ Corrected Traced OpenAI Client:\")\n",
    "test_traced_openai_client_corrected()\n",
    "\n",
    "print(\"\\\\n‚úÖ All corrected proxy object examples completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîÑ FINAL COMPREHENSIVE FLUSH\n",
    "\n",
    "Final flush to ensure all traces from the entire demo are sent to your endpoint.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß FIXED: Enhanced SDK Initialization with Endpoint Debugging\n",
    "\n",
    "The endpoint configuration has been fixed with proper debugging and transport settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1154693373.py, line 33)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\\\"\\\"\\\"Verify that the Beeceptor endpoint is reachable.\\\"\\\"\\\"\u001b[39m\n     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# üîß ENHANCED SDK INITIALIZATION WITH ENDPOINT DEBUGGING\n",
    "import noveum_trace\n",
    "from noveum_trace import trace, trace_agent, trace_llm, trace_tool\n",
    "import logging\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "# üîç COMPREHENSIVE DEBUGGING SETUP\n",
    "print(\"üîß Setting up enhanced debugging for transport layer...\")\n",
    "\n",
    "# Set up comprehensive logging with detailed output\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Enable specific loggers for transport debugging\n",
    "loggers_to_enable = [\n",
    "    'noveum_trace.transport', \n",
    "    'noveum_trace.transport.http_transport',\n",
    "    'urllib3.connectionpool'\n",
    "]\n",
    "\n",
    "for logger_name in loggers_to_enable:\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "print(\"‚úÖ Enhanced debugging enabled\")\n",
    "\n",
    "# üåê ENDPOINT VERIFICATION \n",
    "def verify_endpoint():\n",
    "    \\\"\\\"\\\"Verify that the Beeceptor endpoint is reachable.\\\"\\\"\\\"\n",
    "    endpoint = \"https://noveum-trace.free.beeceptor.com\"\n",
    "    \n",
    "    print(f\"üîç Verifying endpoint reachability: {endpoint}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(endpoint, timeout=10)\n",
    "        print(f\"‚úÖ Base endpoint reachable - Status: {response.status_code}\")\n",
    "        \n",
    "        # Test the actual trace endpoints\n",
    "        trace_endpoints = [\n",
    "            f\"{endpoint}/v1/trace\",   # Single trace endpoint\n",
    "            f\"{endpoint}/v1/traces\"   # Batch trace endpoint  \n",
    "        ]\n",
    "        \n",
    "        for test_endpoint in trace_endpoints:\n",
    "            try:\n",
    "                test_response = requests.head(test_endpoint, timeout=5)\n",
    "                print(f\"üì° {test_endpoint} - Status: {test_response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  {test_endpoint} - Error: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Endpoint verification failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Verify endpoint first\n",
    "endpoint_ok = verify_endpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nüöÄ Initializing Noveum Trace SDK with enhanced configuration...\n",
      "üåê Base Endpoint: https://noveum-trace.free.beeceptor.com\n",
      "üì° Single Trace Endpoint: https://noveum-trace.free.beeceptor.com/v1/trace\n",
      "üì¶ Batch Trace Endpoint: https://noveum-trace.free.beeceptor.com/v1/traces\n",
      "üîß Transport Mode: Individual traces (batch_size=1) for better debugging\n",
      "‚úÖ Noveum Trace SDK initialized successfully!\n",
      "üìä Project: jupyter-test-project\n",
      "üîß Environment: development\n",
      "üåê Transport Endpoint: http://localhost:3000/api\n",
      "üì¶ Batch Size: 100\n",
      "‚è±Ô∏è  Batch Timeout: 5.0s\n",
      "üîç Debug Mode: True\n",
      "\\nüéØ SDK initialization completed!\n",
      "üìã Check the debug log output above for HTTP request details\n",
      "üåê Your traces should now be visible at: https://noveum-trace.free.beeceptor.com\n"
     ]
    }
   ],
   "source": [
    "# üöÄ ENHANCED SDK INITIALIZATION\n",
    "try:\n",
    "    print(\"\\\\nüöÄ Initializing Noveum Trace SDK with enhanced configuration...\")\n",
    "    \n",
    "    # üìã Display endpoint mapping\n",
    "    base_endpoint = \"https://noveum-trace.free.beeceptor.com\"\n",
    "    print(f\"üåê Base Endpoint: {base_endpoint}\")\n",
    "    print(f\"üì° Single Trace Endpoint: {base_endpoint}/v1/trace\")\n",
    "    print(f\"üì¶ Batch Trace Endpoint: {base_endpoint}/v1/traces\")\n",
    "    print(f\"üîß Transport Mode: Individual traces (batch_size=1) for better debugging\")\n",
    "    \n",
    "    noveum_trace.init(\n",
    "        api_key=os.getenv('NOVEUM_API_KEY'),\n",
    "        project=\"jupyter-test-project\",\n",
    "        environment=\"development\", \n",
    "        endpoint=base_endpoint,  # SDK will append /v1/trace or /v1/traces automatically\n",
    "        debug=True,  # Enable debug mode\n",
    "        \n",
    "        # üîß Enhanced transport configuration for debugging\n",
    "        transport_config={\n",
    "            \"timeout\": 30,           # 30 second timeout (generous for debugging)\n",
    "            \"retry_attempts\": 0,     # No retries for faster debugging feedback  \n",
    "            \"batch_size\": 1,         # Send traces individually (not batched)\n",
    "            \"batch_timeout\": 0.5,    # Send traces immediately\n",
    "            \"compression\": False,    # No compression for easier debugging\n",
    "            \"verify_ssl\": True       # Verify SSL certificates\n",
    "        },\n",
    "        \n",
    "        # ‚úÖ Comprehensive tracing configuration  \n",
    "        tracing_config={\n",
    "            \"sample_rate\": 1.0,        # Trace 100% of operations\n",
    "            \"capture_errors\": True,    # Capture error details\n",
    "            \"auto_flush\": True         # Automatically flush traces\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Noveum Trace SDK initialized successfully!\")\n",
    "    \n",
    "    # üìä Display configuration details\n",
    "    config = noveum_trace.get_config()\n",
    "    print(f\"üìä Project: {config.project}\")\n",
    "    print(f\"üîß Environment: {config.environment}\")\n",
    "    print(f\"üåê Transport Endpoint: {config.transport.endpoint}\")\n",
    "    print(f\"üì¶ Batch Size: {config.transport.batch_size}\")\n",
    "    print(f\"‚è±Ô∏è  Batch Timeout: {config.transport.batch_timeout}s\")\n",
    "    print(f\"üîç Debug Mode: {config.debug}\")\n",
    "    \n",
    "    print(\"\\\\nüéØ SDK initialization completed!\")\n",
    "    print(\"üìã Check the debug log output above for HTTP request details\")\n",
    "    print(\"üåê Your traces should now be visible at: https://noveum-trace.free.beeceptor.com\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing SDK: {e}\")\n",
    "    print(\"Continuing with demo - traces will be logged locally\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: 7d6cec38-2f53-46e0-99ae-5633c2bb2871\n",
      "DEBUG:noveum_trace.core.client:Started span: ff75d7d0-16ca-4cbe-bbdd-f9c591309093 in trace: 7d6cec38-2f53-46e0-99ae-5633c2bb2871\n",
      "DEBUG:noveum_trace.core.client:Finished span: ff75d7d0-16ca-4cbe-bbdd-f9c591309093\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 7d6cec38-2f53-46e0-99ae-5633c2bb2871 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 7d6cec38-2f53-46e0-99ae-5633c2bb2871\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing trace sending to verify endpoint connectivity...\n",
      "\\nüîÑ Executing traced function...\n",
      "üì° Test trace function executed - this should generate HTTP requests\n",
      "‚úÖ Function result: {'test': 'completed', 'timestamp': 1752952837.3284638}\n",
      "\\nüì§ Forcing flush of traces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:3000 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "DEBUG:noveum_trace.transport.http_transport:Successfully sent batch of 1 traces\n",
      "DEBUG:noveum_trace.transport.batch_processor:Sent batch of 1 traces\n",
      "INFO:noveum_trace.transport.http_transport:HTTP transport flush completed\n",
      "INFO:noveum_trace.core.client:Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Flush completed - traces should have been sent to endpoint\n",
      "\\nüìã ENDPOINT DEBUGGING SUMMARY:\n",
      "üåê Your Beeceptor endpoint: https://noveum-trace.free.beeceptor.com\n",
      "üì° SDK sends traces to:\n",
      "   - Single traces: https://noveum-trace.free.beeceptor.com/v1/trace\n",
      "   - Batch traces:  https://noveum-trace.free.beeceptor.com/v1/traces\n",
      "üîç Check the debug logs above for HTTP request details\n",
      "üìä Check your Beeceptor dashboard to see if traces are arriving\n",
      "\\n‚úÖ Enhanced debugging setup completed!\n"
     ]
    }
   ],
   "source": [
    "# üß™ TEST TRACE SENDING TO VERIFY ENDPOINT CONNECTIVITY\n",
    "print(\"üß™ Testing trace sending to verify endpoint connectivity...\")\n",
    "\n",
    "@trace\n",
    "def test_endpoint_connectivity():\n",
    "    \"\"\"Test function to verify traces are being sent to the configured endpoint.\"\"\"\n",
    "    import time\n",
    "    time.sleep(0.1)\n",
    "    print(\"üì° Test trace function executed - this should generate HTTP requests\")\n",
    "    return {\"test\": \"completed\", \"timestamp\": time.time()}\n",
    "\n",
    "# Execute test function\n",
    "print(\"\\\\nüîÑ Executing traced function...\")\n",
    "result = test_endpoint_connectivity()\n",
    "print(f\"‚úÖ Function result: {result}\")\n",
    "\n",
    "# Force flush any pending traces\n",
    "print(\"\\\\nüì§ Forcing flush of traces...\")\n",
    "try:\n",
    "    noveum_trace.flush()\n",
    "    print(\"‚úÖ Flush completed - traces should have been sent to endpoint\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Flush error: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\\\nüìã ENDPOINT DEBUGGING SUMMARY:\")\n",
    "print(\"üåê Your Beeceptor endpoint: https://noveum-trace.free.beeceptor.com\")\n",
    "print(\"üì° SDK sends traces to:\")\n",
    "print(\"   - Single traces: https://noveum-trace.free.beeceptor.com/v1/trace\")\n",
    "print(\"   - Batch traces:  https://noveum-trace.free.beeceptor.com/v1/traces\")\n",
    "print(\"üîç Check the debug logs above for HTTP request details\")\n",
    "print(\"üìä Check your Beeceptor dashboard to see if traces are arriving\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Enhanced debugging setup completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß Debug: Testing Endpoint Connectivity\n",
    "\n",
    "Let's test if traces are being sent to your configured endpoint and diagnose any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7.2: Auto-Instrumentation and Advanced Features\n",
    "\n",
    "Test auto-instrumentation, proxy objects, and advanced SDK features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import advanced features\n",
    "from noveum_trace import (\n",
    "    auto_instrument, get_instrumented_libraries, is_instrumented,\n",
    "    create_traced_openai_client, create_traced_agent, TracedOpenAIClient,\n",
    "    start_trace, start_span, get_current_trace, get_current_span\n",
    ")\n",
    "\n",
    "# Auto-Instrumentation Examples\n",
    "\n",
    "def test_auto_instrumentation():\n",
    "    \"\"\"Test automatic instrumentation of libraries.\"\"\"\n",
    "    print(\"üîß Testing Auto-Instrumentation...\")\n",
    "    \n",
    "    # Check available instrumentations\n",
    "    available = noveum_trace.get_available_instrumentations()\n",
    "    print(f\"üì¶ Available instrumentations: {available}\")\n",
    "    \n",
    "    # Enable auto-instrumentation for OpenAI (if not already enabled)\n",
    "    if not is_instrumented(\"openai\"):\n",
    "        print(\"üîå Enabling OpenAI auto-instrumentation...\")\n",
    "        try:\n",
    "            auto_instrument(\"openai\")\n",
    "            print(\"‚úÖ OpenAI auto-instrumentation enabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Auto-instrumentation: {e}\")\n",
    "    else:\n",
    "        print(\"‚úÖ OpenAI already instrumented\")\n",
    "    \n",
    "    # Check instrumented libraries\n",
    "    instrumented = get_instrumented_libraries()\n",
    "    print(f\"üîç Currently instrumented: {instrumented}\")\n",
    "    \n",
    "    return instrumented\n",
    "\n",
    "# Proxy Objects for Enhanced Control\n",
    "\n",
    "def test_traced_openai_client():\n",
    "    \"\"\"Test traced OpenAI client proxy.\"\"\"\n",
    "    print(\"üîÑ Testing Traced OpenAI Client...\")\n",
    "    \n",
    "    # Create traced OpenAI client (even without real API key)\n",
    "    try:\n",
    "        traced_client = create_traced_openai_client(\n",
    "            api_key=\"mock-key-for-demo\",\n",
    "            trace_completions=True,\n",
    "            trace_embeddings=True,\n",
    "            capture_content=True\n",
    "        )\n",
    "        print(\"‚úÖ Traced OpenAI client created\")\n",
    "        \n",
    "        # Mock a call (won't actually work without real API key)\n",
    "        print(\"ü§ñ Simulating traced OpenAI call...\")\n",
    "        # In real usage: response = traced_client.chat.completions.create(...)\n",
    "        print(\"‚ÑπÔ∏è  Would automatically trace all OpenAI API calls\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Traced client demo: {e}\")\n",
    "\n",
    "def test_traced_agent_proxy():\n",
    "    \"\"\"Test traced agent proxy for enhanced agent monitoring.\"\"\"\n",
    "    print(\"ü§ñ Testing Traced Agent Proxy...\")\n",
    "    \n",
    "    # Mock agent class\n",
    "    class MockAgent:\n",
    "        def __init__(self, name: str):\n",
    "            self.name = name\n",
    "        \n",
    "        def think(self, problem: str) -> str:\n",
    "            time.sleep(0.2)\n",
    "            return f\"Thinking about: {problem}\"\n",
    "        \n",
    "        def act(self, action: str) -> str:\n",
    "            time.sleep(0.3)\n",
    "            return f\"Performing action: {action}\"\n",
    "        \n",
    "        def plan(self, goal: str) -> List[str]:\n",
    "            time.sleep(0.4)\n",
    "            return [f\"Step 1 for {goal}\", f\"Step 2 for {goal}\", f\"Step 3 for {goal}\"]\n",
    "    \n",
    "    # Create traced agent proxy\n",
    "    original_agent = MockAgent(\"demo_agent\")\n",
    "    traced_agent = create_traced_agent(\n",
    "        agent=original_agent,\n",
    "        agent_id=\"traced_demo_agent\",\n",
    "        auto_trace_methods=[\"think\", \"act\", \"plan\"],\n",
    "        capture_inputs=True,\n",
    "        capture_outputs=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Traced agent proxy created\")\n",
    "    \n",
    "    # Test traced methods\n",
    "    print(\"üß† Testing traced agent methods...\")\n",
    "    \n",
    "    thought = traced_agent.think(\"How to improve AI observability\")\n",
    "    print(f\"üí≠ Think result: {thought}\")\n",
    "    \n",
    "    action = traced_agent.act(\"Implement monitoring dashboard\")\n",
    "    print(f\"‚ö° Action result: {action}\")\n",
    "    \n",
    "    plan = traced_agent.plan(\"Enhance system reliability\")\n",
    "    print(f\"üìã Plan result: {len(plan)} steps\")\n",
    "\n",
    "# Manual Span Creation and Management\n",
    "\n",
    "def test_manual_tracing():\n",
    "    \"\"\"Test manual trace and span creation.\"\"\"\n",
    "    print(\"üîç Testing Manual Tracing...\")\n",
    "    \n",
    "    # Start a manual trace\n",
    "    trace = start_trace(\"manual_demo_trace\")\n",
    "    print(f\"‚úÖ Started trace: {trace.trace_id}\")\n",
    "    \n",
    "    # Create nested spans manually\n",
    "    with trace.span(\"parent_operation\") as parent_span:\n",
    "        parent_span.set_attributes({\n",
    "            \"operation.type\": \"parent\",\n",
    "            \"operation.importance\": \"high\"\n",
    "        })\n",
    "        print(\"üìä Parent span created\")\n",
    "        \n",
    "        # Child span 1\n",
    "        with parent_span.create_child_span(\"child_operation_1\") as child1:\n",
    "            child1.set_attributes({\n",
    "                \"operation.type\": \"child\",\n",
    "                \"child.number\": 1\n",
    "            })\n",
    "            time.sleep(0.2)\n",
    "            print(\"üîπ Child span 1 completed\")\n",
    "        \n",
    "        # Child span 2  \n",
    "        with parent_span.create_child_span(\"child_operation_2\") as child2:\n",
    "            child2.set_attributes({\n",
    "                \"operation.type\": \"child\",\n",
    "                \"child.number\": 2,\n",
    "                \"child.data_processed\": 500\n",
    "            })\n",
    "            time.sleep(0.3)\n",
    "            print(\"üîπ Child span 2 completed\")\n",
    "        \n",
    "        print(\"üìä Parent operation completed\")\n",
    "    \n",
    "    # Finish trace\n",
    "    trace.finish()\n",
    "    print(f\"‚úÖ Manual trace completed: {trace.trace_id}\")\n",
    "\n",
    "# Advanced Configuration and Performance Features\n",
    "\n",
    "def test_advanced_configuration():\n",
    "    \"\"\"Test advanced SDK configuration options.\"\"\"\n",
    "    print(\"‚öôÔ∏è  Testing Advanced Configuration...\")\n",
    "    \n",
    "    # Get current configuration\n",
    "    config = noveum_trace.get_config()\n",
    "    print(f\"üìã Current project: {config.project}\")\n",
    "    print(f\"üåê Current endpoint: {config.transport.endpoint}\")\n",
    "    print(f\"üì¶ Batch size: {config.transport.batch_size}\")\n",
    "    print(f\"‚è±Ô∏è  Batch timeout: {config.transport.batch_timeout}\")\n",
    "    \n",
    "    # Test configuration updates (temporary for demo)\n",
    "    original_debug = config.debug\n",
    "    \n",
    "    # Temporarily enable debug mode\n",
    "    noveum_trace.configure(debug=True)\n",
    "    print(\"üêõ Debug mode enabled temporarily\")\n",
    "    \n",
    "    # Create a trace to demonstrate debug output\n",
    "    with noveum_trace.trace_operation(\"debug_demo_operation\") as span:\n",
    "        span.set_attributes({\"demo\": \"configuration\", \"debug_enabled\": True})\n",
    "        time.sleep(0.1)\n",
    "        print(\"‚úÖ Debug operation completed\")\n",
    "    \n",
    "    # Restore original debug setting\n",
    "    noveum_trace.configure(debug=original_debug)\n",
    "    print(f\"üîß Debug mode restored to: {original_debug}\")\n",
    "\n",
    "# Batch Processing and Performance Monitoring\n",
    "\n",
    "def test_batch_processing():\n",
    "    \"\"\"Test batch operations for performance.\"\"\"\n",
    "    print(\"üì¶ Testing Batch Processing...\")\n",
    "    \n",
    "    # Create multiple operations quickly to test batching\n",
    "    operations = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        with noveum_trace.trace_operation(f\"batch_operation_{i}\", operation_type=\"batch_demo\") as span:\n",
    "            span.set_attributes({\n",
    "                \"batch.operation_number\": i,\n",
    "                \"batch.total_operations\": 5,\n",
    "                \"operation.size\": \"small\"\n",
    "            })\n",
    "            time.sleep(0.05)  # Quick operations\n",
    "            operations.append(f\"operation_{i}\")\n",
    "            print(f\"üî∏ Batch operation {i+1}/5 completed\")\n",
    "    \n",
    "    print(f\"‚úÖ Batch processing completed: {len(operations)} operations\")\n",
    "    \n",
    "    # Force flush to send batched traces\n",
    "    try:\n",
    "        noveum_trace.flush()\n",
    "        print(\"üì§ Forced flush of batched traces\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Flush status: {e}\")\n",
    "\n",
    "# Error Handling and Edge Cases\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test error handling and edge cases.\"\"\"\n",
    "    print(\"‚ö†Ô∏è  Testing Error Handling...\")\n",
    "    \n",
    "    # Test error capture in traced function\n",
    "    @noveum_trace.trace(capture_errors=True, capture_stack_trace=True)\n",
    "    def operation_with_error(should_fail: bool = False):\n",
    "        if should_fail:\n",
    "            raise ValueError(\"This is a demo error for testing\")\n",
    "        return \"Success!\"\n",
    "    \n",
    "    # Test successful operation\n",
    "    try:\n",
    "        result = operation_with_error(should_fail=False)\n",
    "        print(f\"‚úÖ Successful operation: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "    \n",
    "    # Test error capture\n",
    "    try:\n",
    "        result = operation_with_error(should_fail=True)\n",
    "        print(f\"Unexpected success: {result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"‚úÖ Error captured successfully: {e}\")\n",
    "\n",
    "# Run all advanced feature tests\n",
    "print(\"üöÄ Testing Auto-Instrumentation and Advanced Features...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Auto-Instrumentation:\")\n",
    "instrumented_libs = test_auto_instrumentation()\n",
    "\n",
    "print(\"\\\\n2Ô∏è‚É£ Proxy Objects:\")\n",
    "test_traced_openai_client()\n",
    "test_traced_agent_proxy()\n",
    "\n",
    "print(\"\\\\n3Ô∏è‚É£ Manual Tracing:\")\n",
    "test_manual_tracing()\n",
    "\n",
    "print(\"\\\\n4Ô∏è‚É£ Advanced Configuration:\")\n",
    "test_advanced_configuration()\n",
    "\n",
    "print(\"\\\\n5Ô∏è‚É£ Batch Processing:\")\n",
    "test_batch_processing()\n",
    "\n",
    "print(\"\\\\n6Ô∏è‚É£ Error Handling:\")\n",
    "test_error_handling()\n",
    "\n",
    "print(\"\\\\n‚úÖ Advanced features testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Multi-Agent System\n",
    "\n",
    "Test multi-agent workflow tracing with the `@trace_agent` decorator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace_agent(agent_id=\"research_agent\")\n",
    "def research_agent(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Research agent that gathers information.\"\"\"\n",
    "    print(f\"üîç Research Agent: Processing query '{query}'\")\n",
    "\n",
    "    # Simulate research process\n",
    "    time.sleep(0.4)\n",
    "\n",
    "    # Mock research findings\n",
    "    findings = {\n",
    "        \"query\": query,\n",
    "        \"sources\": [\"source1.pdf\", \"source2.html\", \"source3.json\"],\n",
    "        \"key_points\": [\n",
    "            \"Point 1: Observability improves system reliability\",\n",
    "            \"Point 2: Tracing helps identify bottlenecks\",\n",
    "            \"Point 3: Monitoring enables proactive maintenance\"\n",
    "        ],\n",
    "        \"confidence\": 0.87,\n",
    "        \"research_time\": \"0.4s\"\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Research completed with {len(findings['sources'])} sources\")\n",
    "    return findings\n",
    "\n",
    "@trace_agent(agent_id=\"orchestrator\")\n",
    "def orchestrate_workflow(task: str) -> Dict[str, Any]:\n",
    "    \"\"\"Orchestrator agent that coordinates multiple agents.\"\"\"\n",
    "    print(f\"üé≠ Orchestrator: Starting workflow for task '{task}'\")\n",
    "\n",
    "    # Step 1: Research\n",
    "    print(\"\\nüîç Step 1: Research Phase\")\n",
    "    research_data = research_agent(task)\n",
    "\n",
    "    # Step 2: Analysis (simplified)\n",
    "    print(\"\\nüìä Step 2: Analysis Phase\")\n",
    "    analysis_data = {\n",
    "        \"insights\": [\"Observability is crucial\", \"Tracing provides insights\"],\n",
    "        \"quality_score\": 0.89\n",
    "    }\n",
    "\n",
    "    # Final orchestration result\n",
    "    workflow_result = {\n",
    "        \"task\": task,\n",
    "        \"workflow_id\": \"wf-001\",\n",
    "        \"phases_completed\": 2,\n",
    "        \"research_summary\": research_data[\"key_points\"],\n",
    "        \"analysis_summary\": analysis_data[\"insights\"],\n",
    "        \"overall_confidence\": research_data[\"confidence\"],\n",
    "        \"total_time\": \"1.0s\"\n",
    "    }\n",
    "\n",
    "    print(\"\\n‚úÖ Orchestrator: Workflow completed successfully\")\n",
    "    return workflow_result\n",
    "\n",
    "# Test the multi-agent workflow\n",
    "task = \"Analyze the importance of observability in AI systems\"\n",
    "workflow_result = orchestrate_workflow(task)\n",
    "print(\"\\nüé≠ Final Workflow Result:\")\n",
    "print(f\"Task: {workflow_result['task']}\")\n",
    "print(f\"Confidence: {workflow_result['overall_confidence']:.2f}\")\n",
    "print(f\"Phases: {workflow_result['phases_completed']}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: Tool Tracing\n",
    "\n",
    "Test tool tracing with the `@trace_tool` decorator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace_tool(tool_name=\"calculator\")\n",
    "def calculate(operation: str, a: float, b: float) -> Dict[str, Any]:\n",
    "    \"\"\"A calculator tool with tracing.\"\"\"\n",
    "    print(f\"üî¢ Calculator: Performing {operation} on {a} and {b}\")\n",
    "\n",
    "    operations = {\n",
    "        \"add\": lambda x, y: x + y,\n",
    "        \"subtract\": lambda x, y: x - y,\n",
    "        \"multiply\": lambda x, y: x * y,\n",
    "        \"divide\": lambda x, y: x / y if y != 0 else None\n",
    "    }\n",
    "\n",
    "    if operation not in operations:\n",
    "        return {\"error\": f\"Unknown operation: {operation}\"}\n",
    "\n",
    "    try:\n",
    "        result = operations[operation](a, b)\n",
    "        if result is None:\n",
    "            return {\"error\": \"Division by zero\"}\n",
    "\n",
    "        return {\n",
    "            \"operation\": operation,\n",
    "            \"operands\": [a, b],\n",
    "            \"result\": result,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"success\": False}\n",
    "\n",
    "@trace_tool(tool_name=\"text_analyzer\")\n",
    "def analyze_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Text analysis tool with tracing.\"\"\"\n",
    "    print(f\"üìù Text Analyzer: Analyzing text of length {len(text)}\")\n",
    "\n",
    "    # Simulate analysis\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    analysis = {\n",
    "        \"text_length\": len(text),\n",
    "        \"word_count\": len(text.split()),\n",
    "        \"sentence_count\": text.count('.') + text.count('!') + text.count('?'),\n",
    "        \"avg_word_length\": sum(len(word) for word in text.split()) / len(text.split()) if text.split() else 0\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Analysis complete: {analysis['word_count']} words, {analysis['sentence_count']} sentences\")\n",
    "    return analysis\n",
    "\n",
    "# Test tool tracing\n",
    "calc_result = calculate(\"multiply\", 15, 4)\n",
    "print(f\"\\nüî¢ Calculator Result: {calc_result}\")\n",
    "\n",
    "text_to_analyze = \"This is a sample text for testing the noveum-trace SDK. It contains multiple sentences!\"\n",
    "text_analysis = analyze_text(text_to_analyze)\n",
    "print(f\"\\nüìù Text Analysis Result: {text_analysis}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 8: Summary and Cleanup\n",
    "\n",
    "Test summary and cleanup of resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "# Test summary\n",
    "def print_test_summary():\n",
    "    \"\"\"Print a summary of all tests performed.\"\"\"\n",
    "    print(\"üìã NOVEUM TRACE SDK TEST SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Check SDK version\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(\"noveum-trace\").version\n",
    "        print(f\"‚úÖ SDK Version: {version}\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  Could not determine SDK version\")\n",
    "\n",
    "    # Environment check\n",
    "    print(f\"‚úÖ Python Version: {sys.version.split()[0]}\")\n",
    "    print(f\"‚úÖ Environment Variables: {'‚úì' if os.getenv('NOVEUM_API_KEY') else '‚úó'}\")\n",
    "\n",
    "    # Features tested\n",
    "    features_tested = [\n",
    "        \"Basic function tracing (@trace)\",\n",
    "        \"LLM call tracing (@trace_llm)\",\n",
    "        \"Agent workflow tracing (@trace_agent)\",\n",
    "        \"Tool tracing (@trace_tool)\",\n",
    "        \"Multi-agent orchestration\",\n",
    "        \"Error handling\",\n",
    "        \"Framework integration simulation\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nüß™ Features Tested:\")\n",
    "    for feature in features_tested:\n",
    "        print(f\"  ‚úÖ {feature}\")\n",
    "\n",
    "    print(\"\\nüéØ Key Results:\")\n",
    "    print(\"  üîß All decorators: Functional\")\n",
    "    print(\"  ü§ñ Multi-agent support: Functional\")\n",
    "    print(\"  üîå Framework integration: Simulated successfully\")\n",
    "\n",
    "    print(\"\\n‚úÖ All tests completed successfully!\")\n",
    "    print(\"\\nüìñ Next Steps:\")\n",
    "    print(\"  1. Set up your actual NOVEUM_API_KEY for production use\")\n",
    "    print(\"  2. Integrate with your LLM applications\")\n",
    "    print(\"  3. Set up dashboards and monitoring\")\n",
    "    print(\"  4. Configure alerting based on trace data\")\n",
    "\n",
    "# Clean up function\n",
    "def cleanup_resources():\n",
    "    \"\"\"Clean up any resources created during testing.\"\"\"\n",
    "    print(\"üßπ Cleaning up test resources...\")\n",
    "\n",
    "    try:\n",
    "        # Attempt to flush any pending traces\n",
    "        noveum_trace.flush()\n",
    "        print(\"‚úÖ Traces flushed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Trace flush: {e}\")\n",
    "\n",
    "    print(\"‚úÖ Cleanup completed\")\n",
    "\n",
    "# Run summary and cleanup\n",
    "print_test_summary()\n",
    "cleanup_resources()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You have successfully tested the Noveum Trace SDK from PyPI! \n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "- ‚úÖ Installation from PyPI\n",
    "- ‚úÖ Environment setup with proper API keys\n",
    "- ‚úÖ Basic function tracing with `@trace`\n",
    "- ‚úÖ LLM call tracing with `@trace_llm`\n",
    "- ‚úÖ Agent workflow tracing with `@trace_agent`\n",
    "- ‚úÖ Tool tracing with `@trace_tool`\n",
    "- ‚úÖ Multi-agent system orchestration\n",
    "- ‚úÖ Error handling and edge cases\n",
    "- ‚úÖ Performance considerations\n",
    "- ‚úÖ Framework integration patterns\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Production Setup**: Replace the dummy API key with your actual Noveum API key\n",
    "2. **Integration**: Integrate these patterns into your existing LLM applications\n",
    "3. **Monitoring**: Set up dashboards to monitor your traced applications\n",
    "4. **Optimization**: Use the trace data to optimize your application performance\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- üìö [Noveum Trace Documentation](https://docs.noveum.ai)\n",
    "- üêô [GitHub Repository](https://github.com/Noveum/noveum-trace)\n",
    "- üì¶ [PyPI Package](https://pypi.org/project/noveum-trace/)\n",
    "\n",
    "Happy tracing! üöÄ\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
