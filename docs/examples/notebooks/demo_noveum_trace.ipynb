{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Noveum Trace SDK Testing Notebook\n",
    "\n",
    "This notebook demonstrates the complete functionality of the Noveum Trace SDK installed from PyPI.\n",
    "\n",
    "## Features Tested:\n",
    "- Basic installation and setup\n",
    "- Environment variable configuration\n",
    "- Function tracing with decorators\n",
    "- LLM call tracing\n",
    "- Agent workflow tracing\n",
    "- Multi-agent systems\n",
    "- Tool tracing\n",
    "- Context managers\n",
    "- Streaming support\n",
    "- Integration examples\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Install Required Dependencies\n",
    "\n",
    "First, we'll install noveum-trace from PyPI along with some additional dependencies for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: noveum-trace in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (0.3.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: openai in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (1.97.0)\n",
      "Requirement already satisfied: anthropic in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (0.57.1)\n",
      "Requirement already satisfied: requests>=2.25.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from noveum-trace) (2.32.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: noveum-trace in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (0.3.3)\n",
      "Requirement already satisfied: pip in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (25.1.1)\n",
      "Requirement already satisfied: python-dotenv>=0.19.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from noveum-trace) (1.1.1)\n",
      "Requirement already satisfied: requests>=2.25.0 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from noveum-trace) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shashank/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages (from requests>=2.25.0->noveum-trace) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install noveum-trace from PyPI and testing dependencies\n",
    "%pip install noveum-trace python-dotenv openai anthropic\n",
    "%pip install --upgrade noveum-trace pip\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Set Up Environment Variables\n",
    "\n",
    "Configure the necessary environment variables for the SDK to work properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Current directory: /Users/shashank/Projects/Noveum/noveum-trace/docs/examples/notebooks\n",
      "üìÑ Found .env file: /Users/shashank/Projects/Noveum/noveum-trace/.env\n",
      "‚úÖ NOVEUM_API_KEY found in environment\n",
      "‚úÖ OPENAI_API_KEY found in environment\n",
      "\n",
      "üìã Environment Variables Status:\n",
      "NOVEUM_API_KEY: ‚úì\n",
      "OPENAI_API_KEY: ‚úì\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "# Note: Using robust path detection since __file__ is not available in Jupyter notebooks\n",
    "try:\n",
    "    # Try to get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    print(f\"üìÅ Current directory: {current_dir}\")\n",
    "    \n",
    "    # Look for .env file in current directory and parent directories\n",
    "    env_file_found = False\n",
    "    search_dir = current_dir\n",
    "    \n",
    "    for _ in range(5):  # Search up to 5 levels up\n",
    "        env_file = os.path.join(search_dir, '.env')\n",
    "        if os.path.exists(env_file):\n",
    "            print(f\"üìÑ Found .env file: {env_file}\")\n",
    "            load_dotenv(env_file)\n",
    "            env_file_found = True\n",
    "            break\n",
    "        search_dir = os.path.dirname(search_dir)\n",
    "        if search_dir == os.path.dirname(search_dir):  # Reached root\n",
    "            break\n",
    "    \n",
    "    if not env_file_found:\n",
    "        print(\"‚ÑπÔ∏è  No .env file found - continuing without it\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error loading .env file: {e}\")\n",
    "    print(\"‚ÑπÔ∏è  Continuing without .env file\")\n",
    "\n",
    "# Set up environment variables for testing\n",
    "# Replace with your actual API key or set in .env file\n",
    "if not os.getenv('NOVEUM_API_KEY'):\n",
    "    # For testing purposes, you can set a dummy API key\n",
    "    # In production, use your actual Noveum API key\n",
    "    os.environ['NOVEUM_API_KEY'] = 'noveum_API_KEY'\n",
    "    print(\"‚ö†Ô∏è  Using dummy API key for testing. Set NOVEUM_API_KEY environment variable for production use.\")\n",
    "else:\n",
    "    print(\"‚úÖ NOVEUM_API_KEY found i\")\n",
    "\n",
    "# Optional: Set OpenAI API key for LLM examples\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"‚ÑπÔ∏è  OPENAI_API_KEY not found. LLM examples will use mock responses.\")\n",
    "else:\n",
    "    print(\"‚úÖ OPENAI_API_KEY found in environment\")\n",
    "\n",
    "print(\"\\nüìã Environment Variables Status:\")\n",
    "print(f\"NOVEUM_API_KEY: {'‚úì' if os.getenv('NOVEUM_API_KEY') else '‚úó'}\")\n",
    "print(f\"OPENAI_API_KEY: {'‚úì' if os.getenv('OPENAI_API_KEY') else '‚úó'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîÑ FLUSH HELPER: Automatic Trace Sending\n",
    "\n",
    "To ensure all traces are sent immediately to your endpoint, we'll create a helper function that can be called after any traced operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Flush helper functions initialized\n",
      "üìã Usage:\n",
      "  - Call flush_traces('operation_name') after any traced operation\n",
      "  - Use @auto_flush_decorator on functions containing traced operations\n",
      "  - This ensures immediate trace sending to your endpoint\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH HELPER FUNCTIONS FOR IMMEDIATE TRACE SENDING\n",
    "\n",
    "def flush_traces(operation_name=\"Operation\"):\n",
    "    \"\"\"\n",
    "    Helper function to flush traces immediately to endpoint.\n",
    "    Call this after any traced operation to ensure traces are sent right away.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        noveum_trace.flush()\n",
    "        print(f\"üì§ ‚úÖ {operation_name} traces flushed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"üì§ ‚ö†Ô∏è  {operation_name} flush warning: {e}\")\n",
    "\n",
    "def auto_flush_decorator(func):\n",
    "    \"\"\"\n",
    "    Decorator that automatically flushes traces after function execution.\n",
    "    Use this for any function that contains traced operations.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        flush_traces(func.__name__)\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Test the flush helper\n",
    "print(\"üîß Flush helper functions initialized\")\n",
    "print(\"üìã Usage:\")\n",
    "print(\"  - Call flush_traces('operation_name') after any traced operation\")\n",
    "print(\"  - Use @auto_flush_decorator on functions containing traced operations\")\n",
    "print(\"  - This ensures immediate trace sending to your endpoint\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Initialize the SDK\n",
    "\n",
    "Initialize the Noveum Trace SDK with your project configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:35 - noveum_trace.transport.batch_processor - INFO - üîÑ Batch processor background thread started (batch_size=10, timeout=2.0s)\n",
      "2025-07-29 23:47:35 - noveum_trace.transport.batch_processor - INFO - Batch processor started with batch_size=10\n",
      "2025-07-29 23:47:35 - noveum_trace.transport.http_transport - INFO - HTTP transport initialized for endpoint: https://api.noveum.ai/api\n",
      "2025-07-29 23:47:35 - noveum_trace.core.client - INFO - Noveum Trace client initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Noveum Trace SDK initialized successfully!\n",
      "üìä Project: jupyter-test-project\n",
      "üîß Environment: development\n",
      "üåê Endpoint: https://api.noveum.ai/api/v1/traces (auto-appended)\n",
      "üîç Debug logging enabled - check console for HTTP request details\n",
      "üìã Config verified - Endpoint: https://api.noveum.ai/api\n",
      "üì¶ Batch size: 10\n",
      "‚è±Ô∏è  Batch timeout: 2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - ‚è∞ TIMEOUT TRIGGER: Sending batch due to timeout (9.4s >= 2.0s)\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 2 traces via send_callback\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 2 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 2 traces\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 2 traces via callback\n",
      "2025-07-29 23:47:47 - noveum_trace.transport.batch_processor - INFO - ‚è∞ TIMEOUT TRIGGER: Sending batch due to timeout (2.1s >= 2.0s)\n",
      "2025-07-29 23:47:47 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 2 traces via send_callback\n",
      "2025-07-29 23:47:47 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 2 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:47 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:47 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 2 traces\n",
      "2025-07-29 23:47:47 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 2 traces via callback\n",
      "2025-07-29 23:47:49 - noveum_trace.transport.batch_processor - INFO - ‚è∞ TIMEOUT TRIGGER: Sending batch due to timeout (2.2s >= 2.0s)\n",
      "2025-07-29 23:47:49 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 2 traces via send_callback\n",
      "2025-07-29 23:47:49 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 2 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:50 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:50 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 2 traces\n",
      "2025-07-29 23:47:50 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 2 traces via callback\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - üì¶ SIZE TRIGGER: Sending batch due to size limit (10 >= 10)\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 10 traces via send_callback\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 10 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 10 traces\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 10 traces via callback\n",
      "2025-07-29 23:47:58 - noveum_trace.transport.batch_processor - INFO - ‚è∞ TIMEOUT TRIGGER: Sending batch due to timeout (5.3s >= 2.0s)\n",
      "2025-07-29 23:47:58 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 2 traces via send_callback\n",
      "2025-07-29 23:47:58 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 2 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:58 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:58 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 2 traces\n",
      "2025-07-29 23:47:58 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 2 traces via callback\n",
      "2025-07-29 23:48:20 - noveum_trace.transport.batch_processor - INFO - ‚è∞ TIMEOUT TRIGGER: Sending batch due to timeout (21.8s >= 2.0s)\n",
      "2025-07-29 23:48:20 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 2 traces via send_callback\n",
      "2025-07-29 23:48:20 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 2 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:48:20 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:48:20 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 2 traces\n",
      "2025-07-29 23:48:20 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 2 traces via callback\n",
      "2025-07-29 23:48:34 - noveum_trace.transport.batch_processor - INFO - ‚è∞ TIMEOUT TRIGGER: Sending batch due to timeout (14.4s >= 2.0s)\n",
      "2025-07-29 23:48:34 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 2 traces via send_callback\n",
      "2025-07-29 23:48:34 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 2 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:48:34 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:48:34 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 2 traces\n",
      "2025-07-29 23:48:34 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 2 traces via callback\n",
      "2025-07-29 23:48:36 - noveum_trace.transport.batch_processor - INFO - ‚è∞ TIMEOUT TRIGGER: Sending batch due to timeout (2.3s >= 2.0s)\n",
      "2025-07-29 23:48:36 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 1 traces via send_callback\n",
      "2025-07-29 23:48:36 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 1 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:48:37 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:48:37 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 1 traces\n",
      "2025-07-29 23:48:37 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 1 traces via callback\n"
     ]
    }
   ],
   "source": [
    "import noveum_trace\n",
    "from noveum_trace import trace, trace_agent, trace_llm, trace_tool\n",
    "import logging\n",
    "\n",
    "# Enable detailed logging to debug transport issues\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "transport_logger = logging.getLogger('noveum_trace.transport')\n",
    "transport_logger.setLevel(logging.DEBUG)\n",
    "ENDPOINT = \"https://api.noveum.ai/api\"\n",
    "\n",
    "# Initialize the SDK with proper endpoint configuration\n",
    "try:\n",
    "    # IMPORTANT: The SDK will append \"/v1/traces\" to your endpoint\n",
    "    # So \"https://noveum-trace.free.beeceptor.com\" becomes \"https://noveum-trace.free.beeceptor.com/v1/traces\"\n",
    "    noveum_trace.init(\n",
    "        api_key=os.getenv('NOVEUM_API_KEY'),\n",
    "        project=\"jupyter-test-project\", \n",
    "        environment=\"development\",\n",
    "        endpoint=ENDPOINT,  # SDK will add /v1/traces automatically\n",
    "        debug=True,  # Enable debug mode for testing\n",
    "        \n",
    "        # Transport configuration for better reliability\n",
    "        transport_config={\n",
    "            \"timeout\": 10,           # 10 second timeout\n",
    "            \"retry_attempts\": 2,     # Retry failed requests 2 times\n",
    "            \"batch_size\": 10,        # Smaller batches for demo\n",
    "            \"batch_timeout\": 2.0,    # Send batches every 2 seconds\n",
    "            \"compression\": False     # Disable compression for debugging\n",
    "        },\n",
    "        \n",
    "        # Tracing configuration\n",
    "        tracing_config={\n",
    "            \"sample_rate\": 1.0,      # Trace 100% of operations\n",
    "            \"capture_errors\": True,\n",
    "            \"capture_stack_traces\": True\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Noveum Trace SDK initialized successfully!\")\n",
    "    print(\"üìä Project: jupyter-test-project\")\n",
    "    print(\"üîß Environment: development\") \n",
    "    print(f\"üåê Endpoint: {ENDPOINT}/v1/traces (auto-appended)\")\n",
    "    print(\"üîç Debug logging enabled - check console for HTTP request details\")\n",
    "    \n",
    "    # Get the current configuration to verify settings\n",
    "    config = noveum_trace.get_config()\n",
    "    print(f\"üìã Config verified - Endpoint: {config.transport.endpoint}\")\n",
    "    print(f\"üì¶ Batch size: {config.transport.batch_size}\")\n",
    "    print(f\"‚è±Ô∏è  Batch timeout: {config.transport.batch_timeout}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing SDK: {e}\")\n",
    "    print(\"Continuing with demo - traces will be logged locally\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: LLM Call Tracing\n",
    "\n",
    "Test LLM call tracing with the `@trace_llm` decorator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:36 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Batch processor flush completed in 0.00s\n",
      "2025-07-29 23:47:36 - noveum_trace.transport.http_transport - INFO - HTTP transport flush completed\n",
      "2025-07-29 23:47:36 - noveum_trace.core.client - INFO - Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ LLM Call Tracing (call_language_model) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER LLM CALL TRACING\n",
    "# This ensures the @trace_llm decorator traces are sent immediately\n",
    "\n",
    "flush_traces(\"LLM Call Tracing (call_language_model)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Calling gpt-4 with prompt: Explain the benefits of observability in AI system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-97576fa8-e782-408d-9756-ba945bcd585b', 'json_data': {'messages': [{'role': 'user', 'content': 'Explain the benefits of observability in AI systems.'}], 'model': 'gpt-4', 'max_tokens': 100}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1331ccc20>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x132c6c3b0> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x133118f50>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 29 Jul 2025 18:17:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'magicapi-inc'), (b'openai-processing-ms', b'5405'), (b'openai-project', b'proj_5py2FGvCF1HwegoK03gVwlEm'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'5411'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'39985'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'22ms'), (b'x-request-id', b'req_c9d0b546d0f05b8bce3c1bf8093f54e9'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=A2S_rJl3imrUhkuchscnI4CmyrHSKQRx4HtrG_XTRVA-1753813063-1.0.1.1-_JCu9x5EWD4K6q8H8Kla__xdlJ9TbpCbJjsmJ86Wc2lmjYdtHMeBnANuje7Dwg_.RrM04BkuOQHjXp.kZ4bzHxUwk6DIjJWBqtL9Po2A7L0; path=/; expires=Tue, 29-Jul-25 18:47:43 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=sztlpabeQqe7jRIEhXbwtYhSoDcUMUGFT2yW9F5vEpI-1753813063195-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'966e9d395bd47eaa-MAA'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Tue, 29 Jul 2025 18:17:43 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'magicapi-inc'), ('openai-processing-ms', '5405'), ('openai-project', 'proj_5py2FGvCF1HwegoK03gVwlEm'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '5411'), ('x-ratelimit-limit-requests', '5000'), ('x-ratelimit-limit-tokens', '40000'), ('x-ratelimit-remaining-requests', '4999'), ('x-ratelimit-remaining-tokens', '39985'), ('x-ratelimit-reset-requests', '12ms'), ('x-ratelimit-reset-tokens', '22ms'), ('x-request-id', 'req_c9d0b546d0f05b8bce3c1bf8093f54e9'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=A2S_rJl3imrUhkuchscnI4CmyrHSKQRx4HtrG_XTRVA-1753813063-1.0.1.1-_JCu9x5EWD4K6q8H8Kla__xdlJ9TbpCbJjsmJ86Wc2lmjYdtHMeBnANuje7Dwg_.RrM04BkuOQHjXp.kZ4bzHxUwk6DIjJWBqtL9Po2A7L0; path=/; expires=Tue, 29-Jul-25 18:47:43 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=sztlpabeQqe7jRIEhXbwtYhSoDcUMUGFT2yW9F5vEpI-1753813063195-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '966e9d395bd47eaa-MAA'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_c9d0b546d0f05b8bce3c1bf8093f54e9\n",
      "2025-07-29 23:47:43 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_call_language_model (ID: 179ccbf1-6c15-430b-b7d6-f229806329b7) - 1 spans\n",
      "2025-07-29 23:47:43 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_call_language_model (ID: 179ccbf1-6c15-430b-b7d6-f229806329b7) - 1 spans\n",
      "2025-07-29 23:47:43 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 179ccbf1-6c15-430b-b7d6-f229806329b7\n",
      "2025-07-29 23:47:43 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 179ccbf1-6c15-430b-b7d6-f229806329b7 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ LLM Response: 1. Data Analysis: Observability allows AI systems to produce huge amounts of data that can be analyzed to understand the system's functionality, assess performance, and inform decision-making.\n",
      "\n",
      "2. Error Detection: With observability, it's easier to understand and monitor the internal states of an AI system. This facilitates early detection of any errors, failures, or anomalies that may occur in the system.\n",
      "\n",
      "3. Improved Debugging: If things go wrong in an AI system, observability helps figure out why it\n"
     ]
    }
   ],
   "source": [
    "# Mock LLM responses for testing (replace with actual API calls if you have keys)\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from noveum_trace import trace_llm\n",
    "\n",
    "def mock_openai_call(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"Mock OpenAI API call for testing.\"\"\"\n",
    "    responses = [\n",
    "        \"This is a mock response from the language model.\",\n",
    "        \"Here's a simulated AI response for testing purposes.\",\n",
    "        \"Mock LLM output to demonstrate tracing functionality.\"\n",
    "    ]\n",
    "    time.sleep(0.3)  # Simulate API call latency\n",
    "    return random.choice(responses)\n",
    "\n",
    "@trace_llm\n",
    "def call_language_model(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"Call a language model with tracing.\"\"\"\n",
    "    print(f\"ü§ñ Calling {model} with prompt: {prompt[:50]}...\")\n",
    "\n",
    "    # Use real OpenAI API if available, otherwise use mock\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        try:\n",
    "            import openai\n",
    "            client = openai.OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  OpenAI API call failed: {e}. Using mock response.\")\n",
    "            return mock_openai_call(prompt, model)\n",
    "    else:\n",
    "        print(\"üìù Using mock LLM response (no API key provided)\")\n",
    "        return mock_openai_call(prompt, model)\n",
    "\n",
    "# Test LLM tracing\n",
    "prompt = \"Explain the benefits of observability in AI systems.\"\n",
    "response = call_language_model(prompt)\n",
    "print(f\"\\nüéØ LLM Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:43 - noveum_trace.transport.batch_processor - INFO - üîÑ FLUSH: Sending current batch of 1 traces\n",
      "2025-07-29 23:47:43 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 1 traces via send_callback\n",
      "2025-07-29 23:47:43 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 1 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.noveum.ai:443\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 1 traces\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 1 traces via callback\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Batch processor flush completed in 0.00s\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.http_transport - INFO - HTTP transport flush completed\n",
      "2025-07-29 23:47:44 - noveum_trace.core.client - INFO - Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Enhanced LLM Tracing (Anthropic + OpenAI + Google) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER ENHANCED LLM TRACING\n",
    "# This ensures all enhanced LLM traces (Anthropic, OpenAI, Google) are sent immediately\n",
    "\n",
    "flush_traces(\"Enhanced LLM Tracing (Anthropic + OpenAI + Google)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5.1: Enhanced LLM Tracing Examples\n",
    "\n",
    "Demonstrate various LLM tracing features including different providers, metadata, and advanced parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:44 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Batch processor flush completed in 0.00s\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.http_transport - INFO - HTTP transport flush completed\n",
      "2025-07-29 23:47:44 - noveum_trace.core.client - INFO - Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Retrieval System Tracing (Vector + Keyword + Hybrid) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER RETRIEVAL SYSTEM TRACING  \n",
    "# This ensures all @trace_retrieval decorator traces are sent immediately\n",
    "\n",
    "flush_traces(\"Retrieval System Tracing (Vector + Keyword + Hybrid)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing Enhanced LLM Tracing...\n",
      "üß† Calling claude-3-haiku with prompt: What are the key benefits of AI observability?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:44 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_call_anthropic (ID: 910f72c6-d46b-468f-a373-98529c9b2649) - 1 spans\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_call_anthropic (ID: 910f72c6-d46b-468f-a373-98529c9b2649) - 1 spans\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 910f72c6-d46b-468f-a373-98529c9b2649\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 910f72c6-d46b-468f-a373-98529c9b2649 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Anthropic Response: Observability in AI systems provides critical insights into model behavior and performance.\n",
      "üìã Enhanced LLM call with metadata: Summarize the importance of AI monitorin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:44 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_call_llm_with_metadata (ID: a5669c96-060a-40ea-8666-71df7e5154c0) - 1 spans\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_call_llm_with_metadata (ID: a5669c96-060a-40ea-8666-71df7e5154c0) - 1 spans\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace a5669c96-060a-40ea-8666-71df7e5154c0\n",
      "2025-07-29 23:47:44 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace a5669c96-060a-40ea-8666-71df7e5154c0 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Enhanced Response: Enhanced response for: Summarize the import...\n",
      "üü¢ Calling gemini-pro with PII protection: How does tracing help with AI compliance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_call_google_ai (ID: 2ee24e92-cad2-47cf-8a48-8ca65f85664b) - 1 spans\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_call_google_ai (ID: 2ee24e92-cad2-47cf-8a48-8ca65f85664b) - 1 spans\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 2ee24e92-cad2-47cf-8a48-8ca65f85664b\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 2ee24e92-cad2-47cf-8a48-8ca65f85664b successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Google AI Response: Google AI response with PII redaction enabled for sensitive data handling.\n",
      "\n",
      "‚úÖ Enhanced LLM tracing completed!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced LLM Tracing Examples\n",
    "\n",
    "# Test different LLM providers with comprehensive metadata\n",
    "@trace_llm(provider=\"anthropic\", capture_tokens=True, estimate_costs=True)\n",
    "def call_anthropic(prompt: str, model: str = \"claude-3-haiku\") -> str:\n",
    "    \"\"\"Call Anthropic Claude with tracing.\"\"\"\n",
    "    print(f\"üß† Calling {model} with prompt: {prompt[:50]}...\")\n",
    "    \n",
    "    # Mock Anthropic response\n",
    "    time.sleep(0.4)\n",
    "    responses = [\n",
    "        \"Observability in AI systems provides critical insights into model behavior and performance.\",\n",
    "        \"Tracing AI workflows enables debugging, optimization, and compliance monitoring.\",\n",
    "        \"Comprehensive monitoring helps ensure AI system reliability and user trust.\"\n",
    "    ]\n",
    "    return random.choice(responses)\n",
    "\n",
    "# Test with custom metadata and tags\n",
    "@trace_llm(\n",
    "    provider=\"openai\", \n",
    "    capture_prompts=True, \n",
    "    capture_completions=True,\n",
    "    metadata={\"experiment\": \"demo\", \"version\": \"1.0\"},\n",
    "    tags={\"environment\": \"notebook\", \"user\": \"demo\"}\n",
    ")\n",
    "def call_llm_with_metadata(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"LLM call with custom metadata and tags.\"\"\"\n",
    "    print(f\"üìã Enhanced LLM call with metadata: {prompt[:40]}...\")\n",
    "    time.sleep(0.3)\n",
    "    return f\"Enhanced response for: {prompt[:20]}...\"\n",
    "\n",
    "# Test Google AI provider\n",
    "@trace_llm(provider=\"google\", capture_tokens=True, redact_pii=True)\n",
    "def call_google_ai(prompt: str, model: str = \"gemini-pro\") -> str:\n",
    "    \"\"\"Call Google AI with PII redaction.\"\"\"\n",
    "    print(f\"üü¢ Calling {model} with PII protection: {prompt[:40]}...\")\n",
    "    time.sleep(0.5)\n",
    "    return \"Google AI response with PII redaction enabled for sensitive data handling.\"\n",
    "\n",
    "# Test various LLM providers\n",
    "print(\"ü§ñ Testing Enhanced LLM Tracing...\")\n",
    "\n",
    "anthropic_response = call_anthropic(\"What are the key benefits of AI observability?\")\n",
    "print(f\"\\nüß† Anthropic Response: {anthropic_response}\")\n",
    "\n",
    "metadata_response = call_llm_with_metadata(\"Summarize the importance of AI monitoring\")\n",
    "print(f\"\\nüìã Enhanced Response: {metadata_response}\")\n",
    "\n",
    "google_response = call_google_ai(\"How does tracing help with AI compliance?\")\n",
    "print(f\"\\nüü¢ Google AI Response: {google_response}\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced LLM tracing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Batch processor flush completed in 0.00s\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - HTTP transport flush completed\n",
      "2025-07-29 23:47:45 - noveum_trace.core.client - INFO - Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Enhanced Multi-Agent System (Data Analyst + Content Curator + Synthesis + Orchestrator) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER ENHANCED MULTI-AGENT SYSTEM TRACING\n",
    "# This ensures all @trace_agent decorator traces are sent immediately\n",
    "\n",
    "flush_traces(\"Enhanced Multi-Agent System (Data Analyst + Content Curator + Synthesis + Orchestrator)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5.2: Retrieval System Tracing\n",
    "\n",
    "Test retrieval operations with the `@trace_retrieval` decorator for RAG systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - üîÑ FLUSH: Sending current batch of 1 traces\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 1 traces via send_callback\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 1 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 1 traces\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 1 traces via callback\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Batch processor flush completed in 0.00s\n",
      "2025-07-29 23:47:45 - noveum_trace.transport.http_transport - INFO - HTTP transport flush completed\n",
      "2025-07-29 23:47:45 - noveum_trace.core.client - INFO - Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Context Managers and Streaming (trace_llm_call + trace_agent_operation + trace_operation + streaming) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER CONTEXT MANAGERS AND STREAMING\n",
    "# This ensures all context manager traces are sent immediately\n",
    "\n",
    "flush_traces(\"Context Managers and Streaming (trace_llm_call + trace_agent_operation + trace_operation + streaming)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Retrieval System Tracing...\n",
      "üîç Vector Search: Finding top 3 results for 'benefits of AI observability'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:46 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_vector_search (ID: 848c03f5-bc0b-44ee-9c4f-49749a17e340) - 1 spans\n",
      "2025-07-29 23:47:46 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_vector_search (ID: 848c03f5-bc0b-44ee-9c4f-49749a17e340) - 1 spans\n",
      "2025-07-29 23:47:46 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 848c03f5-bc0b-44ee-9c4f-49749a17e340\n",
      "2025-07-29 23:47:46 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 848c03f5-bc0b-44ee-9c4f-49749a17e340 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      "üîç Vector Search Results: 3 documents\n",
      "üîé Keyword Search: 'AI monitoring' with filters: {'category': 'technical', 'year': 2024}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:46 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_keyword_search (ID: 028e93f2-d65e-46c2-a53d-6c43cc5e2557) - 1 spans\n",
      "2025-07-29 23:47:46 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_keyword_search (ID: 028e93f2-d65e-46c2-a53d-6c43cc5e2557) - 1 spans\n",
      "2025-07-29 23:47:46 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 028e93f2-d65e-46c2-a53d-6c43cc5e2557\n",
      "2025-07-29 23:47:46 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 028e93f2-d65e-46c2-a53d-6c43cc5e2557 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Keyword Search Results: 3 matches\n",
      "\n",
      "‚úÖ Retrieval tracing completed!\n"
     ]
    }
   ],
   "source": [
    "# Import the retrieval decorator\n",
    "from noveum_trace import trace_retrieval\n",
    "from typing import Dict, Any, Optional\n",
    "import time\n",
    "\n",
    "# Vector search with comprehensive tracing\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"vector_search\",\n",
    "    index_name=\"knowledge_base\",\n",
    "    capture_query=True,\n",
    "    capture_results=True,\n",
    "    capture_scores=True,\n",
    "    metadata={\"index_version\": \"v2.1\", \"embedding_model\": \"text-embedding-ada-002\"}\n",
    ")\n",
    "def vector_search(query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Perform vector search with tracing.\"\"\"\n",
    "    print(f\"üîç Vector Search: Finding top {top_k} results for '{query}'\")\n",
    "    \n",
    "    # Simulate vector search\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    # Mock search results with scores\n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        results.append({\n",
    "            \"document_id\": f\"doc_{i+1}\",\n",
    "            \"content\": f\"Relevant content for '{query}' - document {i+1}\",\n",
    "            \"score\": 0.95 - (i * 0.1),\n",
    "            \"metadata\": {\"source\": f\"source_{i+1}.pdf\", \"page\": i+1}\n",
    "        })\n",
    "    \n",
    "    search_result = {\n",
    "        \"query\": query,\n",
    "        \"total_results\": top_k,\n",
    "        \"results\": results,\n",
    "        \"search_time_ms\": 300,\n",
    "        \"index_stats\": {\"total_docs\": 10000, \"dimensions\": 1536}\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(results)} relevant documents\")\n",
    "    return search_result\n",
    "\n",
    "# Keyword search with metadata capture\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"keyword_search\",\n",
    "    index_name=\"text_corpus\",\n",
    "    capture_metadata=True,\n",
    "    tags={\"search_type\": \"fulltext\", \"language\": \"en\"}\n",
    ")\n",
    "def keyword_search(query: str, filters: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Perform keyword search with filtering.\"\"\"\n",
    "    print(f\"üîé Keyword Search: '{query}' with filters: {filters}\")\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    # Mock keyword search results\n",
    "    results = [\n",
    "        {\"doc_id\": \"kw_1\", \"title\": \"AI Observability Guide\", \"snippet\": \"...observability in AI...\"},\n",
    "        {\"doc_id\": \"kw_2\", \"title\": \"Tracing Best Practices\", \"snippet\": \"...tracing methodologies...\"},\n",
    "        {\"doc_id\": \"kw_3\", \"title\": \"Monitoring AI Systems\", \"snippet\": \"...monitoring strategies...\"}\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"filters\": filters or {},\n",
    "        \"results\": results,\n",
    "        \"total_matches\": len(results)\n",
    "    }\n",
    "\n",
    "# Test retrieval operations\n",
    "print(\"üîç Testing Retrieval System Tracing...\")\n",
    "\n",
    "# Test vector search\n",
    "vector_result = vector_search(\"benefits of AI observability\", top_k=3)\n",
    "print(f\"\\nüîç Vector Search Results: {len(vector_result['results'])} documents\")\n",
    "\n",
    "# Test keyword search with filters\n",
    "keyword_result = keyword_search(\"AI monitoring\", filters={\"category\": \"technical\", \"year\": 2024})\n",
    "print(f\"\\nüîé Keyword Search Results: {keyword_result['total_matches']} matches\")\n",
    "\n",
    "print(\"\\n‚úÖ Retrieval tracing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6.1: Enhanced Multi-Agent System\n",
    "\n",
    "Test advanced multi-agent workflows with specialized agents and complex coordination patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing Enhanced Multi-Agent System...\n",
      "üìä Data Analyst: Analyzing dataset with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:48 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_data_analyst_agent (ID: ce017c4e-f102-4907-9a86-acd5637eac28) - 1 spans\n",
      "2025-07-29 23:47:48 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_data_analyst_agent (ID: ce017c4e-f102-4907-9a86-acd5637eac28) - 1 spans\n",
      "2025-07-29 23:47:48 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace ce017c4e-f102-4907-9a86-acd5637eac28\n",
      "2025-07-29 23:47:48 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace ce017c4e-f102-4907-9a86-acd5637eac28 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis complete: 0.92 quality score\n",
      "\n",
      "üìä Data Analysis: 0.92 quality score\n",
      "üìù Content Curator: Processing 4 content items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:48 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_content_curator_agent (ID: f8d71cb7-8109-4b71-a644-8fc3534211b4) - 1 spans\n",
      "2025-07-29 23:47:48 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_content_curator_agent (ID: f8d71cb7-8109-4b71-a644-8fc3534211b4) - 1 spans\n",
      "2025-07-29 23:47:48 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace f8d71cb7-8109-4b71-a644-8fc3534211b4\n",
      "2025-07-29 23:47:48 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace f8d71cb7-8109-4b71-a644-8fc3534211b4 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Curated 3/4 items\n",
      "\n",
      "üìù Content Curation: 3/4 items retained\n",
      "\n",
      "‚úÖ Enhanced multi-agent system testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Multi-Agent System Examples\n",
    "# Import required types to ensure they're available in this cell\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Specialized agents with different roles and capabilities\n",
    "@trace_agent(\n",
    "    agent_id=\"data_analyst\",\n",
    "    role=\"analyst\",\n",
    "    agent_type=\"specialist\",\n",
    "    capabilities=[\"data_analysis\", \"statistical_modeling\", \"visualization\"],\n",
    "    capture_reasoning=True,\n",
    "    metadata={\"specialization\": \"quantitative_analysis\", \"confidence_threshold\": 0.8}\n",
    ")\n",
    "def data_analyst_agent(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Specialized data analysis agent.\"\"\"\n",
    "    print(f\"üìä Data Analyst: Analyzing dataset with {len(data.get('samples', []))} samples\")\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Mock data analysis\n",
    "    analysis = {\n",
    "        \"agent_id\": \"data_analyst\",\n",
    "        \"analysis_type\": \"quantitative\",\n",
    "        \"findings\": {\n",
    "            \"data_quality\": 0.92,\n",
    "            \"pattern_confidence\": 0.87,\n",
    "            \"anomalies_detected\": 3,\n",
    "            \"recommendations\": [\n",
    "                \"Data quality is high with 92% confidence\",\n",
    "                \"3 anomalies detected requiring investigation\",\n",
    "                \"Statistical patterns show strong correlation\"\n",
    "            ]\n",
    "        },\n",
    "        \"reasoning_steps\": [\n",
    "            \"Loaded and validated input data\",\n",
    "            \"Applied statistical analysis methods\", \n",
    "            \"Identified patterns and anomalies\",\n",
    "            \"Generated confidence-based recommendations\"\n",
    "        ],\n",
    "        \"processing_time\": 0.5\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Analysis complete: {analysis['findings']['data_quality']:.2f} quality score\")\n",
    "    return analysis\n",
    "\n",
    "@trace_agent(\n",
    "    agent_id=\"content_curator\",\n",
    "    role=\"curator\",\n",
    "    agent_type=\"content_specialist\", \n",
    "    capabilities=[\"content_filtering\", \"quality_assessment\", \"summarization\"],\n",
    "    capture_tools=True\n",
    ")\n",
    "def content_curator_agent(content_list: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Content curation and quality assessment agent.\"\"\"\n",
    "    print(f\"üìù Content Curator: Processing {len(content_list)} content items\")\n",
    "    \n",
    "    time.sleep(0.4)\n",
    "    \n",
    "    # Mock content curation using tools\n",
    "    high_quality_content = []\n",
    "    for i, content in enumerate(content_list):\n",
    "        if i < 3:  # Mock: keep first 3 as high quality\n",
    "            high_quality_content.append({\n",
    "                **content,\n",
    "                \"quality_score\": 0.9 - (i * 0.05),\n",
    "                \"curation_reason\": \"Meets quality standards\"\n",
    "            })\n",
    "    \n",
    "    curation_result = {\n",
    "        \"agent_id\": \"content_curator\",\n",
    "        \"input_count\": len(content_list),\n",
    "        \"curated_count\": len(high_quality_content),\n",
    "        \"curated_content\": high_quality_content,\n",
    "        \"tools_used\": [\"quality_scorer\", \"content_filter\", \"summarizer\"],\n",
    "        \"curation_metrics\": {\n",
    "            \"retention_rate\": len(high_quality_content) / len(content_list),\n",
    "            \"average_quality\": sum(item[\"quality_score\"] for item in high_quality_content) / len(high_quality_content)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Curated {len(high_quality_content)}/{len(content_list)} items\")\n",
    "    return curation_result\n",
    "\n",
    "# Test the enhanced multi-agent system\n",
    "print(\"ü§ñ Testing Enhanced Multi-Agent System...\")\n",
    "\n",
    "# Mock data for demonstration\n",
    "sample_data = {\"samples\": [f\"sample_{i}\" for i in range(100)]}\n",
    "sample_content = [\n",
    "    {\"id\": 1, \"title\": \"AI Observability\", \"content\": \"Content about observability\"},\n",
    "    {\"id\": 2, \"title\": \"Tracing Systems\", \"content\": \"Content about tracing\"},\n",
    "    {\"id\": 3, \"title\": \"Monitoring Tools\", \"content\": \"Content about monitoring\"},\n",
    "    {\"id\": 4, \"title\": \"Low Quality\", \"content\": \"Poor content\"}\n",
    "]\n",
    "\n",
    "# Test individual agents\n",
    "analyst_result = data_analyst_agent(sample_data)\n",
    "print(f\"\\nüìä Data Analysis: {analyst_result['findings']['data_quality']:.2f} quality score\")\n",
    "\n",
    "curator_result = content_curator_agent(sample_content)\n",
    "print(f\"\\nüìù Content Curation: {curator_result['curated_count']}/{curator_result['input_count']} items retained\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced multi-agent system testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7.1: Context Managers and Streaming\n",
    "\n",
    "Test context managers for inline tracing and streaming LLM responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Context Managers...\n",
      "\\n1Ô∏è‚É£ Context Manager Examples:\n",
      "üîÑ Processing user query: 'What are the benefits of AI observabilit...'\n",
      "ü§ñ Making LLM call within context manager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:50 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_llm.query_processing (ID: 2681342c-1a34-48c0-acca-0a98c49cb0d5) - 1 spans\n",
      "2025-07-29 23:47:50 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_llm.query_processing (ID: 2681342c-1a34-48c0-acca-0a98c49cb0d5) - 1 spans\n",
      "2025-07-29 23:47:50 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 2681342c-1a34-48c0-acca-0a98c49cb0d5\n",
      "2025-07-29 23:47:50 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 2681342c-1a34-48c0-acca-0a98c49cb0d5 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM response generated: 57 characters\n",
      "üì§ Final response: Final: Processed response for: what are the benefi...\n",
      "ü§ñ Agent Task: 'Analyze system performance metrics'\n",
      "‚öôÔ∏è  Executing agent task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:50 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_agent.task_execution (ID: 8ed7e413-57bc-44ab-be2b-efcdd13a6767) - 1 spans\n",
      "2025-07-29 23:47:50 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_agent.task_execution (ID: 8ed7e413-57bc-44ab-be2b-efcdd13a6767) - 1 spans\n",
      "2025-07-29 23:47:50 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 8ed7e413-57bc-44ab-be2b-efcdd13a6767\n",
      "2025-07-29 23:47:50 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 8ed7e413-57bc-44ab-be2b-efcdd13a6767 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent task completed with 95.0% success rate\n",
      "üîß Starting complex operation...\n",
      "üì• Step 1: Loading data...\n",
      "‚öôÔ∏è  Step 2: Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:51 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_complex_data_processing (ID: 9ebc4ba6-780c-43c3-b829-50952bc6ca79) - 1 spans\n",
      "2025-07-29 23:47:51 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_complex_data_processing (ID: 9ebc4ba6-780c-43c3-b829-50952bc6ca79) - 1 spans\n",
      "2025-07-29 23:47:51 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 9ebc4ba6-780c-43c3-b829-50952bc6ca79\n",
      "2025-07-29 23:47:51 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 9ebc4ba6-780c-43c3-b829-50952bc6ca79 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Step 3: Generating output...\n",
      "‚úÖ Complex operation completed successfully\n",
      "\\n‚úÖ Context managers testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Import context managers and streaming features\n",
    "from noveum_trace import (\n",
    "    trace_llm_call, trace_agent_operation, trace_operation, \n",
    "    streaming_llm, trace_streaming, ThreadContext\n",
    ")\n",
    "from typing import Dict, Any, Iterator\n",
    "\n",
    "# Context Manager Examples - Inline Tracing\n",
    "\n",
    "def process_user_query_with_context_managers(user_input: str) -> str:\n",
    "    \"\"\"Demonstrate inline tracing with context managers.\"\"\"\n",
    "    print(f\"üîÑ Processing user query: '{user_input[:40]}...'\")\n",
    "    \n",
    "    # Some preprocessing (not traced)\n",
    "    cleaned_input = user_input.strip().lower()\n",
    "    \n",
    "    # Trace just the LLM call using context manager\n",
    "    with trace_llm_call(model=\"gpt-4\", provider=\"openai\", operation=\"query_processing\") as span:\n",
    "        print(\"ü§ñ Making LLM call within context manager...\")\n",
    "        time.sleep(0.4)\n",
    "        \n",
    "        # Mock LLM response\n",
    "        response = f\"Processed response for: {cleaned_input[:30]}...\"\n",
    "        \n",
    "        # Add custom attributes to the span\n",
    "        span.set_attributes({\n",
    "            \"llm.input_length\": len(cleaned_input),\n",
    "            \"llm.output_length\": len(response),\n",
    "            \"llm.processing_type\": \"query_understanding\"\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ LLM response generated: {len(response)} characters\")\n",
    "    \n",
    "    # Post-processing (not traced)\n",
    "    final_response = f\"Final: {response}\"\n",
    "    print(f\"üì§ Final response: {final_response[:50]}...\")\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "# Agent operation context manager\n",
    "def agent_task_with_context_manager(task: str) -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate agent operation tracing with context manager.\"\"\"\n",
    "    print(f\"ü§ñ Agent Task: '{task}'\")\n",
    "    \n",
    "    with trace_agent_operation(\n",
    "        agent_type=\"task_agent\", \n",
    "        operation=\"task_execution\",\n",
    "        capabilities=[\"task_planning\", \"execution\", \"monitoring\"]\n",
    "    ) as span:\n",
    "        print(\"‚öôÔ∏è  Executing agent task...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Mock agent work\n",
    "        result = {\n",
    "            \"task\": task,\n",
    "            \"status\": \"completed\",\n",
    "            \"steps_executed\": 5,\n",
    "            \"success_rate\": 0.95\n",
    "        }\n",
    "        \n",
    "        # Add agent-specific attributes\n",
    "        span.set_attributes({\n",
    "            \"agent.task_complexity\": \"medium\",\n",
    "            \"agent.steps_executed\": result[\"steps_executed\"],\n",
    "            \"agent.success_rate\": result[\"success_rate\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Agent task completed with {result['success_rate']:.1%} success rate\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Generic operation context manager (FIXED SYNTAX)\n",
    "def complex_operation_with_tracing() -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate generic operation tracing.\"\"\"\n",
    "    print(\"üîß Starting complex operation...\")\n",
    "    \n",
    "    # CORRECT SYNTAX: trace_operation(operation_name, attributes=...)\n",
    "    with trace_operation(\"complex_data_processing\", \n",
    "                        attributes={\"operation_type\": \"data_pipeline\", \"complexity\": \"high\"}) as span:\n",
    "        # Step 1: Data loading\n",
    "        print(\"üì• Step 1: Loading data...\")\n",
    "        time.sleep(0.2)\n",
    "        span.set_attributes({\"step\": \"data_loading\", \"records_loaded\": 1000})\n",
    "        \n",
    "        # Step 2: Processing\n",
    "        print(\"‚öôÔ∏è  Step 2: Processing data...\")\n",
    "        time.sleep(0.3)\n",
    "        span.set_attributes({\"step\": \"processing\", \"records_processed\": 950})\n",
    "        \n",
    "        # Step 3: Output\n",
    "        print(\"üì§ Step 3: Generating output...\")\n",
    "        time.sleep(0.1)\n",
    "        span.set_attributes({\"step\": \"output\", \"records_output\": 950})\n",
    "        \n",
    "        result = {\n",
    "            \"operation\": \"complex_data_processing\",\n",
    "            \"input_records\": 1000,\n",
    "            \"processed_records\": 950,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Complex operation completed successfully\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test context managers\n",
    "print(\"üîÑ Testing Context Managers...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Context Manager Examples:\")\n",
    "query_result = process_user_query_with_context_managers(\"What are the benefits of AI observability?\")\n",
    "\n",
    "agent_result = agent_task_with_context_manager(\"Analyze system performance metrics\")\n",
    "\n",
    "operation_result = complex_operation_with_tracing()\n",
    "\n",
    "print(\"\\\\n‚úÖ Context managers testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7.2: Auto-Instrumentation and Advanced Features\n",
    "\n",
    "Test auto-instrumentation, proxy objects, and advanced SDK features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing Auto-Instrumentation and Advanced Features...\n",
      "\\n1Ô∏è‚É£ Auto-Instrumentation:\n",
      "üîß Testing Auto-Instrumentation...\n",
      "üì¶ Available instrumentations: ['openai', 'anthropic', 'langchain']\n",
      "üîå Enabling OpenAI auto-instrumentation...\n",
      "‚úÖ OpenAI auto-instrumentation enabled\n",
      "üîç Currently instrumented: ['openai']\n",
      "\\n2Ô∏è‚É£ Proxy Objects:\n",
      "üîÑ Testing Traced OpenAI Client...\n",
      "‚ÑπÔ∏è  Traced client demo: create_traced_openai_client() got an unexpected keyword argument 'api_key'\n",
      "ü§ñ Testing Traced Agent Proxy...\n",
      "‚úÖ Traced agent proxy created\n",
      "üß† Testing traced agent methods...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:51 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_agent.think (ID: dd785591-03d2-4b68-9ce6-71bc9042e88a) - 1 spans\n",
      "2025-07-29 23:47:51 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_agent.think (ID: dd785591-03d2-4b68-9ce6-71bc9042e88a) - 1 spans\n",
      "2025-07-29 23:47:51 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace dd785591-03d2-4b68-9ce6-71bc9042e88a\n",
      "2025-07-29 23:47:51 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace dd785591-03d2-4b68-9ce6-71bc9042e88a successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí≠ Think result: Thinking about: How to improve AI observability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:51 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_agent.act (ID: aae400b4-b852-416a-aa7d-f782f7f6b2f6) - 1 spans\n",
      "2025-07-29 23:47:51 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_agent.act (ID: aae400b4-b852-416a-aa7d-f782f7f6b2f6) - 1 spans\n",
      "2025-07-29 23:47:51 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace aae400b4-b852-416a-aa7d-f782f7f6b2f6\n",
      "2025-07-29 23:47:51 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace aae400b4-b852-416a-aa7d-f782f7f6b2f6 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Action result: Performing action: Implement monitoring dashboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_agent.plan (ID: b6e5ea7e-d127-4abe-9050-008d55e992e9) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_agent.plan (ID: b6e5ea7e-d127-4abe-9050-008d55e992e9) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace b6e5ea7e-d127-4abe-9050-008d55e992e9\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace b6e5ea7e-d127-4abe-9050-008d55e992e9 successfully queued for export\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_operation_with_error (ID: 580b449f-d712-4ca0-b13b-5ba504c09301) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_operation_with_error (ID: 580b449f-d712-4ca0-b13b-5ba504c09301) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 580b449f-d712-4ca0-b13b-5ba504c09301\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 580b449f-d712-4ca0-b13b-5ba504c09301 successfully queued for export\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_operation_with_error (ID: da1c9e4f-72f3-45fc-b750-b31d46dddba1) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_operation_with_error (ID: da1c9e4f-72f3-45fc-b750-b31d46dddba1) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace da1c9e4f-72f3-45fc-b750-b31d46dddba1\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace da1c9e4f-72f3-45fc-b750-b31d46dddba1 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Plan result: 3 steps\n",
      "\\n3Ô∏è‚É£ Manual Tracing:\n",
      "üîç Testing Manual Tracing...\n",
      "‚úÖ Started trace: 5e94b733-53f8-4e15-877d-c93e89e2e398\n",
      "‚ÑπÔ∏è  Manual tracing demo: 'Trace' object has no attribute 'span'\n",
      "\\n4Ô∏è‚É£ Error Handling:\n",
      "‚ö†Ô∏è  Testing Error Handling...\n",
      "‚úÖ Successful operation: Success!\n",
      "‚úÖ Error captured successfully: This is a demo error for testing\n",
      "\\n‚úÖ Advanced features testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Import advanced features\n",
    "from noveum_trace import (\n",
    "    auto_instrument, get_instrumented_libraries, is_instrumented,\n",
    "    create_traced_openai_client, create_traced_agent, TracedOpenAIClient,\n",
    "    start_trace, start_span, get_current_trace, get_current_span\n",
    ")\n",
    "\n",
    "# Auto-Instrumentation Examples\n",
    "\n",
    "def test_auto_instrumentation():\n",
    "    \"\"\"Test automatic instrumentation of libraries.\"\"\"\n",
    "    print(\"üîß Testing Auto-Instrumentation...\")\n",
    "    \n",
    "    # Check available instrumentations\n",
    "    try:\n",
    "        available = noveum_trace.get_available_instrumentations()\n",
    "        print(f\"üì¶ Available instrumentations: {available}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Available instrumentations: {e}\")\n",
    "    \n",
    "    # Enable auto-instrumentation for OpenAI (if not already enabled)\n",
    "    try:\n",
    "        if not is_instrumented(\"openai\"):\n",
    "            print(\"üîå Enabling OpenAI auto-instrumentation...\")\n",
    "            auto_instrument(\"openai\")\n",
    "            print(\"‚úÖ OpenAI auto-instrumentation enabled\")\n",
    "        else:\n",
    "            print(\"‚úÖ OpenAI already instrumented\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Auto-instrumentation: {e}\")\n",
    "    \n",
    "    # Check instrumented libraries\n",
    "    try:\n",
    "        instrumented = get_instrumented_libraries()\n",
    "        print(f\"üîç Currently instrumented: {instrumented}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Instrumented libraries: {e}\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Proxy Objects for Enhanced Control\n",
    "\n",
    "def test_traced_openai_client():\n",
    "    \"\"\"Test traced OpenAI client proxy.\"\"\"\n",
    "    print(\"üîÑ Testing Traced OpenAI Client...\")\n",
    "    \n",
    "    # Create traced OpenAI client (even without real API key)\n",
    "    try:\n",
    "        traced_client = create_traced_openai_client(\n",
    "            api_key=\"mock-key-for-demo\",\n",
    "            trace_completions=True,\n",
    "            trace_embeddings=True,\n",
    "            capture_content=True\n",
    "        )\n",
    "        print(\"‚úÖ Traced OpenAI client created\")\n",
    "        \n",
    "        # Mock a call (won't actually work without real API key)\n",
    "        print(\"ü§ñ Simulating traced OpenAI call...\")\n",
    "        # In real usage: response = traced_client.chat.completions.create(...)\n",
    "        print(\"‚ÑπÔ∏è  Would automatically trace all OpenAI API calls\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Traced client demo: {e}\")\n",
    "\n",
    "def test_traced_agent_proxy():\n",
    "    \"\"\"Test traced agent proxy for enhanced agent monitoring.\"\"\"\n",
    "    print(\"ü§ñ Testing Traced Agent Proxy...\")\n",
    "    \n",
    "    # Mock agent class\n",
    "    class MockAgent:\n",
    "        def __init__(self, name: str):\n",
    "            self.name = name\n",
    "        \n",
    "        def think(self, problem: str) -> str:\n",
    "            time.sleep(0.2)\n",
    "            return f\"Thinking about: {problem}\"\n",
    "        \n",
    "        def act(self, action: str) -> str:\n",
    "            time.sleep(0.3)\n",
    "            return f\"Performing action: {action}\"\n",
    "        \n",
    "        def plan(self, goal: str) -> List[str]:\n",
    "            time.sleep(0.4)\n",
    "            return [f\"Step 1 for {goal}\", f\"Step 2 for {goal}\", f\"Step 3 for {goal}\"]\n",
    "    \n",
    "    # Create traced agent proxy with CORRECT PARAMETERS\n",
    "    original_agent = MockAgent(\"demo_agent\")\n",
    "    traced_agent = create_traced_agent(\n",
    "        agent=original_agent,\n",
    "        agent_type=\"traced_demo_agent\",  # CORRECT: agent_type instead of agent_id\n",
    "        capabilities=[\"thinking\", \"acting\", \"planning\"],\n",
    "        trace_config={\"capture_inputs\": True, \"capture_outputs\": True}\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Traced agent proxy created\")\n",
    "    \n",
    "    # Test traced methods\n",
    "    print(\"üß† Testing traced agent methods...\")\n",
    "    \n",
    "    thought = traced_agent.think(\"How to improve AI observability\")\n",
    "    print(f\"üí≠ Think result: {thought}\")\n",
    "    \n",
    "    action = traced_agent.act(\"Implement monitoring dashboard\")\n",
    "    print(f\"‚ö° Action result: {action}\")\n",
    "    \n",
    "    plan = traced_agent.plan(\"Enhance system reliability\")\n",
    "    print(f\"üìã Plan result: {len(plan)} steps\")\n",
    "\n",
    "# Manual Span Creation and Management\n",
    "\n",
    "def test_manual_tracing():\n",
    "    \"\"\"Test manual trace and span creation.\"\"\"\n",
    "    print(\"üîç Testing Manual Tracing...\")\n",
    "    \n",
    "    try:\n",
    "        # Start a manual trace\n",
    "        trace = start_trace(\"manual_demo_trace\")\n",
    "        print(f\"‚úÖ Started trace: {trace.trace_id}\")\n",
    "        \n",
    "        # Create nested spans manually\n",
    "        with trace.span(\"parent_operation\") as parent_span:\n",
    "            parent_span.set_attributes({\n",
    "                \"operation.type\": \"parent\",\n",
    "                \"operation.importance\": \"high\"\n",
    "            })\n",
    "            print(\"üìä Parent span created\")\n",
    "            \n",
    "            # Child span 1\n",
    "            with parent_span.create_child_span(\"child_operation_1\") as child1:\n",
    "                child1.set_attributes({\n",
    "                    \"operation.type\": \"child\",\n",
    "                    \"child.number\": 1\n",
    "                })\n",
    "                time.sleep(0.2)\n",
    "                print(\"üîπ Child span 1 completed\")\n",
    "            \n",
    "            # Child span 2  \n",
    "            with parent_span.create_child_span(\"child_operation_2\") as child2:\n",
    "                child2.set_attributes({\n",
    "                    \"operation.type\": \"child\",\n",
    "                    \"child.number\": 2,\n",
    "                    \"child.data_processed\": 500\n",
    "                })\n",
    "                time.sleep(0.3)\n",
    "                print(\"üîπ Child span 2 completed\")\n",
    "            \n",
    "            print(\"üìä Parent operation completed\")\n",
    "        \n",
    "        # Finish trace\n",
    "        trace.finish()\n",
    "        print(f\"‚úÖ Manual trace completed: {trace.trace_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Manual tracing demo: {e}\")\n",
    "\n",
    "# Error Handling and Edge Cases\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test error handling and edge cases.\"\"\"\n",
    "    print(\"‚ö†Ô∏è  Testing Error Handling...\")\n",
    "    \n",
    "    # Test error capture in traced function\n",
    "    @noveum_trace.trace(capture_errors=True, capture_stack_trace=True)\n",
    "    def operation_with_error(should_fail: bool = False):\n",
    "        if should_fail:\n",
    "            raise ValueError(\"This is a demo error for testing\")\n",
    "        return \"Success!\"\n",
    "    \n",
    "    # Test successful operation\n",
    "    try:\n",
    "        result = operation_with_error(should_fail=False)\n",
    "        print(f\"‚úÖ Successful operation: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "    \n",
    "    # Test error capture\n",
    "    try:\n",
    "        result = operation_with_error(should_fail=True)\n",
    "        print(f\"Unexpected success: {result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"‚úÖ Error captured successfully: {e}\")\n",
    "\n",
    "# Run all advanced feature tests\n",
    "print(\"üöÄ Testing Auto-Instrumentation and Advanced Features...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Auto-Instrumentation:\")\n",
    "instrumented_libs = test_auto_instrumentation()\n",
    "\n",
    "print(\"\\\\n2Ô∏è‚É£ Proxy Objects:\")\n",
    "test_traced_openai_client()\n",
    "test_traced_agent_proxy()\n",
    "\n",
    "print(\"\\\\n3Ô∏è‚É£ Manual Tracing:\")\n",
    "test_manual_tracing()\n",
    "\n",
    "print(\"\\\\n4Ô∏è‚É£ Error Handling:\")\n",
    "test_error_handling()\n",
    "\n",
    "print(\"\\\\n‚úÖ Advanced features testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5.1: Enhanced LLM Tracing Examples\n",
    "\n",
    "Demonstrate various LLM tracing features including different providers, metadata, and advanced parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing Enhanced LLM Tracing...\n",
      "üß† Calling claude-3-haiku with prompt: What are the key benefits of AI observability?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_call_anthropic (ID: 63327e7f-a9da-46d2-ba6e-12bb6e48369a) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_call_anthropic (ID: 63327e7f-a9da-46d2-ba6e-12bb6e48369a) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 63327e7f-a9da-46d2-ba6e-12bb6e48369a\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 63327e7f-a9da-46d2-ba6e-12bb6e48369a successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Anthropic Response: Tracing AI workflows enables debugging, optimization, and compliance monitoring.\n",
      "üìã Enhanced LLM call with metadata: Summarize the importance of AI monitorin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_call_llm_with_metadata (ID: 10a48c6d-5b12-4644-a5c4-41ccf78565f5) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_call_llm_with_metadata (ID: 10a48c6d-5b12-4644-a5c4-41ccf78565f5) - 1 spans\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 10a48c6d-5b12-4644-a5c4-41ccf78565f5\n",
      "2025-07-29 23:47:52 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 10a48c6d-5b12-4644-a5c4-41ccf78565f5 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Enhanced Response: Enhanced response for: Summarize the import...\n",
      "üü¢ Calling gemini-pro with PII protection: How does tracing help with AI compliance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_call_google_ai (ID: 8f807a15-ecfc-4406-a486-95462bf09f80) - 1 spans\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_call_google_ai (ID: 8f807a15-ecfc-4406-a486-95462bf09f80) - 1 spans\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 8f807a15-ecfc-4406-a486-95462bf09f80\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 8f807a15-ecfc-4406-a486-95462bf09f80 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Google AI Response: Google AI response with PII redaction enabled for sensitive data handling.\n",
      "\n",
      "‚úÖ Enhanced LLM tracing completed!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced LLM Tracing Examples\n",
    "\n",
    "# Test different LLM providers with comprehensive metadata\n",
    "@trace_llm(provider=\"anthropic\", capture_tokens=True, estimate_costs=True)\n",
    "def call_anthropic(prompt: str, model: str = \"claude-3-haiku\") -> str:\n",
    "    \"\"\"Call Anthropic Claude with tracing.\"\"\"\n",
    "    print(f\"üß† Calling {model} with prompt: {prompt[:50]}...\")\n",
    "    \n",
    "    # Mock Anthropic response\n",
    "    time.sleep(0.4)\n",
    "    responses = [\n",
    "        \"Observability in AI systems provides critical insights into model behavior and performance.\",\n",
    "        \"Tracing AI workflows enables debugging, optimization, and compliance monitoring.\",\n",
    "        \"Comprehensive monitoring helps ensure AI system reliability and user trust.\"\n",
    "    ]\n",
    "    return random.choice(responses)\n",
    "\n",
    "# Test with custom metadata and tags\n",
    "@trace_llm(\n",
    "    provider=\"openai\", \n",
    "    capture_prompts=True, \n",
    "    capture_completions=True,\n",
    "    metadata={\"experiment\": \"demo\", \"version\": \"1.0\"},\n",
    "    tags={\"environment\": \"notebook\", \"user\": \"demo\"}\n",
    ")\n",
    "def call_llm_with_metadata(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"LLM call with custom metadata and tags.\"\"\"\n",
    "    print(f\"üìã Enhanced LLM call with metadata: {prompt[:40]}...\")\n",
    "    time.sleep(0.3)\n",
    "    return f\"Enhanced response for: {prompt[:20]}...\"\n",
    "\n",
    "# Test Google AI provider\n",
    "@trace_llm(provider=\"google\", capture_tokens=True, redact_pii=True)\n",
    "def call_google_ai(prompt: str, model: str = \"gemini-pro\") -> str:\n",
    "    \"\"\"Call Google AI with PII redaction.\"\"\"\n",
    "    print(f\"üü¢ Calling {model} with PII protection: {prompt[:40]}...\")\n",
    "    time.sleep(0.5)\n",
    "    return \"Google AI response with PII redaction enabled for sensitive data handling.\"\n",
    "\n",
    "# Test various LLM providers\n",
    "print(\"ü§ñ Testing Enhanced LLM Tracing...\")\n",
    "\n",
    "anthropic_response = call_anthropic(\"What are the key benefits of AI observability?\")\n",
    "print(f\"\\nüß† Anthropic Response: {anthropic_response}\")\n",
    "\n",
    "metadata_response = call_llm_with_metadata(\"Summarize the importance of AI monitoring\")\n",
    "print(f\"\\nüìã Enhanced Response: {metadata_response}\")\n",
    "\n",
    "google_response = call_google_ai(\"How does tracing help with AI compliance?\")\n",
    "print(f\"\\nüü¢ Google AI Response: {google_response}\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced LLM tracing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5.2: Retrieval System Tracing\n",
    "\n",
    "Test retrieval operations with the `@trace_retrieval` decorator for RAG systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: manual_demo_trace (ID: 5e94b733-53f8-4e15-877d-c93e89e2e398) - 0 spans\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: manual_demo_trace (ID: 5e94b733-53f8-4e15-877d-c93e89e2e398) - 0 spans\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 5e94b733-53f8-4e15-877d-c93e89e2e398\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 5e94b733-53f8-4e15-877d-c93e89e2e398 successfully queued for export\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - üîÑ FLUSH: Sending current batch of 2 traces\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 2 traces via send_callback\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 2 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 2 traces\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 2 traces via callback\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Batch processor flush completed in 0.00s\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - HTTP transport flush completed\n",
      "2025-07-29 23:47:53 - noveum_trace.core.client - INFO - Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Corrected trace_operation Examples (Fixed Syntax) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER CORRECTED TRACE_OPERATION EXAMPLES\n",
    "# This ensures all corrected context manager traces are sent immediately\n",
    "\n",
    "flush_traces(\"Corrected trace_operation Examples (Fixed Syntax)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Retrieval System Tracing...\n",
      "üîç Vector Search: Finding top 3 results for 'benefits of AI observability'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_vector_search (ID: 357b161a-9c37-423e-825f-b80c08d4ec13) - 1 spans\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_vector_search (ID: 357b161a-9c37-423e-825f-b80c08d4ec13) - 1 spans\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 357b161a-9c37-423e-825f-b80c08d4ec13\n",
      "2025-07-29 23:47:53 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 357b161a-9c37-423e-825f-b80c08d4ec13 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      "üîç Vector Search Results: 3 documents\n",
      "üîé Keyword Search: 'AI monitoring' with filters: {'category': 'technical', 'year': 2024}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:54 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_keyword_search (ID: 933d8c11-a727-4b3f-be80-6f9ed9e76ec9) - 1 spans\n",
      "2025-07-29 23:47:54 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_keyword_search (ID: 933d8c11-a727-4b3f-be80-6f9ed9e76ec9) - 1 spans\n",
      "2025-07-29 23:47:54 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 933d8c11-a727-4b3f-be80-6f9ed9e76ec9\n",
      "2025-07-29 23:47:54 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 933d8c11-a727-4b3f-be80-6f9ed9e76ec9 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Keyword Search Results: 3 matches\n",
      "üîó Hybrid Search: 'observability tracing systems' with alpha=0.7\n",
      "üîç Vector Search: Finding top 3 results for 'observability tracing systems'\n",
      "‚úÖ Found 3 relevant documents\n",
      "üîé Keyword Search: 'observability tracing systems' with filters: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:55 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_hybrid_search (ID: c9601541-fbf6-4a1a-8688-50a55e95feea) - 3 spans\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_hybrid_search (ID: c9601541-fbf6-4a1a-8688-50a55e95feea) - 3 spans\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace c9601541-fbf6-4a1a-8688-50a55e95feea\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace c9601541-fbf6-4a1a-8688-50a55e95feea successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Hybrid Search Results: 2 combined results\n",
      "\n",
      "‚úÖ Retrieval tracing completed!\n"
     ]
    }
   ],
   "source": [
    "# Import the retrieval decorator and missing typing imports\n",
    "from noveum_trace import trace_retrieval\n",
    "from typing import Dict, Any, Optional, List, Iterator\n",
    "\n",
    "# Vector search with comprehensive tracing\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"vector_search\",\n",
    "    index_name=\"knowledge_base\",\n",
    "    capture_query=True,\n",
    "    capture_results=True,\n",
    "    capture_scores=True,\n",
    "    metadata={\"index_version\": \"v2.1\", \"embedding_model\": \"text-embedding-ada-002\"}\n",
    ")\n",
    "def vector_search(query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Perform vector search with tracing.\"\"\"\n",
    "    print(f\"üîç Vector Search: Finding top {top_k} results for '{query}'\")\n",
    "    \n",
    "    # Simulate vector search\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    # Mock search results with scores\n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        results.append({\n",
    "            \"document_id\": f\"doc_{i+1}\",\n",
    "            \"content\": f\"Relevant content for '{query}' - document {i+1}\",\n",
    "            \"score\": 0.95 - (i * 0.1),\n",
    "            \"metadata\": {\"source\": f\"source_{i+1}.pdf\", \"page\": i+1}\n",
    "        })\n",
    "    \n",
    "    search_result = {\n",
    "        \"query\": query,\n",
    "        \"total_results\": top_k,\n",
    "        \"results\": results,\n",
    "        \"search_time_ms\": 300,\n",
    "        \"index_stats\": {\"total_docs\": 10000, \"dimensions\": 1536}\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(results)} relevant documents\")\n",
    "    return search_result\n",
    "\n",
    "# Keyword search with metadata capture\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"keyword_search\",\n",
    "    index_name=\"text_corpus\",\n",
    "    capture_metadata=True,\n",
    "    tags={\"search_type\": \"fulltext\", \"language\": \"en\"}\n",
    ")\n",
    "def keyword_search(query: str, filters: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Perform keyword search with filtering.\"\"\"\n",
    "    print(f\"üîé Keyword Search: '{query}' with filters: {filters}\")\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    # Mock keyword search results\n",
    "    results = [\n",
    "        {\"doc_id\": \"kw_1\", \"title\": \"AI Observability Guide\", \"snippet\": \"...observability in AI...\"},\n",
    "        {\"doc_id\": \"kw_2\", \"title\": \"Tracing Best Practices\", \"snippet\": \"...tracing methodologies...\"},\n",
    "        {\"doc_id\": \"kw_3\", \"title\": \"Monitoring AI Systems\", \"snippet\": \"...monitoring strategies...\"}\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"filters\": filters or {},\n",
    "        \"results\": results,\n",
    "        \"total_matches\": len(results)\n",
    "    }\n",
    "\n",
    "# Hybrid search combining vector and keyword\n",
    "@trace_retrieval(\n",
    "    retrieval_type=\"hybrid_search\",\n",
    "    index_name=\"hybrid_index\",\n",
    "    capture_query=True,\n",
    "    capture_results=True,\n",
    "    capture_scores=True\n",
    ")\n",
    "def hybrid_search(query: str, alpha: float = 0.5) -> Dict[str, Any]:\n",
    "    \"\"\"Perform hybrid search combining vector and keyword search.\"\"\"\n",
    "    print(f\"üîó Hybrid Search: '{query}' with alpha={alpha}\")\n",
    "    \n",
    "    time.sleep(0.4)\n",
    "    \n",
    "    # Simulate hybrid search by combining both approaches\n",
    "    vector_results = vector_search(query, top_k=3)\n",
    "    keyword_results = keyword_search(query)\n",
    "    \n",
    "    # Mock hybrid ranking\n",
    "    hybrid_results = []\n",
    "    for i, result in enumerate(vector_results[\"results\"][:2]):\n",
    "        hybrid_results.append({\n",
    "            \"document_id\": result[\"document_id\"],\n",
    "            \"content\": result[\"content\"],\n",
    "            \"vector_score\": result[\"score\"],\n",
    "            \"keyword_score\": 0.8 - (i * 0.1),\n",
    "            \"combined_score\": (result[\"score\"] * alpha) + ((0.8 - i * 0.1) * (1 - alpha)),\n",
    "            \"source\": \"hybrid\"\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"alpha\": alpha,\n",
    "        \"results\": hybrid_results,\n",
    "        \"total_results\": len(hybrid_results),\n",
    "        \"search_strategy\": \"vector + keyword fusion\"\n",
    "    }\n",
    "\n",
    "# Test all retrieval operations\n",
    "print(\"üîç Testing Retrieval System Tracing...\")\n",
    "\n",
    "# Test vector search\n",
    "vector_result = vector_search(\"benefits of AI observability\", top_k=3)\n",
    "print(f\"\\nüîç Vector Search Results: {len(vector_result['results'])} documents\")\n",
    "\n",
    "# Test keyword search with filters\n",
    "keyword_result = keyword_search(\"AI monitoring\", filters={\"category\": \"technical\", \"year\": 2024})\n",
    "print(f\"\\nüîé Keyword Search Results: {keyword_result['total_matches']} matches\")\n",
    "\n",
    "# Test hybrid search\n",
    "hybrid_result = hybrid_search(\"observability tracing systems\", alpha=0.7)\n",
    "print(f\"\\nüîó Hybrid Search Results: {len(hybrid_result['results'])} combined results\")\n",
    "\n",
    "print(\"\\n‚úÖ Retrieval tracing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6.1: Enhanced Multi-Agent System\n",
    "\n",
    "Test advanced multi-agent workflows with specialized agents and complex coordination patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:55 - noveum_trace.transport.batch_processor - INFO - üîÑ FLUSH: Sending current batch of 3 traces\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 3 traces via send_callback\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 3 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 3 traces\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 3 traces via callback\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Batch processor flush completed in 0.00s\n",
      "2025-07-29 23:47:55 - noveum_trace.transport.http_transport - INFO - HTTP transport flush completed\n",
      "2025-07-29 23:47:55 - noveum_trace.core.client - INFO - Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Corrected Proxy Objects (create_traced_agent + create_traced_openai_client) traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER CORRECTED PROXY OBJECT EXAMPLES\n",
    "# This ensures all corrected proxy object traces are sent immediately\n",
    "\n",
    "flush_traces(\"Corrected Proxy Objects (create_traced_agent + create_traced_openai_client)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing Enhanced Multi-Agent System...\n",
      "üé≠ Advanced Orchestrator: Managing workflow for 'Comprehensive analysis of AI system observability data and content'\n",
      "\n",
      "üîÑ Phase 1: Parallel Agent Execution\n",
      "üìä Data Analyst: Analyzing dataset with 100 samples\n",
      "‚úÖ Analysis complete: 0.92 quality score\n",
      "üìù Content Curator: Processing 4 content items\n",
      "‚úÖ Curated 3/4 items\n",
      "\\nüîó Phase 2: Synthesis and Integration\n",
      "üîó Synthesis Agent: Combining insights for context 'Comprehensive analysis of AI system observability data and content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:56 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_advanced_orchestrator (ID: 6c525f71-c7af-4fe6-9c60-73968dcdf991) - 4 spans\n",
      "2025-07-29 23:47:56 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_advanced_orchestrator (ID: 6c525f71-c7af-4fe6-9c60-73968dcdf991) - 4 spans\n",
      "2025-07-29 23:47:56 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 6c525f71-c7af-4fe6-9c60-73968dcdf991\n",
      "2025-07-29 23:47:56 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 6c525f71-c7af-4fe6-9c60-73968dcdf991 successfully queued for export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Synthesis complete with 0.86 confidence\n",
      "\\n‚úÖ Advanced orchestration complete!\n",
      "\\nüé≠ Enhanced Multi-Agent Results:\n",
      "Task: Comprehensive analysis of AI system observability data and content\n",
      "Agents: 3\n",
      "Phases: 2\n",
      "Final Confidence: 0.86\n",
      "Success: True\n",
      "\\n‚úÖ Enhanced multi-agent system testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Multi-Agent System Examples\n",
    "# Import missing typing if not already available\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Specialized agents with different roles and capabilities\n",
    "@trace_agent(\n",
    "    agent_id=\"data_analyst\",\n",
    "    role=\"analyst\",\n",
    "    agent_type=\"specialist\",\n",
    "    capabilities=[\"data_analysis\", \"statistical_modeling\", \"visualization\"],\n",
    "    capture_reasoning=True,\n",
    "    metadata={\"specialization\": \"quantitative_analysis\", \"confidence_threshold\": 0.8}\n",
    ")\n",
    "def data_analyst_agent(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Specialized data analysis agent.\"\"\"\n",
    "    print(f\"üìä Data Analyst: Analyzing dataset with {len(data.get('samples', []))} samples\")\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Mock data analysis\n",
    "    analysis = {\n",
    "        \"agent_id\": \"data_analyst\",\n",
    "        \"analysis_type\": \"quantitative\",\n",
    "        \"findings\": {\n",
    "            \"data_quality\": 0.92,\n",
    "            \"pattern_confidence\": 0.87,\n",
    "            \"anomalies_detected\": 3,\n",
    "            \"recommendations\": [\n",
    "                \"Data quality is high with 92% confidence\",\n",
    "                \"3 anomalies detected requiring investigation\",\n",
    "                \"Statistical patterns show strong correlation\"\n",
    "            ]\n",
    "        },\n",
    "        \"reasoning_steps\": [\n",
    "            \"Loaded and validated input data\",\n",
    "            \"Applied statistical analysis methods\",\n",
    "            \"Identified patterns and anomalies\",\n",
    "            \"Generated confidence-based recommendations\"\n",
    "        ],\n",
    "        \"processing_time\": 0.5\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Analysis complete: {analysis['findings']['data_quality']:.2f} quality score\")\n",
    "    return analysis\n",
    "\n",
    "@trace_agent(\n",
    "    agent_id=\"content_curator\",\n",
    "    role=\"curator\",\n",
    "    agent_type=\"content_specialist\",\n",
    "    capabilities=[\"content_filtering\", \"quality_assessment\", \"summarization\"],\n",
    "    capture_tools=True\n",
    ")\n",
    "def content_curator_agent(content_list: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Content curation and quality assessment agent.\"\"\"\n",
    "    print(f\"üìù Content Curator: Processing {len(content_list)} content items\")\n",
    "    \n",
    "    time.sleep(0.4)\n",
    "    \n",
    "    # Mock content curation using tools\n",
    "    high_quality_content = []\n",
    "    for i, content in enumerate(content_list):\n",
    "        if i < 3:  # Mock: keep first 3 as high quality\n",
    "            high_quality_content.append({\n",
    "                **content,\n",
    "                \"quality_score\": 0.9 - (i * 0.05),\n",
    "                \"curation_reason\": \"Meets quality standards\"\n",
    "            })\n",
    "    \n",
    "    curation_result = {\n",
    "        \"agent_id\": \"content_curator\",\n",
    "        \"input_count\": len(content_list),\n",
    "        \"curated_count\": len(high_quality_content),\n",
    "        \"curated_content\": high_quality_content,\n",
    "        \"tools_used\": [\"quality_scorer\", \"content_filter\", \"summarizer\"],\n",
    "        \"curation_metrics\": {\n",
    "            \"retention_rate\": len(high_quality_content) / len(content_list),\n",
    "            \"average_quality\": sum(item[\"quality_score\"] for item in high_quality_content) / len(high_quality_content)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Curated {len(high_quality_content)}/{len(content_list)} items\")\n",
    "    return curation_result\n",
    "\n",
    "@trace_agent(\n",
    "    agent_id=\"synthesis_agent\",\n",
    "    role=\"synthesizer\",\n",
    "    agent_type=\"integration_specialist\", \n",
    "    capabilities=[\"multi_source_synthesis\", \"insight_generation\", \"report_creation\"],\n",
    "    capture_inputs=True,\n",
    "    capture_outputs=True\n",
    ")\n",
    "def synthesis_agent(analyst_data: Dict, curator_data: Dict, context: str) -> Dict[str, Any]:\n",
    "    \"\"\"Agent that synthesizes insights from multiple sources.\"\"\"\n",
    "    print(f\"üîó Synthesis Agent: Combining insights for context '{context}'\")\n",
    "    \n",
    "    time.sleep(0.6)\n",
    "    \n",
    "    # Synthesize insights from multiple agents\n",
    "    synthesis = {\n",
    "        \"agent_id\": \"synthesis_agent\",\n",
    "        \"context\": context,\n",
    "        \"input_sources\": [\"data_analyst\", \"content_curator\"],\n",
    "        \"synthesis_insights\": [\n",
    "            f\"Data quality score of {analyst_data['findings']['data_quality']:.2f} indicates reliable foundation\",\n",
    "            f\"Content curation retained {curator_data['curated_count']}/{curator_data['input_count']} high-quality items\",\n",
    "            \"Cross-analysis reveals consistent quality patterns across data and content\",\n",
    "            \"Synthesis confidence: High based on convergent evidence\"\n",
    "        ],\n",
    "        \"combined_metrics\": {\n",
    "            \"data_quality\": analyst_data['findings']['data_quality'],\n",
    "            \"content_quality\": curator_data['curation_metrics']['average_quality'],\n",
    "            \"overall_confidence\": (analyst_data['findings']['pattern_confidence'] + \n",
    "                                 curator_data['curation_metrics']['average_quality']) / 2\n",
    "        },\n",
    "        \"final_recommendation\": \"Proceed with high confidence based on quality convergence\"\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Synthesis complete with {synthesis['combined_metrics']['overall_confidence']:.2f} confidence\")\n",
    "    return synthesis\n",
    "\n",
    "# Advanced orchestrator with dependency management\n",
    "@trace_agent(\n",
    "    agent_id=\"advanced_orchestrator\",\n",
    "    role=\"coordinator\", \n",
    "    agent_type=\"orchestrator\",\n",
    "    capabilities=[\"workflow_management\", \"dependency_resolution\", \"result_aggregation\"],\n",
    "    capture_reasoning=True,\n",
    "    metadata={\"orchestration_strategy\": \"parallel_with_dependencies\"}\n",
    ")\n",
    "def advanced_orchestrator(task: str, data_context: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"Advanced orchestrator managing complex multi-agent workflows.\"\"\"\n",
    "    print(f\"üé≠ Advanced Orchestrator: Managing workflow for '{task}'\")\n",
    "    \n",
    "    # Phase 1: Parallel execution of independent agents\n",
    "    print(\"\\nüîÑ Phase 1: Parallel Agent Execution\")\n",
    "    \n",
    "    # Mock data for demonstration\n",
    "    sample_data = {\"samples\": [f\"sample_{i}\" for i in range(100)]}\n",
    "    sample_content = [\n",
    "        {\"id\": 1, \"title\": \"AI Observability\", \"content\": \"Content about observability\"},\n",
    "        {\"id\": 2, \"title\": \"Tracing Systems\", \"content\": \"Content about tracing\"},\n",
    "        {\"id\": 3, \"title\": \"Monitoring Tools\", \"content\": \"Content about monitoring\"},\n",
    "        {\"id\": 4, \"title\": \"Low Quality\", \"content\": \"Poor content\"}\n",
    "    ]\n",
    "    \n",
    "    # Execute agents in parallel (simulated)\n",
    "    analyst_result = data_analyst_agent(sample_data)\n",
    "    curator_result = content_curator_agent(sample_content)\n",
    "    \n",
    "    # Phase 2: Synthesis based on results\n",
    "    print(\"\\\\nüîó Phase 2: Synthesis and Integration\")\n",
    "    synthesis_result = synthesis_agent(analyst_result, curator_result, task)\n",
    "    \n",
    "    # Final orchestration result\n",
    "    orchestration_result = {\n",
    "        \"task\": task,\n",
    "        \"orchestration_id\": \"adv_orch_001\",\n",
    "        \"phases\": {\n",
    "            \"analysis\": analyst_result,\n",
    "            \"curation\": curator_result, \n",
    "            \"synthesis\": synthesis_result\n",
    "        },\n",
    "        \"workflow_metrics\": {\n",
    "            \"total_agents\": 3,\n",
    "            \"execution_phases\": 2,\n",
    "            \"final_confidence\": synthesis_result[\"combined_metrics\"][\"overall_confidence\"],\n",
    "            \"workflow_success\": True\n",
    "        },\n",
    "        \"reasoning\": [\n",
    "            \"Initiated parallel execution of specialist agents\",\n",
    "            \"Data analyst provided quantitative insights\",\n",
    "            \"Content curator filtered and assessed quality\",\n",
    "            \"Synthesis agent combined multi-source insights\",\n",
    "            \"Workflow completed with high confidence\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Advanced orchestration complete!\")\n",
    "    return orchestration_result\n",
    "\n",
    "# Test the enhanced multi-agent system\n",
    "print(\"ü§ñ Testing Enhanced Multi-Agent System...\")\n",
    "\n",
    "# Run the advanced workflow\n",
    "task = \"Comprehensive analysis of AI system observability data and content\"\n",
    "context_data = {\"domain\": \"ai_observability\", \"priority\": \"high\"}\n",
    "\n",
    "workflow_result = advanced_orchestrator(task, context_data)\n",
    "\n",
    "print(\"\\\\nüé≠ Enhanced Multi-Agent Results:\")\n",
    "print(f\"Task: {workflow_result['task']}\")\n",
    "print(f\"Agents: {workflow_result['workflow_metrics']['total_agents']}\")\n",
    "print(f\"Phases: {workflow_result['workflow_metrics']['execution_phases']}\")\n",
    "print(f\"Final Confidence: {workflow_result['workflow_metrics']['final_confidence']:.2f}\")\n",
    "print(f\"Success: {workflow_result['workflow_metrics']['workflow_success']}\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Enhanced multi-agent system testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7.1: Context Managers and Streaming\n",
    "\n",
    "Test context managers for inline tracing and streaming LLM responses.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Comprehensive Demo Summary\n",
    "\n",
    "This notebook provides a **complete demonstration** of the Noveum Trace SDK capabilities:\n",
    "\n",
    "### üé® All Available Decorators:\n",
    "- `@trace` - General purpose function tracing\n",
    "- `@trace_llm` - LLM call tracing with provider-specific features\n",
    "- `@trace_agent` - Agent workflow tracing with role-based capabilities\n",
    "- `@trace_tool` - Tool usage tracing with comprehensive metadata\n",
    "- `@trace_retrieval` - Retrieval operation tracing for RAG systems\n",
    "\n",
    "### üîÑ Context Managers for Inline Tracing:\n",
    "- `trace_llm_call()` - LLM operations within existing functions\n",
    "- `trace_agent_operation()` - Agent tasks with custom attributes\n",
    "- `trace_operation()` - Generic operations with step-by-step tracking\n",
    "- `streaming_llm()` - Real-time streaming LLM response tracing\n",
    "- `ThreadContext()` - Conversation thread management\n",
    "\n",
    "### ü§ñ Multi-Agent System Features:\n",
    "- Basic orchestration patterns\n",
    "- Advanced multi-agent workflows with specialized roles\n",
    "- Dependency management between agents\n",
    "- Parallel execution and result synthesis\n",
    "- Agent capability tracking and reasoning capture\n",
    "\n",
    "### üåä Streaming & Real-time Features:\n",
    "- Token-by-token streaming trace capture\n",
    "- Real-time metrics (tokens/second, time to first token)\n",
    "- Stream metadata and performance analysis\n",
    "- Context-aware streaming within conversations\n",
    "\n",
    "### üöÄ Advanced SDK Features:\n",
    "- Auto-instrumentation for seamless integration\n",
    "- Proxy objects for enhanced control and monitoring\n",
    "- Manual trace/span creation for custom workflows\n",
    "- Batch processing and performance optimization\n",
    "- Configuration management and debugging tools\n",
    "- Comprehensive error handling and edge case testing\n",
    "\n",
    "### üìä Integration Examples:\n",
    "- OpenAI API integration with cost estimation\n",
    "- Anthropic Claude integration with PII redaction\n",
    "- Google AI integration examples\n",
    "- RAG system integration (vector, keyword, hybrid search)\n",
    "- Multi-provider LLM support patterns\n",
    "\n",
    "This comprehensive demo showcases **every major feature** of the Noveum Trace SDK, making it the perfect reference for implementing observability in your AI applications! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:47:56 - noveum_trace.transport.batch_processor - INFO - üîÑ FLUSH: Sending current batch of 1 traces\n",
      "2025-07-29 23:47:56 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 1 traces via send_callback\n",
      "2025-07-29 23:47:56 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 1 traces to https://api.noveum.ai/api/v1/traces\n",
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:47:57 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:47:57 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 1 traces\n",
      "2025-07-29 23:47:57 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 1 traces via callback\n",
      "2025-07-29 23:47:57 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Batch processor flush completed in 0.00s\n",
      "2025-07-29 23:47:57 - noveum_trace.transport.http_transport - INFO - HTTP transport flush completed\n",
      "2025-07-29 23:47:57 - noveum_trace.core.client - INFO - Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ ‚úÖ Enhanced SDK Initialization and Endpoint Testing traces flushed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîÑ FLUSH AFTER ENHANCED SDK INITIALIZATION AND ENDPOINT TESTING\n",
    "# This ensures the endpoint connectivity test traces are sent immediately\n",
    "\n",
    "flush_traces(\"Enhanced SDK Initialization and Endpoint Testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Context Managers and Streaming...\n",
      "\\n1Ô∏è‚É£ Context Manager Examples:\n",
      "üîÑ Processing user query: 'What are the benefits of AI observabilit...'\n",
      "ü§ñ Making LLM call within context manager...\n",
      "‚úÖ LLM response generated: 57 characters\n",
      "üì§ Final response: Final: Processed response for: what are the benefi...\n",
      "ü§ñ Agent Task: 'Analyze system performance metrics'\n",
      "‚öôÔ∏è  Executing agent task...\n",
      "‚úÖ Agent task completed with 95.0% success rate\n",
      "üîß Starting complex operation...\n",
      "üì• Step 1: Loading data...\n",
      "‚öôÔ∏è  Step 2: Processing data...\n",
      "üì§ Step 3: Generating output...\n",
      "‚úÖ Complex operation completed successfully\n",
      "\\n2Ô∏è‚É£ Streaming Examples:\n",
      "üåä Streaming LLM call: 'Explain machine learning conce...'\n",
      "üì∫ Streaming response: This is a streaming response to: Explain machine learning concepts. Each word comes separately. \\n‚úÖ Streaming completed: 96 characters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 204\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# Test streaming\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn2Ô∏è‚É£ Streaming Examples:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m stream_result = \u001b[43mtest_streaming_with_context_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain machine learning concepts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Test thread context\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn3Ô∏è‚É£ Thread Context Examples:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mtest_streaming_with_context_manager\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m    132\u001b[39m stream = mock_streaming_response(prompt)\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Use streaming context manager\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstreaming_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstreaming_chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_manager\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43müì∫ Streaming response: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_response\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages/noveum_trace/streaming.py:494\u001b[39m, in \u001b[36mStreamingContext.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\n\u001b[32m    488\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    489\u001b[39m     exc_type: Optional[\u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m]],\n\u001b[32m    490\u001b[39m     exc_val: Optional[\u001b[38;5;167;01mBaseException\u001b[39;00m],\n\u001b[32m    491\u001b[39m     exc_tb: Optional[TracebackType],\n\u001b[32m    492\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    493\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Exit the context and finish tracing.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages/noveum_trace/streaming.py:92\u001b[39m, in \u001b[36mStreamingSpanManager.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lock:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_finished:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinish_streaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Let the span context manager handle the exception\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.span_context:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Noveum/noveum-trace/venv/lib/python3.13/site-packages/noveum_trace/streaming.py:152\u001b[39m, in \u001b[36mStreamingSpanManager.finish_streaming\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_finished:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_duration\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_time\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Import context managers and streaming features\n",
    "from noveum_trace import (\n",
    "    trace_llm_call, trace_agent_operation, trace_operation, \n",
    "    streaming_llm, trace_streaming, ThreadContext\n",
    ")\n",
    "\n",
    "# Context Manager Examples - Inline Tracing\n",
    "\n",
    "def process_user_query_with_context_managers(user_input: str) -> str:\n",
    "    \"\"\"Demonstrate inline tracing with context managers.\"\"\"\n",
    "    print(f\"üîÑ Processing user query: '{user_input[:40]}...'\")\n",
    "    \n",
    "    # Some preprocessing (not traced)\n",
    "    cleaned_input = user_input.strip().lower()\n",
    "    \n",
    "    # Trace just the LLM call using context manager\n",
    "    with trace_llm_call(model=\"gpt-4\", provider=\"openai\", operation=\"query_processing\") as span:\n",
    "        print(\"ü§ñ Making LLM call within context manager...\")\n",
    "        time.sleep(0.4)\n",
    "        \n",
    "        # Mock LLM response\n",
    "        response = f\"Processed response for: {cleaned_input[:30]}...\"\n",
    "        \n",
    "        # Add custom attributes to the span\n",
    "        span.set_attributes({\n",
    "            \"llm.input_length\": len(cleaned_input),\n",
    "            \"llm.output_length\": len(response),\n",
    "            \"llm.processing_type\": \"query_understanding\"\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ LLM response generated: {len(response)} characters\")\n",
    "    \n",
    "    # Post-processing (not traced)\n",
    "    final_response = f\"Final: {response}\"\n",
    "    print(f\"üì§ Final response: {final_response[:50]}...\")\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "# Agent operation context manager\n",
    "def agent_task_with_context_manager(task: str) -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate agent operation tracing with context manager.\"\"\"\n",
    "    print(f\"ü§ñ Agent Task: '{task}'\")\n",
    "    \n",
    "    with trace_agent_operation(\n",
    "        agent_type=\"task_agent\", \n",
    "        operation=\"task_execution\",\n",
    "        capabilities=[\"task_planning\", \"execution\", \"monitoring\"]\n",
    "    ) as span:\n",
    "        print(\"‚öôÔ∏è  Executing agent task...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Mock agent work\n",
    "        result = {\n",
    "            \"task\": task,\n",
    "            \"status\": \"completed\",\n",
    "            \"steps_executed\": 5,\n",
    "            \"success_rate\": 0.95\n",
    "        }\n",
    "        \n",
    "        # Add agent-specific attributes\n",
    "        span.set_attributes({\n",
    "            \"agent.task_complexity\": \"medium\",\n",
    "            \"agent.steps_executed\": result[\"steps_executed\"],\n",
    "            \"agent.success_rate\": result[\"success_rate\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Agent task completed with {result['success_rate']:.1%} success rate\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Generic operation context manager\n",
    "def complex_operation_with_tracing() -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate generic operation tracing.\"\"\"\n",
    "    print(\"üîß Starting complex operation...\")\n",
    "    \n",
    "    with trace_operation(operation_name=\"complex_data_processing\") as span:\n",
    "        # Step 1: Data loading\n",
    "        print(\"üì• Step 1: Loading data...\")\n",
    "        time.sleep(0.2)\n",
    "        span.set_attributes({\"step\": \"data_loading\", \"records_loaded\": 1000})\n",
    "        \n",
    "        # Step 2: Processing\n",
    "        print(\"‚öôÔ∏è  Step 2: Processing data...\")\n",
    "        time.sleep(0.3)\n",
    "        span.set_attributes({\"step\": \"processing\", \"records_processed\": 950})\n",
    "        \n",
    "        # Step 3: Output\n",
    "        print(\"üì§ Step 3: Generating output...\")\n",
    "        time.sleep(0.1)\n",
    "        span.set_attributes({\"step\": \"output\", \"records_output\": 950})\n",
    "        \n",
    "        result = {\n",
    "            \"operation\": \"complex_data_processing\",\n",
    "            \"input_records\": 1000,\n",
    "            \"processed_records\": 950,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Complex operation completed successfully\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Streaming LLM Examples\n",
    "\n",
    "class MockStreamChunk:\n",
    "    \"\"\"Mock streaming response chunk.\"\"\"\n",
    "    def __init__(self, content: str):\n",
    "        self.choices = [MockChoice(content)]\n",
    "\n",
    "class MockChoice:\n",
    "    \"\"\"Mock choice in streaming response.\"\"\"\n",
    "    def __init__(self, content: str):\n",
    "        self.delta = MockDelta(content)\n",
    "\n",
    "class MockDelta:\n",
    "    \"\"\"Mock delta content.\"\"\"\n",
    "    def __init__(self, content: str):\n",
    "        self.content = content\n",
    "\n",
    "def mock_streaming_response(prompt: str) -> Iterator[MockStreamChunk]:\n",
    "    \"\"\"Generate mock streaming response.\"\"\"\n",
    "    words = f\"This is a streaming response to: {prompt}. Each word comes separately.\".split()\n",
    "    for word in words:\n",
    "        time.sleep(0.05)  # Simulate streaming delay\n",
    "        yield MockStreamChunk(word + \" \")\n",
    "\n",
    "def test_streaming_with_context_manager(prompt: str) -> str:\n",
    "    \"\"\"Test streaming LLM with context manager.\"\"\"\n",
    "    print(f\"üåä Streaming LLM call: '{prompt[:30]}...'\")\n",
    "    \n",
    "    # Create mock stream\n",
    "    stream = mock_streaming_response(prompt)\n",
    "    \n",
    "    # Use streaming context manager\n",
    "    with streaming_llm(model=\"gpt-4\", provider=\"openai\", operation=\"streaming_chat\") as stream_manager:\n",
    "        print(\"üì∫ Streaming response: \", end=\"\")\n",
    "        full_response = \"\"\n",
    "        \n",
    "        for chunk in stream:\n",
    "            token = chunk.choices[0].delta.content\n",
    "            if token:\n",
    "                # Add token to stream manager for tracing\n",
    "                stream_manager.add_token(token)\n",
    "                print(token, end=\"\")\n",
    "                full_response += token\n",
    "        \n",
    "        # Add final metadata\n",
    "        stream_manager.add_metadata({\n",
    "            \"streaming.final_length\": len(full_response),\n",
    "            \"streaming.total_chunks\": len(full_response.split())\n",
    "        })\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Streaming completed: {len(full_response)} characters\")\n",
    "    \n",
    "    return full_response.strip()\n",
    "\n",
    "# Thread Context for Conversation Tracking\n",
    "\n",
    "def test_thread_context_conversation() -> None:\n",
    "    \"\"\"Test conversation thread tracking.\"\"\"\n",
    "    print(\"üí¨ Testing Thread Context for Conversations...\")\n",
    "    \n",
    "    with ThreadContext(name=\"demo_conversation\", metadata={\"session\": \"demo\"}) as thread:\n",
    "        # Simulate conversation turns\n",
    "        \n",
    "        # Turn 1\n",
    "        thread.add_message(\"user\", \"Hello, can you help me with AI observability?\")\n",
    "        print(\"üë§ User: Hello, can you help me with AI observability?\")\n",
    "        \n",
    "        # Simulate LLM response within thread\n",
    "        with trace_llm_call(model=\"gpt-4\") as llm_span:\n",
    "            time.sleep(0.3)\n",
    "            response1 = \"I'd be happy to help you with AI observability!\"\n",
    "            thread.add_message(\"assistant\", response1)\n",
    "            print(f\"ü§ñ Assistant: {response1}\")\n",
    "        \n",
    "        # Turn 2  \n",
    "        thread.add_message(\"user\", \"What are the key components?\")\n",
    "        print(\"üë§ User: What are the key components?\")\n",
    "        \n",
    "        with trace_llm_call(model=\"gpt-4\") as llm_span:\n",
    "            time.sleep(0.4)\n",
    "            response2 = \"Key components include tracing, metrics, and logging.\"\n",
    "            thread.add_message(\"assistant\", response2)\n",
    "            print(f\"ü§ñ Assistant: {response2}\")\n",
    "        \n",
    "        # Get thread statistics\n",
    "        stats = thread.get_statistics()\n",
    "        print(f\"\\\\nüìä Thread Stats: {stats['message_count']} messages, {stats['turn_count']} turns\")\n",
    "\n",
    "# Test all context manager and streaming features\n",
    "print(\"üîÑ Testing Context Managers and Streaming...\")\n",
    "\n",
    "# Test context managers\n",
    "print(\"\\\\n1Ô∏è‚É£ Context Manager Examples:\")\n",
    "query_result = process_user_query_with_context_managers(\"What are the benefits of AI observability?\")\n",
    "\n",
    "agent_result = agent_task_with_context_manager(\"Analyze system performance metrics\")\n",
    "\n",
    "operation_result = complex_operation_with_tracing()\n",
    "\n",
    "# Test streaming\n",
    "print(\"\\\\n2Ô∏è‚É£ Streaming Examples:\")\n",
    "stream_result = test_streaming_with_context_manager(\"Explain machine learning concepts\")\n",
    "\n",
    "# Test thread context\n",
    "print(\"\\\\n3Ô∏è‚É£ Thread Context Examples:\")\n",
    "test_thread_context_conversation()\n",
    "\n",
    "print(\"\\\\n‚úÖ Context managers and streaming testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ‚úÖ FIXED: Correct trace_operation Usage\n",
    "\n",
    "The `trace_operation()` context manager has been fixed with the correct syntax:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Started trace: ab5c73b5-6281-4057-9072-a652ab851852\n",
      "DEBUG:noveum_trace.core.client:Started span: bc1d3195-e720-499e-a9e5-4ca0f6e44cd2 in trace: ab5c73b5-6281-4057-9072-a652ab851852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing CORRECTED Context Manager Usage...\n",
      "\\n1Ô∏è‚É£ Correct trace_operation Usage:\n",
      "üîß Testing CORRECT trace_operation usage...\n",
      "üì• Step 1: Loading data...\n",
      "‚öôÔ∏è  Step 2: Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: bc1d3195-e720-499e-a9e5-4ca0f6e44cd2\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace ab5c73b5-6281-4057-9072-a652ab851852 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: ab5c73b5-6281-4057-9072-a652ab851852\n",
      "DEBUG:noveum_trace.core.client:Started trace: c220f277-b383-4889-a59b-5936d551dff4\n",
      "DEBUG:noveum_trace.core.client:Started span: d9fa3f3b-6e87-4973-acfa-eda05ead43b9 in trace: c220f277-b383-4889-a59b-5936d551dff4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Step 3: Generating output...\n",
      "‚úÖ Complex operation completed successfully\n",
      "\\n2Ô∏è‚É£ Correct Batch Operations:\n",
      "üì¶ Testing CORRECT batch trace_operation usage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: d9fa3f3b-6e87-4973-acfa-eda05ead43b9\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace c220f277-b383-4889-a59b-5936d551dff4 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: c220f277-b383-4889-a59b-5936d551dff4\n",
      "DEBUG:noveum_trace.core.client:Started trace: 66dac688-4039-4a23-94ff-f9a31bf4b033\n",
      "DEBUG:noveum_trace.core.client:Started span: de8a7c20-a98f-40a1-9d39-8c9863065544 in trace: 66dac688-4039-4a23-94ff-f9a31bf4b033\n",
      "DEBUG:noveum_trace.core.client:Finished span: de8a7c20-a98f-40a1-9d39-8c9863065544\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace 66dac688-4039-4a23-94ff-f9a31bf4b033 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: 66dac688-4039-4a23-94ff-f9a31bf4b033\n",
      "DEBUG:noveum_trace.core.client:Started trace: f638fb44-816e-41ec-961e-0a132d7e2c66\n",
      "DEBUG:noveum_trace.core.client:Started span: 79f65cc8-6997-4f4e-8a3b-ea110fc7e52e in trace: f638fb44-816e-41ec-961e-0a132d7e2c66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ Batch operation 1/3 completed\n",
      "üî∏ Batch operation 2/3 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:noveum_trace.core.client:Finished span: 79f65cc8-6997-4f4e-8a3b-ea110fc7e52e\n",
      "DEBUG:noveum_trace.transport.http_transport:Trace f638fb44-816e-41ec-961e-0a132d7e2c66 queued for export\n",
      "DEBUG:noveum_trace.core.client:Finished trace: f638fb44-816e-41ec-961e-0a132d7e2c66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ Batch operation 3/3 completed\n",
      "‚úÖ Batch processing completed: 3 operations\n",
      "\\n‚úÖ All corrected context manager examples completed!\n",
      "Operation result: True\n",
      "Batch operations: 3 completed\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED trace_operation Examples\n",
    "# The correct syntax is: trace_operation(operation_name, attributes=dict, tags=dict)\n",
    "\n",
    "def test_correct_trace_operation_usage():\n",
    "    \"\"\"Demonstrate the CORRECT syntax for trace_operation context manager.\"\"\"\n",
    "    print(\"üîß Testing CORRECT trace_operation usage...\")\n",
    "    \n",
    "    # ‚úÖ CORRECT: First parameter is operation_name (string), second is attributes (dict)\n",
    "    with trace_operation(\"data_processing_pipeline\", \n",
    "                        attributes={\"operation_type\": \"data_pipeline\", \"complexity\": \"high\"}) as span:\n",
    "        # Step 1: Data loading\n",
    "        print(\"üì• Step 1: Loading data...\")\n",
    "        time.sleep(0.2)\n",
    "        span.set_attributes({\"step\": \"data_loading\", \"records_loaded\": 1000})\n",
    "        \n",
    "        # Step 2: Processing\n",
    "        print(\"‚öôÔ∏è  Step 2: Processing data...\")\n",
    "        time.sleep(0.3)\n",
    "        span.set_attributes({\"step\": \"processing\", \"records_processed\": 950})\n",
    "        \n",
    "        # Step 3: Output\n",
    "        print(\"üì§ Step 3: Generating output...\")\n",
    "        time.sleep(0.1)\n",
    "        span.set_attributes({\"step\": \"output\", \"records_output\": 950})\n",
    "        \n",
    "        result = {\n",
    "            \"operation\": \"data_processing_pipeline\",\n",
    "            \"input_records\": 1000,\n",
    "            \"processed_records\": 950,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Complex operation completed successfully\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def test_correct_batch_operations():\n",
    "    \"\"\"Demonstrate correct trace_operation usage in batch processing.\"\"\"\n",
    "    print(\"üì¶ Testing CORRECT batch trace_operation usage...\")\n",
    "    \n",
    "    operations = []\n",
    "    \n",
    "    for i in range(3):  # Reduced to 3 for demo\n",
    "        # ‚úÖ CORRECT: operation_name first, then attributes dict\n",
    "        with trace_operation(f\"batch_operation_{i}\", \n",
    "                           attributes={\"operation_type\": \"batch_demo\", \"batch_index\": i}) as span:\n",
    "            span.set_attributes({\n",
    "                \"batch.operation_number\": i,\n",
    "                \"batch.total_operations\": 3,\n",
    "                \"operation.size\": \"small\"\n",
    "            })\n",
    "            time.sleep(0.1)  # Quick operations\n",
    "            operations.append(f\"operation_{i}\")\n",
    "            print(f\"üî∏ Batch operation {i+1}/3 completed\")\n",
    "    \n",
    "    print(f\"‚úÖ Batch processing completed: {len(operations)} operations\")\n",
    "    return operations\n",
    "\n",
    "# Test the corrected functions\n",
    "print(\"üîß Testing CORRECTED Context Manager Usage...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Correct trace_operation Usage:\")\n",
    "operation_result = test_correct_trace_operation_usage()\n",
    "\n",
    "print(\"\\\\n2Ô∏è‚É£ Correct Batch Operations:\")\n",
    "batch_result = test_correct_batch_operations()\n",
    "\n",
    "print(f\"\\\\n‚úÖ All corrected context manager examples completed!\")\n",
    "print(f\"Operation result: {operation_result['success']}\")\n",
    "print(f\"Batch operations: {len(batch_result)} completed\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ‚úÖ FIXED: Correct create_traced_agent Usage\n",
    "\n",
    "The `create_traced_agent()` function has been fixed with the correct parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing CORRECTED Proxy Object Functions...\n",
      "\\n1Ô∏è‚É£ Corrected Traced Agent:\n",
      "ü§ñ Testing Traced Agent Proxy (CORRECTED)...\n",
      "‚úÖ Traced agent proxy created successfully!\n",
      "üß† Testing traced agent methods...\n",
      "üí≠ Think result: Thinking about: How to improve AI observability\n",
      "‚ö° Action result: Performing action: Implement monitoring dashboard\n",
      "üìã Plan result: 3 steps\n",
      "\\n2Ô∏è‚É£ Corrected Traced OpenAI Client:\n",
      "üîÑ Testing Traced OpenAI Client (CORRECTED)...\n",
      "‚úÖ Traced OpenAI client created successfully!\n",
      "ü§ñ Simulating traced OpenAI call...\n",
      "‚ÑπÔ∏è  Would automatically trace all OpenAI API calls\n",
      "\\n‚úÖ All corrected proxy object examples completed!\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED create_traced_agent Example\n",
    "# The correct signature is: create_traced_agent(agent, agent_type, capabilities, trace_config)\n",
    "\n",
    "def test_traced_agent_proxy_corrected():\n",
    "    \"\"\"Test traced agent proxy with CORRECT parameters.\"\"\"\n",
    "    print(\"ü§ñ Testing Traced Agent Proxy (CORRECTED)...\")\n",
    "    \n",
    "    # Mock agent class\n",
    "    class MockAgent:\n",
    "        def __init__(self, name: str):\n",
    "            self.name = name\n",
    "        \n",
    "        def think(self, problem: str) -> str:\n",
    "            time.sleep(0.2)\n",
    "            return f\"Thinking about: {problem}\"\n",
    "        \n",
    "        def act(self, action: str) -> str:\n",
    "            time.sleep(0.3)\n",
    "            return f\"Performing action: {action}\"\n",
    "        \n",
    "        def plan(self, goal: str) -> List[str]:\n",
    "            time.sleep(0.4)\n",
    "            return [f\"Step 1 for {goal}\", f\"Step 2 for {goal}\", f\"Step 3 for {goal}\"]\n",
    "    \n",
    "    # ‚úÖ CORRECT: Use proper parameter names and structure\n",
    "    original_agent = MockAgent(\"demo_agent\")\n",
    "    traced_agent = create_traced_agent(\n",
    "        agent=original_agent,\n",
    "        agent_type=\"traced_demo_agent\",  # ‚úÖ CORRECT: agent_type (not agent_id)\n",
    "        capabilities=[\"thinking\", \"acting\", \"planning\"],  # ‚úÖ CORRECT: capabilities (not auto_trace_methods)\n",
    "        trace_config={\"capture_inputs\": True, \"capture_outputs\": True}  # ‚úÖ CORRECT: trace_config dict\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Traced agent proxy created successfully!\")\n",
    "    \n",
    "    # Test traced methods\n",
    "    print(\"üß† Testing traced agent methods...\")\n",
    "    \n",
    "    thought = traced_agent.think(\"How to improve AI observability\")\n",
    "    print(f\"üí≠ Think result: {thought}\")\n",
    "    \n",
    "    action = traced_agent.act(\"Implement monitoring dashboard\")\n",
    "    print(f\"‚ö° Action result: {action}\")\n",
    "    \n",
    "    plan = traced_agent.plan(\"Enhance system reliability\")\n",
    "    print(f\"üìã Plan result: {len(plan)} steps\")\n",
    "    \n",
    "    return traced_agent\n",
    "\n",
    "def test_traced_openai_client_corrected():\n",
    "    \"\"\"Test traced OpenAI client with CORRECT parameters.\"\"\"\n",
    "    print(\"üîÑ Testing Traced OpenAI Client (CORRECTED)...\")\n",
    "    \n",
    "    # ‚úÖ CORRECT: Use actual OpenAI client instance, not direct parameters\n",
    "    try:\n",
    "        # First create a real OpenAI client (even with mock key)\n",
    "        import openai\n",
    "        original_client = openai.OpenAI(api_key=\"mock-key-for-demo\")\n",
    "        \n",
    "        # ‚úÖ CORRECT: Pass the client instance to the tracer\n",
    "        traced_client = create_traced_openai_client(\n",
    "            original_client=original_client,\n",
    "            trace_config={\"trace_completions\": True, \"capture_content\": True}\n",
    "        )\n",
    "        print(\"‚úÖ Traced OpenAI client created successfully!\")\n",
    "        \n",
    "        # Mock a call (won't actually work without real API key)\n",
    "        print(\"ü§ñ Simulating traced OpenAI call...\")\n",
    "        print(\"‚ÑπÔ∏è  Would automatically trace all OpenAI API calls\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Traced client demo: {e}\")\n",
    "        print(\"üìù Note: This requires a real OpenAI client instance\")\n",
    "\n",
    "# Test the corrected functions\n",
    "print(\"üöÄ Testing CORRECTED Proxy Object Functions...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Corrected Traced Agent:\")\n",
    "traced_agent = test_traced_agent_proxy_corrected()\n",
    "\n",
    "print(\"\\\\n2Ô∏è‚É£ Corrected Traced OpenAI Client:\")\n",
    "test_traced_openai_client_corrected()\n",
    "\n",
    "print(\"\\\\n‚úÖ All corrected proxy object examples completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîÑ FINAL COMPREHENSIVE FLUSH\n",
    "\n",
    "Final flush to ensure all traces from the entire demo are sent to your endpoint.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß FIXED: Enhanced SDK Initialization with Endpoint Debugging\n",
    "\n",
    "The endpoint configuration has been fixed with proper debugging and transport settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1154693373.py, line 33)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\\\"\\\"\\\"Verify that the Beeceptor endpoint is reachable.\\\"\\\"\\\"\u001b[39m\n     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# üîß ENHANCED SDK INITIALIZATION WITH ENDPOINT DEBUGGING\n",
    "import noveum_trace\n",
    "from noveum_trace import trace, trace_agent, trace_llm, trace_tool\n",
    "import logging\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "# üîç COMPREHENSIVE DEBUGGING SETUP\n",
    "print(\"üîß Setting up enhanced debugging for transport layer...\")\n",
    "\n",
    "# Set up comprehensive logging with detailed output\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Enable specific loggers for transport debugging\n",
    "loggers_to_enable = [\n",
    "    'noveum_trace.transport', \n",
    "    'noveum_trace.transport.http_transport',\n",
    "    'urllib3.connectionpool'\n",
    "]\n",
    "\n",
    "for logger_name in loggers_to_enable:\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "print(\"‚úÖ Enhanced debugging enabled\")\n",
    "\n",
    "# üåê ENDPOINT VERIFICATION \n",
    "def verify_endpoint():\n",
    "    \\\"\\\"\\\"Verify that the Beeceptor endpoint is reachable.\\\"\\\"\\\"\n",
    "    endpoint = \"https://noveum-trace.free.beeceptor.com\"\n",
    "    \n",
    "    print(f\"üîç Verifying endpoint reachability: {endpoint}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(endpoint, timeout=10)\n",
    "        print(f\"‚úÖ Base endpoint reachable - Status: {response.status_code}\")\n",
    "        \n",
    "        # Test the actual trace endpoints\n",
    "        trace_endpoints = [\n",
    "            f\"{endpoint}/v1/trace\",   # Single trace endpoint\n",
    "            f\"{endpoint}/v1/traces\"   # Batch trace endpoint  \n",
    "        ]\n",
    "        \n",
    "        for test_endpoint in trace_endpoints:\n",
    "            try:\n",
    "                test_response = requests.head(test_endpoint, timeout=5)\n",
    "                print(f\"üì° {test_endpoint} - Status: {test_response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  {test_endpoint} - Error: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Endpoint verification failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Verify endpoint first\n",
    "endpoint_ok = verify_endpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nüöÄ Initializing Noveum Trace SDK with enhanced configuration...\n",
      "üåê Base Endpoint: https://api.noveum.ai/api\n",
      "üì° Single Trace Endpoint: https://api.noveum.ai/api/v1/trace\n",
      "üì¶ Batch Trace Endpoint: https://api.noveum.ai/api/v1/traces\n",
      "üîß Transport Mode: Individual traces (batch_size=1) for better debugging\n",
      "‚úÖ Noveum Trace SDK initialized successfully!\n",
      "üìä Project: jupyter-test-project\n",
      "üîß Environment: development\n",
      "üåê Transport Endpoint: https://api.noveum.ai/api\n",
      "üì¶ Batch Size: 10\n",
      "‚è±Ô∏è  Batch Timeout: 2.0s\n",
      "üîç Debug Mode: True\n",
      "\\nüéØ SDK initialization completed!\n",
      "üìã Check the debug log output above for HTTP request details\n",
      "üåê Your traces should now be visible at: https://api.noveum.ai/api\n"
     ]
    }
   ],
   "source": [
    "# üöÄ ENHANCED SDK INITIALIZATION\n",
    "try:\n",
    "    print(\"\\\\nüöÄ Initializing Noveum Trace SDK with enhanced configuration...\")\n",
    "    \n",
    "    # üìã Display endpoint mapping\n",
    "    base_endpoint = \"https://api.noveum.ai/api\"\n",
    "    print(f\"üåê Base Endpoint: {base_endpoint}\")\n",
    "    print(f\"üì° Single Trace Endpoint: {base_endpoint}/v1/trace\")\n",
    "    print(f\"üì¶ Batch Trace Endpoint: {base_endpoint}/v1/traces\")\n",
    "    print(f\"üîß Transport Mode: Individual traces (batch_size=1) for better debugging\")\n",
    "    \n",
    "    noveum_trace.init(\n",
    "        api_key=os.getenv('NOVEUM_API_KEY'),\n",
    "        project=\"jupyter-test-project\",\n",
    "        environment=\"development\", \n",
    "        endpoint=base_endpoint,  # SDK will append /v1/trace or /v1/traces automatically\n",
    "        debug=True,  # Enable debug mode\n",
    "        \n",
    "        # üîß Enhanced transport configuration for debugging\n",
    "        transport_config={\n",
    "            \"timeout\": 30,           # 30 second timeout (generous for debugging)\n",
    "            \"retry_attempts\": 0,     # No retries for faster debugging feedback  \n",
    "            \"batch_size\": 1,         # Send traces individually (not batched)\n",
    "            \"batch_timeout\": 0.5,    # Send traces immediately\n",
    "            \"compression\": False,    # No compression for easier debugging\n",
    "            \"verify_ssl\": True       # Verify SSL certificates\n",
    "        },\n",
    "        \n",
    "        # ‚úÖ Comprehensive tracing configuration  \n",
    "        tracing_config={\n",
    "            \"sample_rate\": 1.0,        # Trace 100% of operations\n",
    "            \"capture_errors\": True,    # Capture error details\n",
    "            \"auto_flush\": True         # Automatically flush traces\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Noveum Trace SDK initialized successfully!\")\n",
    "    \n",
    "    # üìä Display configuration details\n",
    "    config = noveum_trace.get_config()\n",
    "    print(f\"üìä Project: {config.project}\")\n",
    "    print(f\"üîß Environment: {config.environment}\")\n",
    "    print(f\"üåê Transport Endpoint: {config.transport.endpoint}\")\n",
    "    print(f\"üì¶ Batch Size: {config.transport.batch_size}\")\n",
    "    print(f\"‚è±Ô∏è  Batch Timeout: {config.transport.batch_timeout}s\")\n",
    "    print(f\"üîç Debug Mode: {config.debug}\")\n",
    "    \n",
    "    print(\"\\\\nüéØ SDK initialization completed!\")\n",
    "    print(\"üìã Check the debug log output above for HTTP request details\")\n",
    "    print(\"üåê Your traces should now be visible at: \" + base_endpoint)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing SDK: {e}\")\n",
    "    print(\"Continuing with demo - traces will be logged locally\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß Debug: Testing Endpoint Connectivity\n",
    "\n",
    "Let's test if traces are being sent to your configured endpoint and diagnose any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7.2: Auto-Instrumentation and Advanced Features\n",
    "\n",
    "Test auto-instrumentation, proxy objects, and advanced SDK features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing Auto-Instrumentation and Advanced Features...\n",
      "\\n1Ô∏è‚É£ Auto-Instrumentation:\n",
      "üîß Testing Auto-Instrumentation...\n",
      "üì¶ Available instrumentations: ['openai', 'anthropic', 'langchain']\n",
      "‚úÖ OpenAI already instrumented\n",
      "üîç Currently instrumented: ['openai']\n",
      "\\n2Ô∏è‚É£ Proxy Objects:\n",
      "üîÑ Testing Traced OpenAI Client...\n",
      "‚ÑπÔ∏è  Traced client demo: create_traced_openai_client() got an unexpected keyword argument 'api_key'\n",
      "ü§ñ Testing Traced Agent Proxy...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_traced_agent() got an unexpected keyword argument 'auto_trace_methods'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 240\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn2Ô∏è‚É£ Proxy Objects:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    239\u001b[39m test_traced_openai_client()\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[43mtest_traced_agent_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn3Ô∏è‚É£ Manual Tracing:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    243\u001b[39m test_manual_tracing()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mtest_traced_agent_proxy\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Create traced agent proxy\u001b[39;00m\n\u001b[32m     81\u001b[39m original_agent = MockAgent(\u001b[33m\"\u001b[39m\u001b[33mdemo_agent\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m traced_agent = \u001b[43mcreate_traced_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43moriginal_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_trace_methods\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthink\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mact\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapture_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapture_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     87\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Traced agent proxy created\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Test traced methods\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: create_traced_agent() got an unexpected keyword argument 'auto_trace_methods'"
     ]
    }
   ],
   "source": [
    "# Import advanced features\n",
    "from noveum_trace import (\n",
    "    auto_instrument, get_instrumented_libraries, is_instrumented,\n",
    "    create_traced_openai_client, create_traced_agent, TracedOpenAIClient,\n",
    "    start_trace, start_span, get_current_trace, get_current_span\n",
    ")\n",
    "\n",
    "# Auto-Instrumentation Examples\n",
    "\n",
    "def test_auto_instrumentation():\n",
    "    \"\"\"Test automatic instrumentation of libraries.\"\"\"\n",
    "    print(\"üîß Testing Auto-Instrumentation...\")\n",
    "    \n",
    "    # Check available instrumentations\n",
    "    available = noveum_trace.get_available_instrumentations()\n",
    "    print(f\"üì¶ Available instrumentations: {available}\")\n",
    "    \n",
    "    # Enable auto-instrumentation for OpenAI (if not already enabled)\n",
    "    if not is_instrumented(\"openai\"):\n",
    "        print(\"üîå Enabling OpenAI auto-instrumentation...\")\n",
    "        try:\n",
    "            auto_instrument(\"openai\")\n",
    "            print(\"‚úÖ OpenAI auto-instrumentation enabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Auto-instrumentation: {e}\")\n",
    "    else:\n",
    "        print(\"‚úÖ OpenAI already instrumented\")\n",
    "    \n",
    "    # Check instrumented libraries\n",
    "    instrumented = get_instrumented_libraries()\n",
    "    print(f\"üîç Currently instrumented: {instrumented}\")\n",
    "    \n",
    "    return instrumented\n",
    "\n",
    "# Proxy Objects for Enhanced Control\n",
    "\n",
    "def test_traced_openai_client():\n",
    "    \"\"\"Test traced OpenAI client proxy.\"\"\"\n",
    "    print(\"üîÑ Testing Traced OpenAI Client...\")\n",
    "    \n",
    "    # Create traced OpenAI client (even without real API key)\n",
    "    try:\n",
    "        traced_client = create_traced_openai_client(\n",
    "            api_key=\"mock-key-for-demo\",\n",
    "            trace_completions=True,\n",
    "            trace_embeddings=True,\n",
    "            capture_content=True\n",
    "        )\n",
    "        print(\"‚úÖ Traced OpenAI client created\")\n",
    "        \n",
    "        # Mock a call (won't actually work without real API key)\n",
    "        print(\"ü§ñ Simulating traced OpenAI call...\")\n",
    "        # In real usage: response = traced_client.chat.completions.create(...)\n",
    "        print(\"‚ÑπÔ∏è  Would automatically trace all OpenAI API calls\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Traced client demo: {e}\")\n",
    "\n",
    "def test_traced_agent_proxy():\n",
    "    \"\"\"Test traced agent proxy for enhanced agent monitoring.\"\"\"\n",
    "    print(\"ü§ñ Testing Traced Agent Proxy...\")\n",
    "    \n",
    "    # Mock agent class\n",
    "    class MockAgent:\n",
    "        def __init__(self, name: str):\n",
    "            self.name = name\n",
    "        \n",
    "        def think(self, problem: str) -> str:\n",
    "            time.sleep(0.2)\n",
    "            return f\"Thinking about: {problem}\"\n",
    "        \n",
    "        def act(self, action: str) -> str:\n",
    "            time.sleep(0.3)\n",
    "            return f\"Performing action: {action}\"\n",
    "        \n",
    "        def plan(self, goal: str) -> List[str]:\n",
    "            time.sleep(0.4)\n",
    "            return [f\"Step 1 for {goal}\", f\"Step 2 for {goal}\", f\"Step 3 for {goal}\"]\n",
    "    \n",
    "    # Create traced agent proxy\n",
    "    original_agent = MockAgent(\"demo_agent\")\n",
    "    traced_agent = create_traced_agent(\n",
    "        agent=original_agent,\n",
    "        auto_trace_methods=[\"think\", \"act\", \"plan\"],\n",
    "        capture_inputs=True,\n",
    "        capture_outputs=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Traced agent proxy created\")\n",
    "    \n",
    "    # Test traced methods\n",
    "    print(\"üß† Testing traced agent methods...\")\n",
    "    \n",
    "    thought = traced_agent.think(\"How to improve AI observability\")\n",
    "    print(f\"üí≠ Think result: {thought}\")\n",
    "    \n",
    "    action = traced_agent.act(\"Implement monitoring dashboard\")\n",
    "    print(f\"‚ö° Action result: {action}\")\n",
    "    \n",
    "    plan = traced_agent.plan(\"Enhance system reliability\")\n",
    "    print(f\"üìã Plan result: {len(plan)} steps\")\n",
    "\n",
    "# Manual Span Creation and Management\n",
    "\n",
    "def test_manual_tracing():\n",
    "    \"\"\"Test manual trace and span creation.\"\"\"\n",
    "    print(\"üîç Testing Manual Tracing...\")\n",
    "    \n",
    "    # Start a manual trace\n",
    "    trace = start_trace(\"manual_demo_trace\")\n",
    "    print(f\"‚úÖ Started trace: {trace.trace_id}\")\n",
    "    \n",
    "    # Create nested spans manually\n",
    "    with trace.span(\"parent_operation\") as parent_span:\n",
    "        parent_span.set_attributes({\n",
    "            \"operation.type\": \"parent\",\n",
    "            \"operation.importance\": \"high\"\n",
    "        })\n",
    "        print(\"üìä Parent span created\")\n",
    "        \n",
    "        # Child span 1\n",
    "        with parent_span.create_child_span(\"child_operation_1\") as child1:\n",
    "            child1.set_attributes({\n",
    "                \"operation.type\": \"child\",\n",
    "                \"child.number\": 1\n",
    "            })\n",
    "            time.sleep(0.2)\n",
    "            print(\"üîπ Child span 1 completed\")\n",
    "        \n",
    "        # Child span 2  \n",
    "        with parent_span.create_child_span(\"child_operation_2\") as child2:\n",
    "            child2.set_attributes({\n",
    "                \"operation.type\": \"child\",\n",
    "                \"child.number\": 2,\n",
    "                \"child.data_processed\": 500\n",
    "            })\n",
    "            time.sleep(0.3)\n",
    "            print(\"üîπ Child span 2 completed\")\n",
    "        \n",
    "        print(\"üìä Parent operation completed\")\n",
    "    \n",
    "    # Finish trace\n",
    "    trace.finish()\n",
    "    print(f\"‚úÖ Manual trace completed: {trace.trace_id}\")\n",
    "\n",
    "# Advanced Configuration and Performance Features\n",
    "\n",
    "def test_advanced_configuration():\n",
    "    \"\"\"Test advanced SDK configuration options.\"\"\"\n",
    "    print(\"‚öôÔ∏è  Testing Advanced Configuration...\")\n",
    "    \n",
    "    # Get current configuration\n",
    "    config = noveum_trace.get_config()\n",
    "    print(f\"üìã Current project: {config.project}\")\n",
    "    print(f\"üåê Current endpoint: {config.transport.endpoint}\")\n",
    "    print(f\"üì¶ Batch size: {config.transport.batch_size}\")\n",
    "    print(f\"‚è±Ô∏è  Batch timeout: {config.transport.batch_timeout}\")\n",
    "    \n",
    "    # Test configuration updates (temporary for demo)\n",
    "    original_debug = config.debug\n",
    "    \n",
    "    # Temporarily enable debug mode\n",
    "    noveum_trace.configure(debug=True)\n",
    "    print(\"üêõ Debug mode enabled temporarily\")\n",
    "    \n",
    "    # Create a trace to demonstrate debug output\n",
    "    with noveum_trace.trace_operation(\"debug_demo_operation\") as span:\n",
    "        span.set_attributes({\"demo\": \"configuration\", \"debug_enabled\": True})\n",
    "        time.sleep(0.1)\n",
    "        print(\"‚úÖ Debug operation completed\")\n",
    "    \n",
    "    # Restore original debug setting\n",
    "    noveum_trace.configure(debug=original_debug)\n",
    "    print(f\"üîß Debug mode restored to: {original_debug}\")\n",
    "\n",
    "# Batch Processing and Performance Monitoring\n",
    "\n",
    "def test_batch_processing():\n",
    "    \"\"\"Test batch operations for performance.\"\"\"\n",
    "    print(\"üì¶ Testing Batch Processing...\")\n",
    "    \n",
    "    # Create multiple operations quickly to test batching\n",
    "    operations = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        with noveum_trace.trace_operation(f\"batch_operation_{i}\", operation_type=\"batch_demo\") as span:\n",
    "            span.set_attributes({\n",
    "                \"batch.operation_number\": i,\n",
    "                \"batch.total_operations\": 5,\n",
    "                \"operation.size\": \"small\"\n",
    "            })\n",
    "            time.sleep(0.05)  # Quick operations\n",
    "            operations.append(f\"operation_{i}\")\n",
    "            print(f\"üî∏ Batch operation {i+1}/5 completed\")\n",
    "    \n",
    "    print(f\"‚úÖ Batch processing completed: {len(operations)} operations\")\n",
    "    \n",
    "    # Force flush to send batched traces\n",
    "    try:\n",
    "        noveum_trace.flush()\n",
    "        print(\"üì§ Forced flush of batched traces\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Flush status: {e}\")\n",
    "\n",
    "# Error Handling and Edge Cases\n",
    "\n",
    "def test_error_handling():\n",
    "    \"\"\"Test error handling and edge cases.\"\"\"\n",
    "    print(\"‚ö†Ô∏è  Testing Error Handling...\")\n",
    "    \n",
    "    # Test error capture in traced function\n",
    "    @noveum_trace.trace(capture_errors=True, capture_stack_trace=True)\n",
    "    def operation_with_error(should_fail: bool = False):\n",
    "        if should_fail:\n",
    "            raise ValueError(\"This is a demo error for testing\")\n",
    "        return \"Success!\"\n",
    "    \n",
    "    # Test successful operation\n",
    "    try:\n",
    "        result = operation_with_error(should_fail=False)\n",
    "        print(f\"‚úÖ Successful operation: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "    \n",
    "    # Test error capture\n",
    "    try:\n",
    "        result = operation_with_error(should_fail=True)\n",
    "        print(f\"Unexpected success: {result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"‚úÖ Error captured successfully: {e}\")\n",
    "\n",
    "# Run all advanced feature tests\n",
    "print(\"üöÄ Testing Auto-Instrumentation and Advanced Features...\")\n",
    "\n",
    "print(\"\\\\n1Ô∏è‚É£ Auto-Instrumentation:\")\n",
    "instrumented_libs = test_auto_instrumentation()\n",
    "\n",
    "print(\"\\\\n2Ô∏è‚É£ Proxy Objects:\")\n",
    "test_traced_openai_client()\n",
    "test_traced_agent_proxy()\n",
    "\n",
    "print(\"\\\\n3Ô∏è‚É£ Manual Tracing:\")\n",
    "test_manual_tracing()\n",
    "\n",
    "print(\"\\\\n4Ô∏è‚É£ Advanced Configuration:\")\n",
    "test_advanced_configuration()\n",
    "\n",
    "print(\"\\\\n5Ô∏è‚É£ Batch Processing:\")\n",
    "test_batch_processing()\n",
    "\n",
    "print(\"\\\\n6Ô∏è‚É£ Error Handling:\")\n",
    "test_error_handling()\n",
    "\n",
    "print(\"\\\\n‚úÖ Advanced features testing completed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Multi-Agent System\n",
    "\n",
    "Test multi-agent workflow tracing with the `@trace_agent` decorator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ Orchestrator: Starting workflow for task 'Analyze the importance of observability in AI systems'\n",
      "\n",
      "üîç Step 1: Research Phase\n",
      "üîç Research Agent: Processing query 'Analyze the importance of observability in AI systems'\n",
      "‚úÖ Research completed with 3 sources\n",
      "\n",
      "üìä Step 2: Analysis Phase\n",
      "\n",
      "‚úÖ Orchestrator: Workflow completed successfully\n",
      "\n",
      "üé≠ Final Workflow Result:\n",
      "Task: Analyze the importance of observability in AI systems\n",
      "Confidence: 0.87\n",
      "Phases: 2\n"
     ]
    }
   ],
   "source": [
    "@trace_agent(agent_id=\"research_agent\")\n",
    "def research_agent(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Research agent that gathers information.\"\"\"\n",
    "    print(f\"üîç Research Agent: Processing query '{query}'\")\n",
    "\n",
    "    # Simulate research process\n",
    "    time.sleep(0.4)\n",
    "\n",
    "    # Mock research findings\n",
    "    findings = {\n",
    "        \"query\": query,\n",
    "        \"sources\": [\"source1.pdf\", \"source2.html\", \"source3.json\"],\n",
    "        \"key_points\": [\n",
    "            \"Point 1: Observability improves system reliability\",\n",
    "            \"Point 2: Tracing helps identify bottlenecks\",\n",
    "            \"Point 3: Monitoring enables proactive maintenance\"\n",
    "        ],\n",
    "        \"confidence\": 0.87,\n",
    "        \"research_time\": \"0.4s\"\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Research completed with {len(findings['sources'])} sources\")\n",
    "    return findings\n",
    "\n",
    "@trace_agent(agent_id=\"orchestrator\")\n",
    "def orchestrate_workflow(task: str) -> Dict[str, Any]:\n",
    "    \"\"\"Orchestrator agent that coordinates multiple agents.\"\"\"\n",
    "    print(f\"üé≠ Orchestrator: Starting workflow for task '{task}'\")\n",
    "\n",
    "    # Step 1: Research\n",
    "    print(\"\\nüîç Step 1: Research Phase\")\n",
    "    research_data = research_agent(task)\n",
    "\n",
    "    # Step 2: Analysis (simplified)\n",
    "    print(\"\\nüìä Step 2: Analysis Phase\")\n",
    "    analysis_data = {\n",
    "        \"insights\": [\"Observability is crucial\", \"Tracing provides insights\"],\n",
    "        \"quality_score\": 0.89\n",
    "    }\n",
    "\n",
    "    # Final orchestration result\n",
    "    workflow_result = {\n",
    "        \"task\": task,\n",
    "        \"workflow_id\": \"wf-001\",\n",
    "        \"phases_completed\": 2,\n",
    "        \"research_summary\": research_data[\"key_points\"],\n",
    "        \"analysis_summary\": analysis_data[\"insights\"],\n",
    "        \"overall_confidence\": research_data[\"confidence\"],\n",
    "        \"total_time\": \"1.0s\"\n",
    "    }\n",
    "\n",
    "    print(\"\\n‚úÖ Orchestrator: Workflow completed successfully\")\n",
    "    return workflow_result\n",
    "\n",
    "# Test the multi-agent workflow\n",
    "task = \"Analyze the importance of observability in AI systems\"\n",
    "workflow_result = orchestrate_workflow(task)\n",
    "print(\"\\nüé≠ Final Workflow Result:\")\n",
    "print(f\"Task: {workflow_result['task']}\")\n",
    "print(f\"Confidence: {workflow_result['overall_confidence']:.2f}\")\n",
    "print(f\"Phases: {workflow_result['phases_completed']}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: Tool Tracing\n",
    "\n",
    "Test tool tracing with the `@trace_tool` decorator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Calculator: Performing multiply on 15 and 4\n",
      "\n",
      "üî¢ Calculator Result: {'operation': 'multiply', 'operands': [15, 4], 'result': 60, 'success': True}\n",
      "üìù Text Analyzer: Analyzing text of length 87\n",
      "‚úÖ Analysis complete: 14 words, 2 sentences\n",
      "\n",
      "üìù Text Analysis Result: {'text_length': 87, 'word_count': 14, 'sentence_count': 2, 'avg_word_length': 5.285714285714286}\n"
     ]
    }
   ],
   "source": [
    "@trace_tool(tool_name=\"calculator\")\n",
    "def calculate(operation: str, a: float, b: float) -> Dict[str, Any]:\n",
    "    \"\"\"A calculator tool with tracing.\"\"\"\n",
    "    print(f\"üî¢ Calculator: Performing {operation} on {a} and {b}\")\n",
    "\n",
    "    operations = {\n",
    "        \"add\": lambda x, y: x + y,\n",
    "        \"subtract\": lambda x, y: x - y,\n",
    "        \"multiply\": lambda x, y: x * y,\n",
    "        \"divide\": lambda x, y: x / y if y != 0 else None\n",
    "    }\n",
    "\n",
    "    if operation not in operations:\n",
    "        return {\"error\": f\"Unknown operation: {operation}\"}\n",
    "\n",
    "    try:\n",
    "        result = operations[operation](a, b)\n",
    "        if result is None:\n",
    "            return {\"error\": \"Division by zero\"}\n",
    "\n",
    "        return {\n",
    "            \"operation\": operation,\n",
    "            \"operands\": [a, b],\n",
    "            \"result\": result,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"success\": False}\n",
    "\n",
    "@trace_tool(tool_name=\"text_analyzer\")\n",
    "def analyze_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Text analysis tool with tracing.\"\"\"\n",
    "    print(f\"üìù Text Analyzer: Analyzing text of length {len(text)}\")\n",
    "\n",
    "    # Simulate analysis\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    analysis = {\n",
    "        \"text_length\": len(text),\n",
    "        \"word_count\": len(text.split()),\n",
    "        \"sentence_count\": text.count('.') + text.count('!') + text.count('?'),\n",
    "        \"avg_word_length\": sum(len(word) for word in text.split()) / len(text.split()) if text.split() else 0\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Analysis complete: {analysis['word_count']} words, {analysis['sentence_count']} sentences\")\n",
    "    return analysis\n",
    "\n",
    "# Test tool tracing\n",
    "calc_result = calculate(\"multiply\", 15, 4)\n",
    "print(f\"\\nüî¢ Calculator Result: {calc_result}\")\n",
    "\n",
    "text_to_analyze = \"This is a sample text for testing the noveum-trace SDK. It contains multiple sentences!\"\n",
    "text_analysis = analyze_text(text_to_analyze)\n",
    "print(f\"\\nüìù Text Analysis Result: {text_analysis}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 8: Summary and Cleanup\n",
    "\n",
    "Test summary and cleanup of resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7d/c1z608491jq84m9f40phz5rh0000gn/T/ipykernel_79881/3807004596.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "2025-07-29 23:51:35 - noveum_trace.transport.http_transport - INFO - üì§ EXPORTING TRACE: auto_trace_llm.streaming_chat (ID: 9642c2f8-6113-4311-a5a5-123b6f128c38) - 12 spans\n",
      "2025-07-29 23:51:35 - noveum_trace.transport.batch_processor - INFO - üì• ADDING TRACE TO QUEUE: auto_trace_llm.streaming_chat (ID: 9642c2f8-6113-4311-a5a5-123b6f128c38) - 12 spans\n",
      "2025-07-29 23:51:35 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully queued trace 9642c2f8-6113-4311-a5a5-123b6f128c38\n",
      "2025-07-29 23:51:35 - noveum_trace.transport.http_transport - INFO - ‚úÖ Trace 9642c2f8-6113-4311-a5a5-123b6f128c38 successfully queued for export\n",
      "2025-07-29 23:51:35 - noveum_trace.transport.batch_processor - INFO - üîÑ FLUSH: Sending current batch of 1 traces\n",
      "2025-07-29 23:51:35 - noveum_trace.transport.batch_processor - INFO - üì§ SENDING BATCH: 1 traces via send_callback\n",
      "2025-07-29 23:51:35 - noveum_trace.transport.http_transport - INFO - üöÄ SENDING BATCH: 1 traces to https://api.noveum.ai/api/v1/traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã NOVEUM TRACE SDK TEST SUMMARY\n",
      "==================================================\n",
      "‚úÖ SDK Version: 0.3.3\n",
      "‚úÖ Python Version: 3.13.5\n",
      "‚úÖ Environment Variables: ‚úì\n",
      "\n",
      "üß™ Features Tested:\n",
      "  ‚úÖ Basic function tracing (@trace)\n",
      "  ‚úÖ LLM call tracing (@trace_llm)\n",
      "  ‚úÖ Agent workflow tracing (@trace_agent)\n",
      "  ‚úÖ Tool tracing (@trace_tool)\n",
      "  ‚úÖ Multi-agent orchestration\n",
      "  ‚úÖ Error handling\n",
      "  ‚úÖ Framework integration simulation\n",
      "\n",
      "üéØ Key Results:\n",
      "  üîß All decorators: Functional\n",
      "  ü§ñ Multi-agent support: Functional\n",
      "  üîå Framework integration: Simulated successfully\n",
      "\n",
      "‚úÖ All tests completed successfully!\n",
      "\n",
      "üìñ Next Steps:\n",
      "  1. Set up your actual NOVEUM_API_KEY for production use\n",
      "  2. Integrate with your LLM applications\n",
      "  3. Set up dashboards and monitoring\n",
      "  4. Configure alerting based on trace data\n",
      "üßπ Cleaning up test resources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://api.noveum.ai:443 \"POST /api/v1/traces HTTP/1.1\" 200 None\n",
      "2025-07-29 23:51:36 - noveum_trace.transport.http_transport - INFO - üì° HTTP RESPONSE: Status 200 from https://api.noveum.ai/api/v1/traces\n",
      "2025-07-29 23:51:36 - noveum_trace.transport.http_transport - INFO - ‚úÖ Successfully sent batch of 1 traces\n",
      "2025-07-29 23:51:36 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Successfully sent batch of 1 traces via callback\n",
      "2025-07-29 23:51:36 - noveum_trace.transport.batch_processor - INFO - ‚úÖ Batch processor flush completed in 0.00s\n",
      "2025-07-29 23:51:36 - noveum_trace.transport.http_transport - INFO - HTTP transport flush completed\n",
      "2025-07-29 23:51:36 - noveum_trace.core.client - INFO - Flushed all pending traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Traces flushed successfully\n",
      "‚úÖ Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "# Test summary\n",
    "def print_test_summary():\n",
    "    \"\"\"Print a summary of all tests performed.\"\"\"\n",
    "    print(\"üìã NOVEUM TRACE SDK TEST SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Check SDK version\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(\"noveum-trace\").version\n",
    "        print(f\"‚úÖ SDK Version: {version}\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  Could not determine SDK version\")\n",
    "\n",
    "    # Environment check\n",
    "    print(f\"‚úÖ Python Version: {sys.version.split()[0]}\")\n",
    "    print(f\"‚úÖ Environment Variables: {'‚úì' if os.getenv('NOVEUM_API_KEY') else '‚úó'}\")\n",
    "\n",
    "    # Features tested\n",
    "    features_tested = [\n",
    "        \"Basic function tracing (@trace)\",\n",
    "        \"LLM call tracing (@trace_llm)\",\n",
    "        \"Agent workflow tracing (@trace_agent)\",\n",
    "        \"Tool tracing (@trace_tool)\",\n",
    "        \"Multi-agent orchestration\",\n",
    "        \"Error handling\",\n",
    "        \"Framework integration simulation\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nüß™ Features Tested:\")\n",
    "    for feature in features_tested:\n",
    "        print(f\"  ‚úÖ {feature}\")\n",
    "\n",
    "    print(\"\\nüéØ Key Results:\")\n",
    "    print(\"  üîß All decorators: Functional\")\n",
    "    print(\"  ü§ñ Multi-agent support: Functional\")\n",
    "    print(\"  üîå Framework integration: Simulated successfully\")\n",
    "\n",
    "    print(\"\\n‚úÖ All tests completed successfully!\")\n",
    "    print(\"\\nüìñ Next Steps:\")\n",
    "    print(\"  1. Set up your actual NOVEUM_API_KEY for production use\")\n",
    "    print(\"  2. Integrate with your LLM applications\")\n",
    "    print(\"  3. Set up dashboards and monitoring\")\n",
    "    print(\"  4. Configure alerting based on trace data\")\n",
    "\n",
    "# Clean up function\n",
    "def cleanup_resources():\n",
    "    \"\"\"Clean up any resources created during testing.\"\"\"\n",
    "    print(\"üßπ Cleaning up test resources...\")\n",
    "\n",
    "    try:\n",
    "        # Attempt to flush any pending traces\n",
    "        noveum_trace.flush()\n",
    "        print(\"‚úÖ Traces flushed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Trace flush: {e}\")\n",
    "\n",
    "    print(\"‚úÖ Cleanup completed\")\n",
    "\n",
    "# Run summary and cleanup\n",
    "print_test_summary()\n",
    "cleanup_resources()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You have successfully tested the Noveum Trace SDK from PyPI! \n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "- ‚úÖ Installation from PyPI\n",
    "- ‚úÖ Environment setup with proper API keys\n",
    "- ‚úÖ Basic function tracing with `@trace`\n",
    "- ‚úÖ LLM call tracing with `@trace_llm`\n",
    "- ‚úÖ Agent workflow tracing with `@trace_agent`\n",
    "- ‚úÖ Tool tracing with `@trace_tool`\n",
    "- ‚úÖ Multi-agent system orchestration\n",
    "- ‚úÖ Error handling and edge cases\n",
    "- ‚úÖ Performance considerations\n",
    "- ‚úÖ Framework integration patterns\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Production Setup**: Replace the dummy API key with your actual Noveum API key\n",
    "2. **Integration**: Integrate these patterns into your existing LLM applications\n",
    "3. **Monitoring**: Set up dashboards to monitor your traced applications\n",
    "4. **Optimization**: Use the trace data to optimize your application performance\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- üìö [Noveum Trace Documentation](https://docs.noveum.ai)\n",
    "- üêô [GitHub Repository](https://github.com/Noveum/noveum-trace)\n",
    "- üì¶ [PyPI Package](https://pypi.org/project/noveum-trace/)\n",
    "\n",
    "Happy tracing! üöÄ\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
